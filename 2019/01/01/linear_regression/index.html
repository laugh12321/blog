<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>机器学习|线性回归算法详解 - Laugh&#039;s blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Laugh&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Laugh&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="线性回归线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习|线性回归算法详解"><meta property="og:url" content="http://www.laugh12321.cn/blog/2019/01/01/linear_regression/"><meta property="og:site_name" content="Laugh&#039;s blog"><meta property="og:description" content="线性回归线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/03.jpg"><meta property="article:published_time" content="2019-01-01T00:00:00.000Z"><meta property="article:modified_time" content="2020-10-21T05:59:02.931Z"><meta property="article:author" content="Laugh"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Linear Regression"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/03.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.laugh12321.cn/blog"},"headline":"Laugh's blog","image":["https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/photo/03.jpg"],"datePublished":"2019-01-01T00:00:00.000Z","dateModified":"2020-10-21T05:59:02.931Z","author":{"@type":"Person","name":"laugh12321"},"description":"线性回归线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。"}</script><link rel="canonical" href="http://www.laugh12321.cn/blog/2019/01/01/linear_regression/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-180999768-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-180999768-1');</script><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Laugh</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">首页</a><a class="navbar-item" href="/blog/archives">归档</a><a class="navbar-item" href="/blog/categories">分类</a><a class="navbar-item" href="/blog/tags">标签</a><a class="navbar-item" href="/blog/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/03.jpg" alt="机器学习|线性回归算法详解"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-01-01T00:00:00.000Z" title="2019-01-01T00:00:00.000Z">2019-01-01</time>发表</span><span class="level-item"><time dateTime="2020-10-21T05:59:02.931Z" title="2020-10-21T05:59:02.931Z">2020-10-21</time>更新</span><span class="level-item"><a class="link-muted" href="/blog/categories/Machine-Learning/">Machine Learning</a><span> / </span><a class="link-muted" href="/blog/categories/Machine-Learning/Linear-Regression/">Linear Regression</a></span><span class="level-item">22 分钟读完 (大约3303个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习|线性回归算法详解</h1><div class="content"><h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。</p>
<a id="more"></a>

<h2 id="线性回归介绍"><a href="#线性回归介绍" class="headerlink" title="线性回归介绍"></a>线性回归介绍</h2><p>在了解线性回归之前，我们得先了解分类和回归问题的区别。</p>
<p>首先，回归问题和分类问题一样，训练数据都包含标签，这也是监督学习的特点。而不同之处在于，分类问题预测的是类别，回归问题预测的是连续值。</p>
<p>例如，回归问题往往解决：</p>
<ul>
<li>股票价格预测</li>
<li>房价预测</li>
<li>洪水水位线</li>
</ul>
<p>上面列举的问题，我们需要预测的目标都不是类别，而是实数连续值。</p>
<p><img width='800px' src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/00.png"></img></p>
<p>也就是说，回归问题旨在实现对连续值的预测，例如股票的价格、房价的趋势等。比如，下方展现了一个房屋面积和价格的对应关系图。</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/1.png" alt="此处输入图片的描述"></p>
<p>如上图所示，不同的房屋面积对应着不同的价格。现在，假设我手中有一套房屋想要出售，而出售时就需要预先对房屋进行估值。于是，我想通过上图，也就是其他房屋的售价来判断手中的房产价值是多少。应该怎么做呢？</p>
<p>我采用的方法是这样的。如下图所示，首先画了一条<font color="red">红色</font>的直线，让其大致验证<font color="orange">橙色</font>点分布的延伸趋势。然后，我将已知房屋的面积大小对应到红色直线上，也就是<font color="blue">蓝色</font>点所在位置。最后，再找到蓝色点对应于房屋的价格作为房屋最终的预估价值。</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/2.png" alt="此处输入图片的描述"></p>
<p>在上图呈现的这个过程中，通过找到一条直线去拟合数据点的分布趋势的过程，就是<strong>线性回归</strong>的过程。而线性回归中的「线性」代指线性关系，也就是图中所绘制的红色直线。</p>
<p>此时，你可能心中会有一个疑问。上图中的红色直线是怎么绘制出来的呢？为什么不可以像下图中另外两条绿色虚线，而偏偏要选择红色直线呢？</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/3.png" alt="此处输入图片的描述"></p>
<p>上图中的绿色虚线的确也能反应数据点的分布趋势。所以，找到最适合的那一条红色直线，也是线性回归中需要解决的重要问题之一。</p>
<p>通过上面这个小例子，相信你对线性回归已经有一点点印象了，至少大致明白它能做什么。接下来的内容中，我们将了解线性回归背后的数学原理，以及使用 Python 代码对其实现。</p>
<h2 id="线性回归原理及实现"><a href="#线性回归原理及实现" class="headerlink" title="线性回归原理及实现"></a>线性回归原理及实现</h2><h3 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h3><p>上面针对线性回归的介绍内容中，我们列举了一个房屋面积与房价变化的例子。其中，房屋面积为自变量，而房价则为因变量。另外，我们将只有 1 个自变量的线性拟合过程叫做一元线性回归。</p>
<p>下面，我们就生成一组房屋面积和房价变化的示例数据。<code>x</code> 为房屋面积，单位是平方米; <code>y</code> 为房价，单位是万元。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">56</span>, <span class="number">72</span>, <span class="number">69</span>, <span class="number">88</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">76</span>, <span class="number">79</span>, <span class="number">94</span>, <span class="number">74</span>])</span><br><span class="line">y = np.array([<span class="number">92</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">110</span>, <span class="number">130</span>, <span class="number">99</span>, <span class="number">96</span>, <span class="number">102</span>, <span class="number">105</span>, <span class="number">92</span>])</span><br></pre></td></tr></table></figure>

<p>示例数据由 <code>10</code> 组房屋面积及价格对应组成。接下来，通过 Matplotlib 绘制数据点，<code>x, y</code> 分别对应着横坐标和纵坐标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Area&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Price&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/out_00.png"></p>
<p>正如上面所说，线性回归即通过线性方程（<code>1</code> 次函数）去拟合数据点。那么，我们令函数的表达式为：</p>
<p>$$ y(x, w) = w_0 + w_1x \tag{1} $$</p>
<p>公式（1）是典型的一元一次函数表达式，我们通过组合不同的 $w_0$ 和 $w_1$ 的值得到不同的拟合直线。我们对公式（1）进行代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x, w0, w1</span>):</span></span><br><span class="line">    y = w0 + w1 * x</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>

<p>那么，<strong>哪一条直线最能反应出数据的变化趋势呢？</strong></p>
<p>如下图所示，当我们使用 $y(x, w) = w_0 + w_1x$ 对数据进行拟合时，我们能得到拟合的整体误差，即图中蓝色线段的长度总和。如果某一条直线对应的误差值最小，是不是就代表这条直线最能反映数据点的分布趋势呢？</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/4.png"></p>
<h3 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h3><p>正如上面所说，如果一个数据点为 ($x_{i}$, $y_{i}$)，那么它对应的误差就为:</p>
<p>$$y_{i}-(w_0 + w_1x_{i}) \tag2$$</p>
<p>上面的误差往往也称之为残差。但是在机器学习中，我们更喜欢称作「损失」，即真实值和预测值之间的偏离程度。那么，对应 n 个全部数据点而言，其对应的残差损失总和就为：<br>$$<br>\sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i})) \tag3<br>$$<br>在线性回归中，我们更偏向于使用均方误差作为衡量损失的指标，而均方误差即为残差的平方和。公式如下：</p>
<p>$$<br>\sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2\tag4<br>$$<br>对于公式（4）而言，机器学习中有一个专门的名词，那就是「平方损失函数」。而为了得到拟合参数 $w_0$ 和 $w_1$ 最优的数值，我们的目标就是让公式（4）对应的平方损失函数最小。</p>
<p>同样，我们可以对公式（4）进行代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span>(<span class="params">x, y, w0, w1</span>):</span></span><br><span class="line">    loss = <span class="built_in">sum</span>(np.square(y - (w0 + w1*x)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<h3 id="最小二乘法及代数求解"><a href="#最小二乘法及代数求解" class="headerlink" title="最小二乘法及代数求解"></a>最小二乘法及代数求解</h3><p>最小二乘法是用于求解线性回归拟合参数 $w$ 的一种常用方法。最小二乘法中的「二乘」代表平方，最小二乘也就是最小平方。而这里的平方就是指代上面的平方损失函数。</p>
<p>简单来讲，最小二乘法也就是求解平方损失函数最小值的方法。那么，到底该怎样求解呢？这就需要使用到高等数学中的知识。推导如下：</p>
<p>首先，平方损失函数为：</p>
<p>$$<br>f = \sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2 \tag5<br>$$<br>我们的目标是求取平方损失函数 $min(f)$ 最小时，对应的 $w$。首先求 $f$ 的 <code>1</code> 阶偏导数：</p>
<p>$$<br>\frac{\partial f}{\partial w_{0}}=-2(\sum_{i=1}^{n}{y_i}-nw_{0}-w_{1}\sum_{i=1}^{n}{x_i})\<br>\frac{\partial f}{\partial w_{1}}=-2(\sum_{i=1}^{n}{x_iy_i}-w_{0}\sum_{i=1}^{n}{x_i}-w_{1}\sum_{i=1}^{n}{x_i}^2) \tag6<br>$$<br>然后，我们令 $\frac{\partial f}{\partial w_{0}}=0$ 以及  $\frac{\partial f}{\partial w_{1}}=0$，解得：<br>$$<br>w_{1}=\frac {n\sum_{}^{}{x_iy_i}-\sum_{}^{}{x_i}\sum_{}^{}{y_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2}\<br>w_{0}=\frac {\sum_{}^{}{x_i}^2\sum_{}^{}{y_i}-\sum_{}^{}{x_i}\sum_{}^{}{x_iy_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2}\tag7<br>$$<br>到目前为止，已经求出了平方损失函数最小时对应的 $w$ 参数值，这也就是最佳拟合直线。</p>
<h3 id="线性回归-Python-实现"><a href="#线性回归-Python-实现" class="headerlink" title="线性回归 Python 实现"></a>线性回归 Python 实现</h3><p>我们将公式（7）求解得到 $w$ 的过程进行代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w_calculator</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(x)</span><br><span class="line">    w1 = (n*<span class="built_in">sum</span>(x*y) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(y))/(n*<span class="built_in">sum</span>(x*x) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x))</span><br><span class="line">    w0 = (<span class="built_in">sum</span>(x*x)*<span class="built_in">sum</span>(y) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x*y))/(n*<span class="built_in">sum</span>(x*x)-<span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x))</span><br><span class="line">    <span class="keyword">return</span> w0, w1</span><br></pre></td></tr></table></figure>

<p>于是，可以向函数 <code>w_calculator(x, y)</code> 中传入 <code>x</code> 和 <code>y</code> 得到 $w_0$ 和 $w_1$ 的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w_calculator(x, y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">41.33509168550616</span>, <span class="number">0.7545842753077117</span>)</span><br></pre></td></tr></table></figure>

<p>当然，我们也可以求得此时对应的平方损失的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w0 = w_calculator(x, y)[<span class="number">0</span>]</span><br><span class="line">w1 = w_calculator(x, y)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">square_loss(x, y, w0, w1)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">447.69153479025357</span></span><br></pre></td></tr></table></figure>

<p>接下来，我们尝试将拟合得到的直线绘制到原图中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_temp = np.linspace(<span class="number">50</span>,<span class="number">120</span>,<span class="number">100</span>) <span class="comment"># 绘制直线生成的临时点</span></span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x_temp, x_temp*w1 + w0, <span class="string">&#x27;r&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/out_01.png"></p>
<p>从上图可以看出，拟合的效果还是不错的。那么，如果你手中有一套 <code>150</code> 平米的房产想售卖，获得预估报价就只需要带入方程即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(<span class="number">150</span>, w0, w1)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">154.5227329816629</span></span><br></pre></td></tr></table></figure>

<p>这里得到的预估售价约为 <code>154</code> 万元。</p>
<h3 id="线性回归-scikit-learn-实现"><a href="#线性回归-scikit-learn-实现" class="headerlink" title="线性回归 scikit-learn 实现"></a>线性回归 scikit-learn 实现</h3><p>上面的内容中，我们学习了什么是最小二乘法，以及使用 Python 对最小二乘线性回归进行了完整实现。那么，我们如何利用机器学习开源模块 scikit-learn 实现最小二乘线性回归方法呢？</p>
<p>使用 scikit-learn 实现线性回归的过程会简单很多，这里要用到 <code>LinearRegression()</code> 类。看一下其中的参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.LinearRegression(fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, copy_X=<span class="literal">True</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>fit_intercept</code>: 默认为 True，计算截距项。</li>
<li><code>normalize</code>: 默认为 False，不针对数据进行标准化处理。</li>
<li><code>copy_X</code>: 默认为 True，即使用数据的副本进行操作，防止影响原数据。</li>
<li><code>n_jobs</code>: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;scikit-learn 线性回归拟合</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(x.reshape(<span class="built_in">len</span>(x),<span class="number">1</span>), y) <span class="comment"># 训练, reshape 操作把数据处理成 fit 能接受的形状</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到模型拟合参数</span></span><br><span class="line">model.intercept_, model.coef_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">41.33509168550615</span>, array([<span class="number">0.75458428</span>]))</span><br></pre></td></tr></table></figure>

<p>我们通过 <code>model.intercept_</code> 得到拟合的截距项，即上面的 $w_{0}$，通过 <code>model.coef_</code> 得到 $x$ 的系数，即上面的 $w_{1}$。对比发现，结果是<strong>完全一致</strong>的。</p>
<p>同样，我们可以预测 <code>150</code> 平米房产的价格：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict([[<span class="number">150</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">154.52273298</span>])</span><br></pre></td></tr></table></figure>

<p>可以看到，这里得出的结果和自行实现计算结果一致。</p>
<h2 id="最小二乘法的矩阵推导及实现"><a href="#最小二乘法的矩阵推导及实现" class="headerlink" title="最小二乘法的矩阵推导及实现"></a>最小二乘法的矩阵推导及实现</h2><p>学习完上面的内容，相信你已经了解了什么是最小二乘法，以及如何使用最小二乘法进行线性回归拟合。上面，实验采用了求偏导数的方法，并通过代数求解找到了最佳拟合参数 <code>w</code> 的值。</p>
<p>这里，我们尝试另外一种方法，即通过矩阵的变换来计算参数 <code>w</code> 。推导如下：</p>
<p>首先，一元线性函数的表达式为 $ y(x, w) = w_0 + w_1x$，表达成矩阵形式为：</p>
<p>$$\begin{bmatrix}1, x_{1} \ 1, x_{2} \ … \ 1, x_{9} \ 1, x_{10} \end{bmatrix} * \begin{bmatrix}w_{0} \ w_{1} \end{bmatrix} = \begin{bmatrix}y_{1} \ y_{2} \ … \ y_{9} \ y_{10} \end{bmatrix} \Rightarrow \begin{bmatrix}1, 56 \ 1, 72 \ … \ 1, 94 \ 1, 74 \end{bmatrix}* \begin{bmatrix}w_{0} \ w_{1} \end{bmatrix}= \begin{bmatrix}92 \ 102 \ … \ 105 \ 92 \end{bmatrix} \tag{8a}$$</p>
<p>即：<br>$$ y(x, w) = XW \tag{8b} $$</p>
<p>（8）式中，$W$ 为 $\begin{bmatrix}w_{0}<br>\ w_{1}<br>\end{bmatrix}$，而 $X$ 则是 $\begin{bmatrix}1, x_{1}<br>\ 1, x_{2}<br>\ …<br>\ 1, x_{9}<br>\ 1, x_{10}<br>\end{bmatrix}$ 矩阵。然后，平方损失函数为：<br>$$<br>f = \sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2 =(y-XW)^T(y-XW)\tag{9}<br>$$<br>此时，对矩阵求偏导数（超纲）得到：</p>
<p>$$<br>\frac{\partial f}{\partial W}=2<em>X^TXW-2</em>X^Ty=0 \tag{10}<br>$$<br>当矩阵 $X^TX$ 满秩（不满秩后面的实验中会讨论）时，$(X^TX)^{-1}X^TX=E$，且 $EW=W$。所以，$(X^TX)^{-1}X^TXW=(X^TX)^{-1}X^Ty$。最终得到：<br>$$<br>W=(X^TX)^{-1}X^Ty \tag{11}<br>$$<br>我们可以针对公式（11）进行代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w_matrix</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    w = (x.T * x).I * x.T * y</span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>

<p>我们针对原 <code>x</code> 数据添加截距项系数 <code>1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.matrix([[<span class="number">1</span>,<span class="number">56</span>],[<span class="number">1</span>,<span class="number">72</span>],[<span class="number">1</span>,<span class="number">69</span>],[<span class="number">1</span>,<span class="number">88</span>],[<span class="number">1</span>,<span class="number">102</span>],[<span class="number">1</span>,<span class="number">86</span>],[<span class="number">1</span>,<span class="number">76</span>],[<span class="number">1</span>,<span class="number">79</span>],[<span class="number">1</span>,<span class="number">94</span>],[<span class="number">1</span>,<span class="number">74</span>]])</span><br><span class="line">y = np.matrix([<span class="number">92</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">110</span>, <span class="number">130</span>, <span class="number">99</span>, <span class="number">96</span>, <span class="number">102</span>, <span class="number">105</span>, <span class="number">92</span>])</span><br><span class="line"></span><br><span class="line">w_matrix(x, y.reshape(<span class="number">10</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">matrix([[<span class="number">41.33509169</span>],</span><br><span class="line">        [ <span class="number">0.75458428</span>]])</span><br></pre></td></tr></table></figure>

<p>可以看到，矩阵计算结果和前面的代数计算结果一致。你可能会有疑问，那就是为什么要采用矩阵变换的方式计算？一开始学习的代数计算方法不好吗？</p>
<p>其实，并不是说代数计算方式不好，在小数据集下二者运算效率接近。但是，当我们面对十万或百万规模的数据时，矩阵计算的效率就会高很多，这就是为什么要学习矩阵计算的原因。</p>
<p><strong>参考</strong>：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">最小二乘法-维基百科</a></li>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8">线性回归-维基百科</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/37031188">知乎问答-最小二乘法的本质是什么？</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>机器学习|线性回归算法详解</p><p><a href="http://www.laugh12321.cn/blog/2019/01/01/linear_regression/">http://www.laugh12321.cn/blog/2019/01/01/linear_regression/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Laugh</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-01-01</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-10-21</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Linear-Regression/">Linear Regression</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5f902f1aa809f500123872bf&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/alipay-reward-image.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/wechat-reward-image.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/01/02/evaluation_index_with_linear_regression/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">机器学习|线性回归三大评价指标实现『MAE, MSE, MAPE』</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2018/12/24/update_next_for_jekyll/"><span class="level-item">Jekyll + NexT + GitHub Pages 主题深度优化</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.css"><script src="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "8cf575efe1fad1dfbf35523e7ca10821",
            repo: "blog",
            owner: "laugh12321",
            clientID: "f831b7db637971edc8c9",
            clientSecret: "9767a6cb27b89b67e2875e8915671499b75a6436",
            admin: ["laugh12321"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars0.githubusercontent.com/u/38065710?s=400&amp;u=80ee4d360562451484188e6f1677fa442720891b&amp;v=4" alt="Laugh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Laugh</p><p class="is-size-6 is-block">The life I want, there is no shortcut.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Northern Hemisphere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/blog/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/blog/categories"><p class="title">15</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/blog/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/laugh12321" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/blog/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#线性回归"><span class="level-left"><span class="level-item">1</span><span class="level-item">线性回归</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#线性回归介绍"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">线性回归介绍</span></span></a></li><li><a class="level is-mobile" href="#线性回归原理及实现"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">线性回归原理及实现</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#一元线性回归"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">一元线性回归</span></span></a></li><li><a class="level is-mobile" href="#平方损失函数"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">平方损失函数</span></span></a></li><li><a class="level is-mobile" href="#最小二乘法及代数求解"><span class="level-left"><span class="level-item">1.2.3</span><span class="level-item">最小二乘法及代数求解</span></span></a></li><li><a class="level is-mobile" href="#线性回归-Python-实现"><span class="level-left"><span class="level-item">1.2.4</span><span class="level-item">线性回归 Python 实现</span></span></a></li><li><a class="level is-mobile" href="#线性回归-scikit-learn-实现"><span class="level-left"><span class="level-item">1.2.5</span><span class="level-item">线性回归 scikit-learn 实现</span></span></a></li></ul></li><li><a class="level is-mobile" href="#最小二乘法的矩阵推导及实现"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">最小二乘法的矩阵推导及实现</span></span></a></li></ul></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Algorithm/Sorting-algorithm/"><span class="level-start"><span class="level-item">Sorting algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Decision-Tree/"><span class="level-start"><span class="level-item">Decision Tree</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/K-Nearest-Neighbors/"><span class="level-start"><span class="level-item">K-Nearest Neighbors</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Linear-Regression/"><span class="level-start"><span class="level-item">Linear Regression</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Naive-Bayes/"><span class="level-start"><span class="level-item">Naive Bayes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Polynomial-Regression/"><span class="level-start"><span class="level-item">Polynomial Regression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Support-Vector-Machine/"><span class="level-start"><span class="level-item">Support Vector Machine</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Ubuntu/"><span class="level-start"><span class="level-item">Ubuntu</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/"><span class="level-start"><span class="level-item">ECS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/Seafile/"><span class="level-start"><span class="level-item">Seafile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Blog/"><span class="tag">Blog</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ECS/"><span class="tag">ECS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GitHub-Pages/"><span class="tag">GitHub Pages</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Jekyll/"><span class="tag">Jekyll</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Nearest-Neighbors/"><span class="tag">K-Nearest Neighbors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAE/"><span class="tag">MAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAPE/"><span class="tag">MAPE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MSE/"><span class="tag">MSE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Next/"><span class="tag">Next</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Seafile/"><span class="tag">Seafile</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Sorting-algorithm/"><span class="tag">Sorting algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Support-Vector-Machine/"><span class="tag">Support Vector Machine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Laugh</a><p class="is-size-7"><span>&copy; 2020 Laugh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>