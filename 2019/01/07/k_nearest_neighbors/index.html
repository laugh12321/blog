<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>机器学习| K-近邻算法详解 - Laugh&#039;s blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Laugh&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Laugh&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="最近邻算法介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 $x$，在训练集中找到与 $x$ 最相似的训练样本 $y$，用 $y$ 的样本对应的类别作为未知类别数据 $x$ 的类别，从而达到分类的效果。"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习| K-近邻算法详解"><meta property="og:url" content="http://www.laugh12321.cn/blog/2019/01/07/k_nearest_neighbors/"><meta property="og:site_name" content="Laugh&#039;s blog"><meta property="og:description" content="最近邻算法介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 $x$，在训练集中找到与 $x$ 最相似的训练样本 $y$，用 $y$ 的样本对应的类别作为未知类别数据 $x$ 的类别，从而达到分类的效果。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/06.jpg"><meta property="article:published_time" content="2019-01-07T00:00:00.000Z"><meta property="article:modified_time" content="2020-10-21T05:59:02.931Z"><meta property="article:author" content="Laugh"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="K-Nearest Neighbors"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/06.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.laugh12321.cn/blog"},"headline":"Laugh's blog","image":["https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/photo/06.jpg"],"datePublished":"2019-01-07T00:00:00.000Z","dateModified":"2020-10-21T05:59:02.931Z","author":{"@type":"Person","name":"laugh12321"},"description":"最近邻算法介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 $x$，在训练集中找到与 $x$ 最相似的训练样本 $y$，用 $y$ 的样本对应的类别作为未知类别数据 $x$ 的类别，从而达到分类的效果。"}</script><link rel="canonical" href="http://www.laugh12321.cn/blog/2019/01/07/k_nearest_neighbors/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-180999768-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-180999768-1');</script><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Laugh</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">首页</a><a class="navbar-item" href="/blog/archives">归档</a><a class="navbar-item" href="/blog/categories">分类</a><a class="navbar-item" href="/blog/tags">标签</a><a class="navbar-item" href="/blog/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/06.jpg" alt="机器学习| K-近邻算法详解"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-01-07T00:00:00.000Z" title="2019-01-07T00:00:00.000Z">2019-01-07</time>发表</span><span class="level-item"><time dateTime="2020-10-21T05:59:02.931Z" title="2020-10-21T05:59:02.931Z">2020-10-21</time>更新</span><span class="level-item"><a class="link-muted" href="/blog/categories/Machine-Learning/">Machine Learning</a><span> / </span><a class="link-muted" href="/blog/categories/Machine-Learning/K-Nearest-Neighbors/">K-Nearest Neighbors</a></span><span class="level-item">18 分钟读完 (大约2682个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习| K-近邻算法详解</h1><div class="content"><h3 id="最近邻算法"><a href="#最近邻算法" class="headerlink" title="最近邻算法"></a>最近邻算法</h3><p>介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 $x$，在训练集中找到与 $x$ 最相似的训练样本 $y$，用 $y$ 的样本对应的类别作为未知类别数据 $x$ 的类别，从而达到分类的效果。</p>
<a id="more"></a>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/1.png"></p>
<p>如上图所示，通过计算数据 $X_{u}$ （未知样本）和已知类别 ${\omega_{1},\omega_{2},\omega_{3}}$ （已知样本）之间的距离，判断 $X_{u}$ 与不同训练集的相似度，最终判断 $X_{u}$ 的类别。显然，这里将<font color="green">绿色未知样本</font>类别判定与<font color="red">红色已知样本</font>类别相同较为合适。</p>
<h3 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K-近邻算法"></a>K-近邻算法</h3><p>K-近邻（K-Nearest Neighbors，简称：KNN）算法是最近邻（NN）算法的一个推广，也是机器学习分类算法中最简单的方法之一。KNN 算法的核心思想和最近邻算法思想相似，都是通过寻找和未知样本相似的类别进行分类。但 NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好。</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/2.png"></p>
<p>如上图所示，对于未知测试样本(图中<font color='red'> ？</font>所示)采用 KNN 算法进行分类，首先计算未知样本和训练样本之间的相似度，找出最近 K 个相邻样本（在图中 K 值为 3，圈定距离 ？最近的 3 个样本），再根据最近的 K 个样本最终判断未知样本的类别。</p>
<h2 id="K-近邻算法实现"><a href="#K-近邻算法实现" class="headerlink" title="K-近邻算法实现"></a>K-近邻算法实现</h2><p>KNN 算法在理论上已经非常成熟，其简单、易于理解的思想以及良好的分类准确度使得 KNN 算法应用非常广泛。算法的具体流程主要是以下的 4 个步骤：</p>
<ol>
<li><strong>数据准备</strong>：通过数据清洗，数据处理，将每条数据整理成向量。  </li>
<li><strong>计算距离</strong>：计算测试数据与训练数据之间的距离。  </li>
<li><strong>寻找邻居</strong>：找到与测试数据距离最近的 K 个训练数据样本。  </li>
<li><strong>决策分类</strong>：根据决策规则，从 K 个邻居得到测试数据的类别。</li>
</ol>
<p><img width='900px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/3.png"></img></p>
<h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><p>下面，我们尝试完成一个 KNN 分类流程。首先，生成一组示例数据，共包含 2 个类别（<code>A</code>和<code>B</code>），其中每一条数据包含两个特征（<code>x</code>和<code>y</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;生成示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span>():</span></span><br><span class="line">    features = np.array(</span><br><span class="line">        [[<span class="number">2.88</span>, <span class="number">3.05</span>], [<span class="number">3.1</span>, <span class="number">2.45</span>], [<span class="number">3.05</span>, <span class="number">2.8</span>], [<span class="number">2.9</span>, <span class="number">2.7</span>], [<span class="number">2.75</span>, <span class="number">3.4</span>],</span><br><span class="line">         [<span class="number">3.23</span>, <span class="number">2.9</span>], [<span class="number">3.2</span>, <span class="number">3.75</span>], [<span class="number">3.5</span>, <span class="number">2.9</span>], [<span class="number">3.65</span>, <span class="number">3.6</span>], [<span class="number">3.35</span>, <span class="number">3.3</span>]])</span><br><span class="line">    labels = [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>

<p>然后，我们尝试加载并打印这些数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;打印示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">features, labels = create_data()</span><br><span class="line">print(<span class="string">&#x27;features: \n&#x27;</span>, features)</span><br><span class="line">print(<span class="string">&#x27;labels: \n&#x27;</span>, labels)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">features: </span><br><span class="line"> [[<span class="number">2.88</span> <span class="number">3.05</span>]</span><br><span class="line"> [<span class="number">3.1</span>  <span class="number">2.45</span>]</span><br><span class="line"> [<span class="number">3.05</span> <span class="number">2.8</span> ]</span><br><span class="line"> [<span class="number">2.9</span>  <span class="number">2.7</span> ]</span><br><span class="line"> [<span class="number">2.75</span> <span class="number">3.4</span> ]</span><br><span class="line"> [<span class="number">3.23</span> <span class="number">2.9</span> ]</span><br><span class="line"> [<span class="number">3.2</span>  <span class="number">2.75</span>]</span><br><span class="line"> [<span class="number">3.5</span>  <span class="number">2.9</span> ]</span><br><span class="line"> [<span class="number">3.65</span> <span class="number">3.6</span> ]</span><br><span class="line"> [<span class="number">3.35</span> <span class="number">3.3</span> ]]</span><br><span class="line">labels: </span><br><span class="line"> [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>为了更直观地理解数据，接下来用 Matplotlib 下的 pyplot 包来对数据集进行可视化。为了代码的简洁，我们使用了 <code>map</code> 函数和 <code>lamda</code> 表达式对数据进行处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;示例数据绘图</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">plt.ylim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">x_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], features))  <span class="comment"># 返回每个数据的x特征值</span></span><br><span class="line">y_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], features))</span><br><span class="line">plt.scatter(x_feature[:<span class="number">5</span>], y_feature[:<span class="number">5</span>], c=<span class="string">&quot;b&quot;</span>)  <span class="comment"># 在画布上绘画出&quot;A&quot;类标签的数据点</span></span><br><span class="line">plt.scatter(x_feature[<span class="number">5</span>:], y_feature[<span class="number">5</span>:], c=<span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.scatter([<span class="number">3.18</span>], [<span class="number">3.15</span>], c=<span class="string">&quot;r&quot;</span>, marker=<span class="string">&quot;x&quot;</span>)  <span class="comment"># 待测试点的坐标为 [3.1，3.2]</span></span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/out_00.png"></p>
<p>由上图所示，标签为 <code>A</code>（蓝色圆点）的数据在画布的左下角位置，而标签为 <code>B</code>（绿色圆点）的数据在画布的右上角位置，通过图像可以清楚看出不同标签数据的分布情况。其中<font color="red">红色 x 点</font>即表示需预测类别的测试数据。</p>
<h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>在计算两个样本间的相似度时，可以通过计算样本之间特征值的距离进行表示。若两个样本距离值越大（相距越远），则表示两个样本相似度低，相反，若两个样本值越小（相距越近），则表示两个样本相似度越高。</p>
<p>计算距离的方法有很多，本实验介绍两个最为常用的距离公式：<strong>曼哈顿距离</strong>和<strong>欧式距离</strong>。这两个距离的计算图示如下：</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/4.png"></p>
<h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><p>曼哈顿距离又称马氏距离，出租车距离，是计算距离最简单的方式之一。公式如下：</p>
<p>$$<br>d_{man}=\sum_{i=1}^{N}\left | X_{i}-Y_{i} \right |<br>$$<br>其中： </p>
<ul>
<li>$X$,$Y$：两个数据点</li>
<li>$N$：每个数据中有 $N$ 个特征值</li>
<li>$X_{i}$ ：数据 $X$ 的第 $i$ 个特征值  </li>
</ul>
<p>公式表示为将两个数据 $X$ 和 $Y$ 中每一个对应特征值之间差值的绝对值，再求和，便得到曼哈顿距离。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;曼哈顿距离计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_man</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    d = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(x - y))</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">3.1</span>, <span class="number">3.2</span>])</span><br><span class="line">print(<span class="string">&quot;x:&quot;</span>, x)</span><br><span class="line">y = np.array([<span class="number">2.5</span>, <span class="number">2.8</span>])</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>, y)</span><br><span class="line">d_man = d_man(x, y)</span><br><span class="line">print(d_man)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x: [<span class="number">3.1</span> <span class="number">3.2</span>]</span><br><span class="line">y: [<span class="number">2.5</span> <span class="number">2.8</span>]</span><br><span class="line"><span class="number">1.0000000000000004</span></span><br></pre></td></tr></table></figure>

<h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><p>欧式距离源自 $N$ 维欧氏空间中两点之间的距离公式。表达式如下:</p>
<p>$$<br>d_{euc}= \sqrt{\sum_{i=1}^{N}(X_{i}-Y_{i})^{2}}<br>$$<br>其中：</p>
<ul>
<li>$X$, $Y$ ：两个数据点</li>
<li>$N$：每个数据中有 $N$ 个特征值</li>
<li>$X_{i}$ ：数据 $X$ 的第 $i$ 个特征值  </li>
</ul>
<p>公式表示为将两个数据 X 和 Y 中的每一个对应特征值之间差值的平方，再求和，最后开平方，便是欧式距离。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;欧氏距离的计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_euc</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    d = np.sqrt(np.<span class="built_in">sum</span>(np.square(x - y)))</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.random.random(<span class="number">10</span>)  <span class="comment"># 随机生成10个数的数组作为x特征的值</span></span><br><span class="line">print(<span class="string">&quot;x:&quot;</span>, x)</span><br><span class="line">y = np.random.random(<span class="number">10</span>)</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>, y)</span><br><span class="line">distance_euc = d_euc(x, y)</span><br><span class="line">print(distance_euc)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x: [<span class="number">0.10725148</span> <span class="number">0.78394185</span> <span class="number">0.85568109</span> <span class="number">0.5774587</span>  <span class="number">0.96974919</span> <span class="number">0.79467734</span></span><br><span class="line"> <span class="number">0.26009361</span> <span class="number">0.93204</span>    <span class="number">0.08424034</span> <span class="number">0.16970618</span>]</span><br><span class="line">y: [<span class="number">0.88013554</span> <span class="number">0.5943479</span>  <span class="number">0.31357311</span> <span class="number">0.20830397</span> <span class="number">0.20686205</span> <span class="number">0.9475627</span></span><br><span class="line"> <span class="number">0.61453761</span> <span class="number">0.27882129</span> <span class="number">0.61228018</span> <span class="number">0.75968914</span>]</span><br><span class="line"><span class="number">1.6876178018976438</span></span><br></pre></td></tr></table></figure>

<h3 id="决策规则"><a href="#决策规则" class="headerlink" title="决策规则"></a>决策规则</h3><p>在得到测试样本和训练样本之间的相似度后，通过相似度的排名，可以得到每一个测试样本的 K 个相邻的训练样本，那如何通过 K 个邻居来判断测试样本的最终类别呢？可以根据数据特征对决策规则进行选取，不同的决策规则会产生不同的预测结果，最常用的决策规则是：  </p>
<ul>
<li><strong>多数表决法</strong>：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。  </li>
<li><strong>加权表决法</strong>：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。</li>
</ul>
<p>这里推荐使用多数表决法，这种方法更加简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;多数表决法</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majority_voting</span>(<span class="params">class_count</span>):</span></span><br><span class="line">    sorted_class_count = <span class="built_in">sorted</span>(</span><br><span class="line">        class_count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_class_count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">2</span>, <span class="string">&quot;C&quot;</span>: <span class="number">6</span>, <span class="string">&quot;D&quot;</span>: <span class="number">5</span>&#125;</span><br><span class="line">majority_voting(arr)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;C&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;D&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure>

<p>在多数表决法的定义中，我们导入了 <code>operater</code> 计算模块，目的是对字典类型结构排序。可以从结果中看出函数返回的结果为票数最多的 <code>C</code>，得票为 <code>6</code> 次。</p>
<h3 id="KNN-算法实现"><a href="#KNN-算法实现" class="headerlink" title="KNN 算法实现"></a>KNN 算法实现</h3><p>在学习完以上的各个步骤之后，KNN 算法也逐渐被勾勒出来。以下就是对 KNN 算法的完整实现，本次的距离计算采用<strong>欧式距离</strong>，分类的决策规则为<strong>多数表决法</strong>，定义函数 <code>knn_classify()</code>，其中函数的参数包括：</p>
<ul>
<li><code>test_data</code>：用于分类的输入向量。</li>
<li><code>train_data</code>：输入的训练样本集。</li>
<li><code>labels</code>：样本数据的类标签向量。</li>
<li><code>k</code>：用于选择最近邻居的数目。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;KNN 方法完整实现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn_classify</span>(<span class="params">test_data, train_data, labels, k</span>):</span></span><br><span class="line">    distances = np.array([])  <span class="comment"># 创建一个空的数组用于存放距离</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each_data <span class="keyword">in</span> train_data:  <span class="comment"># 使用欧式距离计算数据相似度</span></span><br><span class="line">        d = d_euc(test_data, each_data)</span><br><span class="line">        distances = np.append(distances, d)</span><br><span class="line"></span><br><span class="line">    sorted_distance_index = distances.argsort()  <span class="comment"># 获取按距离大小排序后的索引</span></span><br><span class="line">    sorted_distance = np.sort(distances)</span><br><span class="line">    r = (sorted_distance[k]+sorted_distance[k<span class="number">-1</span>])/<span class="number">2</span>  <span class="comment"># 计算</span></span><br><span class="line"></span><br><span class="line">    class_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):  <span class="comment"># 多数表决</span></span><br><span class="line">        vote_label = labels[sorted_distance_index[i]]</span><br><span class="line">        class_count[vote_label] = class_count.get(vote_label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    final_label = majority_voting(class_count)</span><br><span class="line">    <span class="keyword">return</span> final_label, r</span><br></pre></td></tr></table></figure>

<h3 id="分类预测"><a href="#分类预测" class="headerlink" title="分类预测"></a>分类预测</h3><p>在实现 KNN 算法之后，接下来就可以对我们未知数据<code>[3.18,3.15]</code>开始分类,假定我们 K 值初始设定为 5，让我们看看分类的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_data = np.array([<span class="number">3.18</span>, <span class="number">3.15</span>])</span><br><span class="line">final_label, r = knn_classify(test_data, features, labels, <span class="number">5</span>)</span><br><span class="line">final_label</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;B&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure>

<h3 id="可视化展示"><a href="#可视化展示" class="headerlink" title="可视化展示"></a>可视化展示</h3><p>在对数据 <code>[3.18,3.15]</code> 实现分类之后，接下来我们同样用画图的方式形象化展示 KNN 算法决策方式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">circle</span>(<span class="params">r, a, b</span>):</span>  <span class="comment"># 为了画出圆，这里采用极坐标的方式对圆进行表示 ：x=r*cosθ，y=r*sinθ。</span></span><br><span class="line">    theta = np.arange(<span class="number">0</span>, <span class="number">2</span>*np.pi, <span class="number">0.01</span>)</span><br><span class="line">    x = a+r * np.cos(theta)</span><br><span class="line">    y = b+r * np.sin(theta)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k_circle_x, k_circle_y = circle(r, <span class="number">3.18</span>, <span class="number">3.15</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">plt.ylim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">x_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], features))  <span class="comment"># 返回每个数据的x特征值</span></span><br><span class="line">y_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], features))</span><br><span class="line">plt.scatter(x_feature[:<span class="number">5</span>], y_feature[:<span class="number">5</span>], c=<span class="string">&quot;b&quot;</span>)  <span class="comment"># 在画布上绘画出&quot;A&quot;类标签的数据点</span></span><br><span class="line">plt.scatter(x_feature[<span class="number">5</span>:], y_feature[<span class="number">5</span>:], c=<span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.scatter([<span class="number">3.18</span>], [<span class="number">3.15</span>], c=<span class="string">&quot;r&quot;</span>, marker=<span class="string">&quot;x&quot;</span>)  <span class="comment"># 待测试点的坐标为 [3.1，3.2]</span></span><br><span class="line">plt.plot(k_circle_x, k_circle_y)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/out_01.png"></p>
<p>如图所示，当我们 <code>K</code> 值为 <code>5</code> 时，与测试样本距离最近的 <code>5</code> 个训练数据（如蓝色圆圈所示）中属于 <code>B</code> 类的有 <code>3</code> 个，属于 <code>A</code> 类的有 <code>2</code> 个，根据多数表决法决策出测试样本的数据为 <code>B</code> 类。</p>
<p>通过尝试不同的 K 值我们会发现，不同的 K 值预测出不同的结果。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>机器学习| K-近邻算法详解</p><p><a href="http://www.laugh12321.cn/blog/2019/01/07/k_nearest_neighbors/">http://www.laugh12321.cn/blog/2019/01/07/k_nearest_neighbors/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Laugh</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-01-07</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-10-21</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/K-Nearest-Neighbors/">K-Nearest Neighbors</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5f902f1aa809f500123872bf&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/alipay-reward-image.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/wechat-reward-image.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/01/19/naive_bayes_basic/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">机器学习| 朴素贝叶斯详解</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/01/04/polynomial_regression/"><span class="level-item">机器学习|多项式回归算法详解</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.css"><script src="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "475ac7717c567208e82cb6a43a964864",
            repo: "blog",
            owner: "laugh12321",
            clientID: "f831b7db637971edc8c9",
            clientSecret: "9767a6cb27b89b67e2875e8915671499b75a6436",
            admin: ["laugh12321"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars0.githubusercontent.com/u/38065710?s=400&amp;u=80ee4d360562451484188e6f1677fa442720891b&amp;v=4" alt="Laugh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Laugh</p><p class="is-size-6 is-block">The life I want, there is no shortcut.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Northern Hemisphere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/blog/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/blog/categories"><p class="title">15</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/blog/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/laugh12321" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/blog/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#最近邻算法"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">最近邻算法</span></span></a></li><li><a class="level is-mobile" href="#K-近邻算法"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">K-近邻算法</span></span></a></li></ul><li><a class="level is-mobile" href="#K-近邻算法实现"><span class="level-left"><span class="level-item">2</span><span class="level-item">K-近邻算法实现</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#数据生成"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">数据生成</span></span></a></li><li><a class="level is-mobile" href="#距离度量"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">距离度量</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#曼哈顿距离"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">曼哈顿距离</span></span></a></li><li><a class="level is-mobile" href="#欧式距离"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">欧式距离</span></span></a></li></ul></li><li><a class="level is-mobile" href="#决策规则"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">决策规则</span></span></a></li><li><a class="level is-mobile" href="#KNN-算法实现"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">KNN 算法实现</span></span></a></li><li><a class="level is-mobile" href="#分类预测"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">分类预测</span></span></a></li><li><a class="level is-mobile" href="#可视化展示"><span class="level-left"><span class="level-item">2.6</span><span class="level-item">可视化展示</span></span></a></li></ul></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Algorithm/Sorting-algorithm/"><span class="level-start"><span class="level-item">Sorting algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Decision-Tree/"><span class="level-start"><span class="level-item">Decision Tree</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/K-Nearest-Neighbors/"><span class="level-start"><span class="level-item">K-Nearest Neighbors</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Linear-Regression/"><span class="level-start"><span class="level-item">Linear Regression</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Naive-Bayes/"><span class="level-start"><span class="level-item">Naive Bayes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Polynomial-Regression/"><span class="level-start"><span class="level-item">Polynomial Regression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Support-Vector-Machine/"><span class="level-start"><span class="level-item">Support Vector Machine</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Ubuntu/"><span class="level-start"><span class="level-item">Ubuntu</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/"><span class="level-start"><span class="level-item">ECS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/Seafile/"><span class="level-start"><span class="level-item">Seafile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Blog/"><span class="tag">Blog</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ECS/"><span class="tag">ECS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GitHub-Pages/"><span class="tag">GitHub Pages</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Jekyll/"><span class="tag">Jekyll</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Nearest-Neighbors/"><span class="tag">K-Nearest Neighbors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAE/"><span class="tag">MAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAPE/"><span class="tag">MAPE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MSE/"><span class="tag">MSE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Next/"><span class="tag">Next</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Seafile/"><span class="tag">Seafile</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Sorting-algorithm/"><span class="tag">Sorting algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Support-Vector-Machine/"><span class="tag">Support Vector Machine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Laugh</a><p class="is-size-7"><span>&copy; 2020 Laugh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>