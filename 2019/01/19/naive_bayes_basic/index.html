<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>机器学习| 朴素贝叶斯详解 - Laugh&#039;s blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Laugh&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Laugh&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习| 朴素贝叶斯详解"><meta property="og:url" content="http://www.laugh12321.cn/blog/2019/01/19/naive_bayes_basic/"><meta property="og:site_name" content="Laugh&#039;s blog"><meta property="og:description" content="在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/Aralsk%2C%20Kazakhstan.jpg"><meta property="article:published_time" content="2019-01-19T00:00:00.000Z"><meta property="article:modified_time" content="2020-10-21T05:59:02.931Z"><meta property="article:author" content="Laugh"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Naive Bayes"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/Aralsk%2C%20Kazakhstan.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.laugh12321.cn/blog"},"headline":"Laugh's blog","image":["https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/photo/Aralsk%2C%20Kazakhstan.jpg"],"datePublished":"2019-01-19T00:00:00.000Z","dateModified":"2020-10-21T05:59:02.931Z","author":{"@type":"Person","name":"laugh12321"},"description":"在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。"}</script><link rel="canonical" href="http://www.laugh12321.cn/blog/2019/01/19/naive_bayes_basic/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-180999768-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-180999768-1');</script><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><link rel="alternate" href="/blog/atom.xml" title="Laugh's blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Laugh</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">首页</a><a class="navbar-item" href="/blog/archives">归档</a><a class="navbar-item" href="/blog/categories">分类</a><a class="navbar-item" href="/blog/tags">标签</a><a class="navbar-item" href="/blog/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/Aralsk%2C%20Kazakhstan.jpg" alt="机器学习| 朴素贝叶斯详解"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-01-19T00:00:00.000Z" title="2019-01-19T00:00:00.000Z">2019-01-19</time>发表</span><span class="level-item"><time dateTime="2020-10-21T05:59:02.931Z" title="2020-10-21T05:59:02.931Z">2020-10-21</time>更新</span><span class="level-item"><a class="link-muted" href="/blog/categories/Machine-Learning/">Machine Learning</a><span> / </span><a class="link-muted" href="/blog/categories/Machine-Learning/Naive-Bayes/">Naive Bayes</a></span><span class="level-item">29 分钟读完 (大约4275个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习| 朴素贝叶斯详解</h1><div class="content"><p>在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。</p>
<a id="more"></a>

<h2 id="朴素贝叶斯基础"><a href="#朴素贝叶斯基础" class="headerlink" title="朴素贝叶斯基础"></a>朴素贝叶斯基础</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>朴素贝叶斯的数学理论基础源于概率论。所以，在学习朴素贝叶斯算法之前，首先对其中涉及到的概率论知识做简要讲解。</p>
<h4 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h4><p>条件概率就是指事件 $A$ 在另外一个事件 $B$ 已经发生条件下的概率。如图所示 ：</p>
<img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/1.png" width="370" height="370">

<p>其中： </p>
<ul>
<li>$P(A)$ 表示 $A$ 事件发生的概率。</li>
<li>$P(B)$ 表示 $B$ 事件发生的概率。</li>
<li>$P(AB)$ 表示 $A, B$ 事件同时发生的概率。 </li>
</ul>
<p>而最终计算得到的 $P(A \mid B)$ 便是条件概率，表示在 $B$ 事件发生的情况下 $A$ 事件发生的概率。</p>
<h4 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h4><p>上面提到了条件概率的基本概念，那么当知道事件 $B$ 发生的情况下事件 $A$ 发生的概率 $P(A \mid B)$，如何求 $P(B \mid A)$ 呢？贝叶斯定理应运而生。根据条件概率公式可以得到:</p>
<p>$$<br>P(B \mid A)=\frac{P(AB)}{P(A)} \tag1<br>$$<br>而同样通过条件概率公式可以得到：</p>
<p>$$<br>P(AB)=P(A \mid B)*P(B) \tag2<br>$$</p>
<p>将 (2) 式带入 (1) 式便可得到完整的贝叶斯定理：</p>
<p>$$<br>P(B \mid A)=\frac{P(AB)}{P(A)}=\frac{P(A \mid B)*P(B)}{P(A)} \tag{3}<br>$$<br>以下，通过一张图来完整且形象的展示条件概率和贝叶斯定理的原理。</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/2.png"></p>
<h4 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a>先验概率</h4><p>先验概率（Prior Probability）指的是根据以往经验和分析得到的概率。例如以上公式中的 $P(A), P(B)$,又例如：$X$ 表示投一枚质地均匀的硬币，正面朝上的概率，显然在我们根据以往的经验下，我们会认为 $X$ 的概率 $P(X) = 0.5$ 。其中 $P(X) = 0.5$ 就是先验概率。</p>
<h4 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h4><p>后验概率（Posterior Probability）是事件发生后求的反向条件概率；即基于先验概率通过贝叶斯公式求得的反向条件概率。例如公式中的 $P(B \mid A)$   就是通过先验概率 $P(A)$和$P(B)$ 得到的后验概率，其通俗的讲就是「执果寻因」中的「因」。</p>
<h3 id="什么是朴素贝叶斯"><a href="#什么是朴素贝叶斯" class="headerlink" title="什么是朴素贝叶斯"></a>什么是朴素贝叶斯</h3><p>朴素贝叶斯（Naive Bayes）就是将贝叶斯原理以及条件独立结合而成的算法，其思想非常的简单，根据贝叶斯公式：<br>$$<br>P(B \mid A)=\frac{P(A \mid B)*P(B)}{P(A)} \tag{4}<br>$$<br>变形表达式为：<br>$$<br>P(类别 \mid 特征)=\frac{P(特征 \mid 类别) * P(类别)}{P(特征)} \tag{5}<br>$$<br>公式（5）利用先验概率，即特征和类别的概率；再利用不同类别中各个特征的概率分布，最后计算得到后验概率，即各个特征分布下的预测不同的类别。</p>
<p>利用贝叶斯原理求解固然是一个很好的方法，但实际生活中数据的特征之间是有相互联系的，在计算 $P(特征\mid类别)$ 时，考虑特征之间的联系会比较麻烦，而朴素贝叶斯则人为的将各个特征割裂开，认定特征之间相互独立。</p>
<p>朴素贝叶斯中的「朴素」，即条件独立，表示其假设预测的各个属性都是相互独立的,每个属性独立地对分类结果产生影响，条件独立在数学上的表示为：$P(AB)=P(A)*P(B)$。这样，使得朴素贝叶斯算法变得简单，但有时会牺牲一定的分类准确率。对于预测数据，求解在该预测数据的属性出现时各个类别的出现概率，将概率值大的类别作为预测数据的类别。</p>
<h2 id="朴素贝叶斯算法实现"><a href="#朴素贝叶斯算法实现" class="headerlink" title="朴素贝叶斯算法实现"></a>朴素贝叶斯算法实现</h2><p>前面主要介绍了朴素贝叶斯算法中几个重要的概率论知识，接下来我们对其进行具体的实现，算法流程如下：</p>
<p><strong>第 1 步</strong>：设<br>$$<br>X = \left { a_{1},a_{2},a_{3},…,a_{n} \right }<br>$$<br>为预测数据，其中 $a_{i}$ 是预测数据的特征值。</p>
<p><strong>第 2 步</strong>：设<br>$$<br>Y = \left {y_{1},y_{2},y_{3},…,y_{m} \right }<br>$$<br>为类别集合。</p>
<p><strong>第 3 步</strong>：计算 $P(y_{1}\mid x)$, $P(y_{2}\mid x)$, $P(y_{3}\mid x)$, $…$, $P(y_{m}\mid x)$。</p>
<p><strong>第 4 步</strong>：寻找 $P(y_{1}\mid x)$, $P(y_{2}\mid x)$, $P(y_{3}\mid x)$, $…$, $P(y_{m}\mid x)$ 中最大的概率 $P(y_{k}\mid x)$ ，则 $x$ 属于类别 $y_{k}$。</p>
<h3 id="生成示例数据"><a href="#生成示例数据" class="headerlink" title="生成示例数据"></a>生成示例数据</h3><p>下面我们利用 python 完成一个朴素贝叶斯算法的分类。首先生成一组示例数据：由 <code>A</code> 和 <code>B</code>两个类别组成，每个类别包含 <code>x</code>,<code>y</code>两个特征值，其中 <code>x</code> 特征包含<code>r,g,b</code>（红，绿，蓝）三个类别，<code>y</code>特征包含<code>s,m,l</code>（小，中，大）三个类别，如同数据 $X = [g,l]$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;生成示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span>():</span></span><br><span class="line">    data = &#123;<span class="string">&quot;x&quot;</span>: [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>],</span><br><span class="line">            <span class="string">&quot;y&quot;</span>: [<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">            <span class="string">&quot;labels&quot;</span>: [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]&#125;</span><br><span class="line">    data = pd.DataFrame(data, columns=[<span class="string">&quot;labels&quot;</span>, <span class="string">&quot;x&quot;</span>, <span class="string">&quot;y&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>

<p>在创建好数据后，接下来进行加载数据，并进行预览。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;加载并预览数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">data = create_data()</span><br><span class="line">data</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/output_00.png"></p>
<h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>根据朴素贝叶斯的原理，最终分类的决策因素是比较 $\left { P(类别 1 \mid 特征),P(类别 2 \mid 特征),…,P(类别 m \mid 特征) \right }$ 各个概率的大小，根据贝叶斯公式得知每一个概率计算的分母 $P(特征)$ 都是相同的，只需要比较分子 $P(类别)$ 和 $P(特征 \mid 类别)$ 乘积的大小。</p>
<p>那么如何得到 $P(类别)$,以及 $P(特征\mid 类别)$呢？在概率论中，可以应用<strong>极大似然估计法</strong>以及<strong>贝叶斯估计法</strong>来估计相应的概率。</p>
<h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>什么是极大似然？下面通过一个简单的例子让你有一个形象的了解：</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/3.png"></p>
<blockquote>
<p><strong>前提条件：</strong>假如有两个外形完全相同箱子，甲箱中有 <code>99</code> 个白球，<code>1</code> 个黑球；乙箱中有 <code>99</code> 个黑球，<code>1</code> 个白球。</p>
</blockquote>
<blockquote>
<p><strong>问题：</strong>当我们进行一次实验，并取出一个球，取出的结果是白球。那么，请问白球是从哪一个箱子里取出的？</p>
</blockquote>
<p>我相信，你的第一印象很可能会是白球从甲箱中取出。因为甲箱中的白球数量多，所以这个推断符合人们经验。其中「最可能」就是「极大似然」。而极大似然估计的目的就是利用已知样本结果，反推最有可能造成这个结果的参数值。</p>
<p>极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：「模型已定，参数未知」。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p>
<p>在概率论中求解极大似然估计的方法比较复杂，基于实验，我们将讲解 $P(B)$ 和 $P(B/A)$ 是如何通过极大似然估计得到的。$P(种类)$ 用数学的方法表示 ：<br>$$<br>P(y_{i}=c_{k})=\frac{\sum_{N}^{i=1}I(y_{i}=c_{k})}{N},k=1,2,3,…,m \tag{6}<br>$$<br>公式(6)中的 $y_{i}$ 表示数据的类别，$c_{k}$ 表示每一条数据的类别。</p>
<p>你可以通俗的理解为，在现有的训练集中，每一个类别所占总数的比例，例如:<strong>生成的数据</strong>中 $P(Y=A)=\frac{8}{15}$，表示训练集中总共有 15 条数据，而类别为 <code>A</code> 的有 8 条数据。  </p>
<p>下面我们用 Python 代码来实现先验概率 $P(种类)$ 的求解：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;P(种类) 先验概率计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_P_labels</span>(<span class="params">labels</span>):</span></span><br><span class="line">    labels = <span class="built_in">list</span>(labels)  <span class="comment"># 转换为 list 类型</span></span><br><span class="line">    P_label = &#123;&#125;  <span class="comment"># 设置空字典用于存入 label 的概率</span></span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        P_label[label] = labels.count(label) / <span class="built_in">float</span>(<span class="built_in">len</span>(labels))  <span class="comment"># p = count(y) / count(Y)</span></span><br><span class="line">    <span class="keyword">return</span> P_label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">P_labels = get_P_labels(data[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line">P_labels</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">0.5333333333333333</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">0.4666666666666667</span>&#125;</span><br></pre></td></tr></table></figure>

<p>$P(特征 \mid 种类)$ 由于公式较为繁琐这里先不给出，直接用叙述的方式能更清晰地帮助理解：</p>
<p>实际需要求的先验估计是特征的每一个类别对应的每一个种类的概率，例如：<strong>生成数据</strong> 中 $P(x_{1}=”r” \mid Y=A)=\frac{4}{8}$， <code>A</code> 的数据有 8 条，而在种类为 <code>A</code> 的数据且特征 <code>x</code> 为 <code>r</code>的有 4 条。</p>
<p>同样我们用代码将先验概率 $P(特征 \mid 种类)$ 实现求解：</p>
<p>首先我们将特征按序号合并生成一个 <code>numpy</code> 类型的数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;导入特征数据并预览</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">train_data = np.array(data.iloc[:, <span class="number">1</span>:])</span><br><span class="line">train_data</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>]], dtype=<span class="built_in">object</span>)</span><br></pre></td></tr></table></figure>

<p>在寻找属于某一类的某一个特征时，我们采用对比索引的方式来完成。<br>开始得到每一个类别的索引：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;类别 A,B 索引</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">labels = data[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line">label_index = []</span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> P_labels.keys():</span><br><span class="line">    temp_index = []</span><br><span class="line">    <span class="comment"># enumerate 函数返回 Series 类型数的索引和值，其中 i 为索引，label 为值</span></span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels):</span><br><span class="line">        <span class="keyword">if</span> (label == y):</span><br><span class="line">            temp_index.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    label_index.append(temp_index)</span><br><span class="line">label_index</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]]</span><br></pre></td></tr></table></figure>

<p>得到 <code>A</code> 和 <code>B</code> 的索引，其中是<code>A</code>类别为前 $8$ 条数据，<code>B</code>类别为后 $7$ 条数据。</p>
<p>在得到类别的索引之后，接下来就是找到我们需要的特征为 <code>r</code>的索引值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;特征 x 为 r 的索引</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">x_index = [i <span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data[:, <span class="number">0</span>]) <span class="keyword">if</span> feature == <span class="string">&#x27;r&#x27;</span>]  <span class="comment"># 效果等同于求类别索引中 for 循环</span></span><br><span class="line">x_index</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">11</span>]</span><br></pre></td></tr></table></figure>

<p>得到的结果为 $x$ 特征值为 $r$ 的数据索引值。</p>
<p>最后通过对比类别为 <code>A</code> 的索引值，计算出既符合 <code>x = r</code> 又符合 <code>A</code> 类别的数据在 <code>A</code> 类别中所占比例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_label = <span class="built_in">set</span>(x_index) &amp; <span class="built_in">set</span>(label_index[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">&#x27;既符合 x = r 又是 A 类别的索引值：&#x27;</span>, x_label)</span><br><span class="line">x_label_count = <span class="built_in">len</span>(x_label)</span><br><span class="line">print(<span class="string">&#x27;先验概率 P(r|A):&#x27;</span>, x_label_count / <span class="built_in">float</span>(<span class="built_in">len</span>(label_index[<span class="number">0</span>])))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">既符合 x = r 又是 A 类别的索引值： &#123;<span class="number">0</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>&#125;</span><br><span class="line">先验概率 P(r|A): <span class="number">0.5</span></span><br></pre></td></tr></table></figure>

<p>为了方便后面函数调用，我们将求 $P(特征\mid 种类)$ 代码整合为一个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;P(特征∣种类) 先验概率计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_P_fea_lab</span>(<span class="params">P_label, features, data</span>):</span></span><br><span class="line">    P_fea_lab = &#123;&#125;</span><br><span class="line">    train_data = data.iloc[:, <span class="number">1</span>:]</span><br><span class="line">    train_data = np.array(train_data)</span><br><span class="line">    labels = data[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> each_label <span class="keyword">in</span> P_label.keys():</span><br><span class="line">        label_index = [i <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">            labels) <span class="keyword">if</span> label == each_label]  <span class="comment"># labels 中出现 y 值的所有数值的下标索引</span></span><br><span class="line">        <span class="comment"># features[0] 在 trainData[:,0] 中出现的值的所有下标索引</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features)):</span><br><span class="line">            feature_index = [i <span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">                train_data[:, j]) <span class="keyword">if</span> feature == features[j]]</span><br><span class="line">            <span class="comment"># set(x_index)&amp;set(y_index) 列出两个表相同的元素</span></span><br><span class="line">            fea_lab_count = <span class="built_in">len</span>(<span class="built_in">set</span>(feature_index) &amp; <span class="built_in">set</span>(label_index))</span><br><span class="line">            key = <span class="built_in">str</span>(features[j]) + <span class="string">&#x27;|&#x27;</span> + <span class="built_in">str</span>(each_label)</span><br><span class="line">            P_fea_lab[key] = fea_lab_count / <span class="built_in">float</span>(<span class="built_in">len</span>(label_index))</span><br><span class="line">    <span class="keyword">return</span> P_fea_lab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">features = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>]</span><br><span class="line">get_P_fea_lab(P_labels, features, data)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;r|A&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line"> <span class="string">&#x27;m|A&#x27;</span>: <span class="number">0.375</span>,</span><br><span class="line"> <span class="string">&#x27;r|B&#x27;</span>: <span class="number">0.14285714285714285</span>,</span><br><span class="line"> <span class="string">&#x27;m|B&#x27;</span>: <span class="number">0.42857142857142855</span>&#125;</span><br></pre></td></tr></table></figure>

<p>可以得到当特征 <code>x</code> 和 <code>y</code> 的值为 <code>r</code> 和 <code>m</code> 时，在不同类别下的先验概率。</p>
<h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><p>在做极大似然估计时，若类别中缺少一些特征，则就会出现概率值为 <code>0</code> 的情况。此时，就会影响后验概率的计算结果，使得分类产生偏差。而解决这一问题最好的方法就是采用贝叶斯估计。  </p>
<p>在计算先验概率 $P(种类)$ 中，贝叶斯估计的数学表达式为：<br>$$<br>P(y_{i}=c_{k})=\frac{\sum_{N}^{i=1}I(y_{i}=c_{k})+\lambda }{N+k\lambda} \tag{8}<br>$$<br>其中 $\lambda \geq 0$ 等价于在随机变量各个取值的频数上赋予一个正数，当 $\lambda=0$ 时就是极大似然估计。在平时常取 $\lambda=1$，这时称为拉普拉斯平滑。例如：<strong>生成数据</strong> 中，$P(Y=A)=\frac{8+1}{15+2*1}=\frac{9}{17}$,取 $\lambda=1$ 此时由于一共有 <code>A</code>，<code>B</code> 两个类别，则 <code>k</code> 取 2。</p>
<p>同样计算 $P(特征 \mid 种类)$ 时，也是给计算时的分子分母加上拉普拉斯平滑。例如：<strong>生成数据</strong> 中，$P(x_{1}=”r” \mid Y=A)=\frac{4+1}{8+3*1}=\frac{5}{11}$ 同样取 $\lambda=1$ 此时由于 <code>x</code> 中有 <code>r</code>, <code>g</code>, <code>b</code> 三个种类，所以这里 k 取值为 3。</p>
<h3 id="朴素贝叶斯算法实现-1"><a href="#朴素贝叶斯算法实现-1" class="headerlink" title="朴素贝叶斯算法实现"></a>朴素贝叶斯算法实现</h3><p>通过上面的内容，相信你已经对朴素贝叶斯算法原理有一定印象。接下来，我们对朴素贝叶斯分类过程进行完整实现。其中，参数估计方法则使用极大似然估计。<br><em>注：分类器实现的公式，请参考《机器学习》- 周志华 P151 页</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;朴素贝叶斯分类器</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">data, features</span>):</span></span><br><span class="line">    <span class="comment"># 求 labels 中每个 label 的先验概率</span></span><br><span class="line">    labels = data[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">    P_label = get_P_labels(labels)</span><br><span class="line">    P_fea_lab = get_P_fea_lab(P_label, features, data)</span><br><span class="line"></span><br><span class="line">    P = &#123;&#125;</span><br><span class="line">    P_show = &#123;&#125;  <span class="comment"># 后验概率</span></span><br><span class="line">    <span class="keyword">for</span> each_label <span class="keyword">in</span> P_label:</span><br><span class="line">        P[each_label] = P_label[each_label]</span><br><span class="line">        <span class="keyword">for</span> each_feature <span class="keyword">in</span> features:</span><br><span class="line">            key = <span class="built_in">str</span>(each_label)+<span class="string">&#x27;|&#x27;</span>+<span class="built_in">str</span>(features)</span><br><span class="line">            P_show[key] = P[each_label] * \</span><br><span class="line">                P_fea_lab[<span class="built_in">str</span>(each_feature) + <span class="string">&#x27;|&#x27;</span> + <span class="built_in">str</span>(each_label)]</span><br><span class="line">            P[each_label] = P[each_label] * \</span><br><span class="line">                P_fea_lab[<span class="built_in">str</span>(each_feature) + <span class="string">&#x27;|&#x27;</span> +</span><br><span class="line">                          <span class="built_in">str</span>(each_label)]  <span class="comment"># 由于分母相同，只需要比较分子</span></span><br><span class="line">    print(P_show)</span><br><span class="line">    features_label = <span class="built_in">max</span>(P, key=P.get)  <span class="comment"># 概率最大值对应的类别</span></span><br><span class="line">    <span class="keyword">return</span> features_label</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classify(data, [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;A|[&#x27;r&#x27;, &#x27;m&#x27;]&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;B|[&#x27;r&#x27;, &#x27;m&#x27;]&quot;</span>: <span class="number">0.02857142857142857</span>&#125;</span><br><span class="line"><span class="string">&#x27;A&#x27;</span></span><br></pre></td></tr></table></figure>

<p>对于特征为 <code>[r,m]</code> 的数据通过朴素贝叶斯分类得到不同类别的概率值，经过比较后分为 <code>A</code> 类。</p>
<h3 id="朴素贝叶斯的三种常见模型"><a href="#朴素贝叶斯的三种常见模型" class="headerlink" title="朴素贝叶斯的三种常见模型"></a>朴素贝叶斯的三种常见模型</h3><p>了解完朴素贝叶斯算法原理后，在实际数据中，我们可以依照特征的数据类型不同，在计算先验概率方面对朴素贝叶斯模型进行划分，并分为：<strong>多项式模型</strong>，<strong>伯努利模型</strong>和<strong>高斯模型</strong>。</p>
<h4 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h4><p>当特征值为离散时，常常使用多项式模型。事实上，在以上实验的参数估计中，我们所应用的就是多项式模型。为避免概率值为 0 的情况出现，多项式模型采用的是贝叶斯估计。</p>
<h4 id="伯努利模型"><a href="#伯努利模型" class="headerlink" title="伯努利模型"></a>伯努利模型</h4><p>与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是 <code>1</code> 和 <code>0</code>（以文本分类为例，某个单词在文档中出现过，则其特征值为 <code>1</code>，否则为 <code>0</code>）。</p>
<p>在伯努利模型中，条件概率 $P(x_{i} \mid y_{k})$ 的计算方式为：</p>
<ul>
<li>当特征值 $x_{i}=1$ 时，$P(x_{i} \mid y_{k})=P(x_{i}=1 \mid y_{k})$;  </li>
<li>当特征值 $x_{i}=0$ 时，$P(x_{i} \mid y_{k})=P(x_{i}=0 \mid y_{k})$。</li>
</ul>
<h4 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h4><p>当特征是连续变量的时候，在不做平滑的情况下，运用多项式模型就会导致很多 $P(x_{i} \mid y_{k})=0$，此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，采用高斯模型。高斯模型是假设连续变量的特征数据是服从高斯分布的，高斯分布函数表达式为：<br>$$<br>P(x_{i}|y_{k})=\frac{1}{\sqrt{2\pi}\sigma_{y_{k},i}}exp(-\frac{(x-\mu_{y_{k},i}) ^{2}}{2\sigma ^{2}<em>{y</em>{k}},i})<br>$$<br>其中：</p>
<ul>
<li>$\mu_{y_{k},i}$ 表示类别为 $y_{k}$ 的样本中，第 $i$ 维特征的均值。  </li>
<li>$\sigma ^{2}<em>{y</em>{k}},i$ 表示类别为 $y_{k}$ 的样本中，第 $i$ 维特征的方差。  </li>
</ul>
<p>高斯分布示意图如下：</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/4.png"></p>
<hr>
<p>关于贝叶斯定理，这里有一个有趣的视频，希望能加深大家对该定理的理解。</p>
<center><video width='800px' controls src="http://labfile.oss.aliyuncs.com/courses/1081/beyes_video.mp4" /></center>

<div style="color: #999;font-size: 12px;text-align: center;">如何用贝叶斯方法帮助内容审核 | 视频来源：[回形针PaperClip](https://weibo.com/u/6414205745?is_all=1)</div></div><div class="article-licensing box"><div class="licensing-title"><p>机器学习| 朴素贝叶斯详解</p><p><a href="http://www.laugh12321.cn/blog/2019/01/19/naive_bayes_basic/">http://www.laugh12321.cn/blog/2019/01/19/naive_bayes_basic/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Laugh</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-01-19</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-10-21</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Naive-Bayes/">Naive Bayes</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5f902f1aa809f500123872bf&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/alipay-reward-image.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/wechat-reward-image.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/01/20/support_vector_machine/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">机器学习| 支持向量机详解</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/01/07/k_nearest_neighbors/"><span class="level-item">机器学习| K-近邻算法详解</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.css"><script src="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "13138ee9d667b524d871e80d9c16031a",
            repo: "blog",
            owner: "laugh12321",
            clientID: "f831b7db637971edc8c9",
            clientSecret: "9767a6cb27b89b67e2875e8915671499b75a6436",
            admin: ["laugh12321"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars0.githubusercontent.com/u/38065710?s=400&amp;u=80ee4d360562451484188e6f1677fa442720891b&amp;v=4" alt="Laugh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Laugh</p><p class="is-size-6 is-block">The life I want, there is no shortcut.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Northern Hemisphere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/blog/archives"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/blog/categories"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/blog/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/laugh12321" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/blog/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#朴素贝叶斯基础"><span class="level-left"><span class="level-item">1</span><span class="level-item">朴素贝叶斯基础</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#基本概念"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">基本概念</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#条件概率"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">条件概率</span></span></a></li><li><a class="level is-mobile" href="#贝叶斯定理"><span class="level-left"><span class="level-item">1.1.2</span><span class="level-item">贝叶斯定理</span></span></a></li><li><a class="level is-mobile" href="#先验概率"><span class="level-left"><span class="level-item">1.1.3</span><span class="level-item">先验概率</span></span></a></li><li><a class="level is-mobile" href="#后验概率"><span class="level-left"><span class="level-item">1.1.4</span><span class="level-item">后验概率</span></span></a></li></ul></li><li><a class="level is-mobile" href="#什么是朴素贝叶斯"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">什么是朴素贝叶斯</span></span></a></li></ul></li><li><a class="level is-mobile" href="#朴素贝叶斯算法实现"><span class="level-left"><span class="level-item">2</span><span class="level-item">朴素贝叶斯算法实现</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#生成示例数据"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">生成示例数据</span></span></a></li><li><a class="level is-mobile" href="#参数估计"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">参数估计</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#极大似然估计"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">极大似然估计</span></span></a></li><li><a class="level is-mobile" href="#贝叶斯估计"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">贝叶斯估计</span></span></a></li></ul></li><li><a class="level is-mobile" href="#朴素贝叶斯算法实现-1"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">朴素贝叶斯算法实现</span></span></a></li><li><a class="level is-mobile" href="#朴素贝叶斯的三种常见模型"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">朴素贝叶斯的三种常见模型</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#多项式模型"><span class="level-left"><span class="level-item">2.4.1</span><span class="level-item">多项式模型</span></span></a></li><li><a class="level is-mobile" href="#伯努利模型"><span class="level-left"><span class="level-item">2.4.2</span><span class="level-item">伯努利模型</span></span></a></li><li><a class="level is-mobile" href="#高斯模型"><span class="level-left"><span class="level-item">2.4.3</span><span class="level-item">高斯模型</span></span></a></li></ul></li></ul></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Algorithm/Sorting-algorithm/"><span class="level-start"><span class="level-item">Sorting algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">12</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Bagging-and-BoosTing/"><span class="level-start"><span class="level-item">Bagging and BoosTing</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Cluster/"><span class="level-start"><span class="level-item">Cluster</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Cluster/K-Means/"><span class="level-start"><span class="level-item">K-Means</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Decision-Tree/"><span class="level-start"><span class="level-item">Decision Tree</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/K-Means/"><span class="level-start"><span class="level-item">K-Means</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/K-Nearest-Neighbors/"><span class="level-start"><span class="level-item">K-Nearest Neighbors</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Linear-Regression/"><span class="level-start"><span class="level-item">Linear Regression</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Naive-Bayes/"><span class="level-start"><span class="level-item">Naive Bayes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Polynomial-Regression/"><span class="level-start"><span class="level-item">Polynomial Regression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Support-Vector-Machine/"><span class="level-start"><span class="level-item">Support Vector Machine</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Ubuntu/"><span class="level-start"><span class="level-item">Ubuntu</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/"><span class="level-start"><span class="level-item">ECS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/Seafile/"><span class="level-start"><span class="level-item">Seafile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Bagging/"><span class="tag">Bagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Blog/"><span class="tag">Blog</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/BoosTing/"><span class="tag">BoosTing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Cluster/"><span class="tag">Cluster</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ECS/"><span class="tag">ECS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GitHub-Pages/"><span class="tag">GitHub Pages</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Jekyll/"><span class="tag">Jekyll</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Means/"><span class="tag">K-Means</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Nearest-Neighbors/"><span class="tag">K-Nearest Neighbors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAE/"><span class="tag">MAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAPE/"><span class="tag">MAPE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MSE/"><span class="tag">MSE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Next/"><span class="tag">Next</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Seafile/"><span class="tag">Seafile</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Sorting-algorithm/"><span class="tag">Sorting algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Support-Vector-Machine/"><span class="tag">Support Vector Machine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Laugh</a><p class="is-size-7"><span>&copy; 2020 Laugh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>