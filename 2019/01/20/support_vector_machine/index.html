<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>机器学习| 支持向量机详解 - Laugh&#039;s blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Laugh&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Laugh&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习| 支持向量机详解"><meta property="og:url" content="http://www.laugh12321.cn/blog/2019/01/20/support_vector_machine/"><meta property="og:site_name" content="Laugh&#039;s blog"><meta property="og:description" content="在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/Baja%20California%20Sur%2C%20Mexico.jpg"><meta property="article:published_time" content="2019-01-20T00:00:00.000Z"><meta property="article:modified_time" content="2020-10-21T05:59:02.931Z"><meta property="article:author" content="Laugh"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Support Vector Machine"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/Baja%20California%20Sur%2C%20Mexico.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.laugh12321.cn/blog"},"headline":"Laugh's blog","image":["https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/photo/Baja%20California%20Sur%2C%20Mexico.jpg"],"datePublished":"2019-01-20T00:00:00.000Z","dateModified":"2020-10-21T05:59:02.931Z","author":{"@type":"Person","name":"laugh12321"},"description":"在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。"}</script><link rel="canonical" href="http://www.laugh12321.cn/blog/2019/01/20/support_vector_machine/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-180999768-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-180999768-1');</script><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Laugh</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">首页</a><a class="navbar-item" href="/blog/archives">归档</a><a class="navbar-item" href="/blog/categories">分类</a><a class="navbar-item" href="/blog/tags">标签</a><a class="navbar-item" href="/blog/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/Baja%20California%20Sur%2C%20Mexico.jpg" alt="机器学习| 支持向量机详解"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-01-20T00:00:00.000Z" title="2019-01-20T00:00:00.000Z">2019-01-20</time>发表</span><span class="level-item"><time dateTime="2020-10-21T05:59:02.931Z" title="2020-10-21T05:59:02.931Z">2020-10-21</time>更新</span><span class="level-item"><a class="link-muted" href="/blog/categories/Machine-Learning/">Machine Learning</a><span> / </span><a class="link-muted" href="/blog/categories/Machine-Learning/Support-Vector-Machine/">Support Vector Machine</a></span><span class="level-item">39 分钟读完 (大约5851个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习| 支持向量机详解</h1><div class="content"><p>在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。</p>
<a id="more"></a>

<h2 id="线性分类支持向量机"><a href="#线性分类支持向量机" class="headerlink" title="线性分类支持向量机"></a>线性分类支持向量机</h2><p>在逻辑回归中，我们尝试通过一条直线针对线性可分数据完成分类。同时，通过最小化对数损失函数来找到最优分割边界，也就是下图中的紫色直线。</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/1.png"></p>
<p>逻辑回归是一种简单高效的线性分类方法。而在这里中，我们将接触到另一种针对线性可分数据进行分类的思路，并把这种方法称之为支持向量机（英语：Support vector machine，简称：SVM）。</p>
<p>如果你第一次接触支持向量机这个名字，可能会感觉读起来比较拗口。至少我当时初次接触支持向量机时，完全不知道为什么会有这样一个怪异的名字。假如你和当时的我一样，那么当你看完下面这段介绍内容后，就应该会对支持向量机这个名词有更深刻的认识了。</p>
<h3 id="支持向量机分类特点"><a href="#支持向量机分类特点" class="headerlink" title="支持向量机分类特点"></a>支持向量机分类特点</h3><p>假设给定一个训练数据集 $T=\lbrace(x_1,y_1),(x_2,y_2),\cdots ,(x_n,y_n)\rbrace$ 。同时，假定已经找到样本空间中的分割平面，其划分公式可以通过以下线性方程来描述：<br>$$<br>wx+b=0\tag{1}<br>$$<br>使用一条直线对线性可分数据集进行分类的过程中，我们已经知道这样的直线可能有很多条：</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/4.png"></p>
<p>问题来了！<strong>哪一条直线是最优的划分方法呢？</strong></p>
<p>在逻辑回归中，我们引入了 S 形曲线和对数损失函数进行优化求解。如今，支持向量机给了一种从几何学上更加直观的方法进行求解，如下图所示：</p>
<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/2.png"></p>
<p>上图展示了支持向量机分类的过程。图中 $wx-b=0$ 为分割直线，我们通过这条直线将数据点分开。与此同时，分割时会在直线的两边再设立两个互相平行的虚线，这两条虚线与分割直线的距离一致。这里的距离往往也被我们称之为「间隔」，而支持向量机的分割特点在于，要使得<strong>分割直线和虚线之间的间隔最大化</strong>。同时也就是两虚线之间的间隔最大化。</p>
<p>对于线性可分的正负样本点而言，位于 $wx-b=1$ 虚线外的点就是正样本点，而位于 $wx-b=-1$ 虚线外的点就是负样本点。另外，正好位于两条虚线上方的样本点就被我们称为支持向量，这也就是支持向量机的名字来源。</p>
<h3 id="支持向量机分类演示"><a href="#支持向量机分类演示" class="headerlink" title="支持向量机分类演示"></a>支持向量机分类演示</h3><p>下面，我们使用 Python 代码来演示支持向量机的分类过程。</p>
<p>首先，我们介绍一种新的示例数据生成方法。即通过 scikit-learn 提供的 <code>samples_generator()</code> 类完成。通过 <code>samples_generator()</code> 类下面提供的不同方法，可以产生不同分布状态的示例数据。首先要用到 <code>make_blobs</code> 方法，该方法可以生成团状数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> samples_generator</span><br><span class="line"></span><br><span class="line">x, y = samples_generator.make_blobs(n_samples=<span class="number">60</span>, centers=<span class="number">2</span>, random_state=<span class="number">30</span>, cluster_std=<span class="number">0.8</span>) <span class="comment"># 生成示例数据</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>)) <span class="comment"># 绘图</span></span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_00.png"></p>
<p>接下来，我们在示例数据中绘制任意 3 条分割线把示例数据分开。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 3 条不同的分割线</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">for</span> m, b <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">-8</span>), (<span class="number">0.5</span>, <span class="number">-6.5</span>), (<span class="number">-0.2</span>, <span class="number">-4.25</span>)]:</span><br><span class="line">    y_temp = m * x_temp + b</span><br><span class="line">    plt.plot(x_temp, y_temp, <span class="string">&#x27;-k&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_01.png"></p>
<p>然后，可以使用 <code>fill_between</code> 方法手动绘制出分类硬间隔。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 3 条不同的分割线</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">for</span> m, b, d <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">-8</span>, <span class="number">0.2</span>), (<span class="number">0.5</span>, <span class="number">-6.5</span>, <span class="number">0.55</span>), (<span class="number">-0.2</span>, <span class="number">-4.25</span>, <span class="number">0.75</span>)]:</span><br><span class="line">    y_temp = m * x_temp + b</span><br><span class="line">    plt.plot(x_temp, y_temp, <span class="string">&#x27;-k&#x27;</span>)</span><br><span class="line">    plt.fill_between(x_temp, y_temp - d, y_temp + d, color=<span class="string">&#x27;#f3e17d&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_02.png"></p>
<div style="color: #999;font-size: 12px;font-style: italic;"><strong>上图为了呈现出分类间隔的效果，手动指定了参数。</strong></div>

<p>可以看出，不同的分割线所对应的间隔大小是不一致的，而支持向量机的目标是找到最大的分类硬间隔所对应的分割线。</p>
<h3 id="硬间隔表示及求解"><a href="#硬间隔表示及求解" class="headerlink" title="硬间隔表示及求解"></a>硬间隔表示及求解</h3><p>我们已经知道支持向量机是根据最大间隔来划分，下面考虑如何求得一个几何间隔最大的分割线。</p>
<p>对于线性可分数据而言，几何间隔最大的分离超平面是唯一的，这里的间隔也被我们称之为「硬间隔」，而间隔最大化也就称为硬间隔最大化。上图实际上就是硬间隔的典型例子。</p>
<p>最大间隔分离超平面，我们希望最大化超平面 $(w,b)$ 关于训练数据集的几何间隔 $\gamma$，满足以下约束条件：每个训练样本点到超平面 $(w,b)$ 的几何间隔至少都是 $\gamma$ ，因此可以转化为以下的约束最优化问题： </p>
<p>$$<br>\max\limits_{w,b}\gamma =\frac{2}{\left |w\right |} \tag{2a}<br>$$</p>
<p>$$<br>\begin{equation}<br>\textrm s.t. y_i(\frac{w}{\left |w\right |}x_i+\frac{b}{\left |w\right |})\geq \frac{\gamma}{2} \tag{2b}<br>\end{equation}<br>$$</p>
<p>实际上，$\gamma$ 的取值并不会影响最优化问题的解，同时，我们根据数学对偶性原则，可以得到面向硬间隔的线性可分数据的支持向量机的最优化问题：<br>$$<br>\min\limits_{w,b}\frac{1}{2}\left |w\right |^2 \tag{3a}<br>$$</p>
<p>$$<br>\begin{equation}<br>\textrm s.t. y_i(wx_i+b)-1\geq 0\tag{3b}<br>\end{equation}<br>$$</p>
<p>我们通常使用拉格朗日乘子法来求解最优化问题，将原始问题转化为对偶问题，通过解对偶问题得到原始问题的解。对公式（3）使用拉格朗日乘子法可得到其「对偶问题」。具体来说，对每条约束添加拉格朗日乘子 $\alpha_i \geq 0$，则该问题的拉格朗日函数可写为：<br>$$<br>L(w,b,\alpha)=\frac{1}{2}\left | w\right |^2+\sum\limits_{i=1}^{m}\alpha_i(1-y_i(wx_i+b)) \tag{4}<br>$$<br>我们通过将公式（4）分别对 $w$ 和 $b$ 求偏导为 <code>0</code> 并代入原式中，可以将 $w$ 和 $b$ 消去，得到公式（3）的对偶问题：<br>$$<br>\max\limits_{\alpha} \sum\limits_{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j x_i x_j \tag{5a}<br>$$</p>
<p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_i y_i=0,\tag{5b}<br>$$</p>
<p>$$<br>\alpha_i \geq 0,i=1,2,\cdots,N  \tag{5c}<br>$$</p>
<p>解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{6}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{7}<br>$$</p>
<h3 id="软间隔表示及求解"><a href="#软间隔表示及求解" class="headerlink" title="软间隔表示及求解"></a>软间隔表示及求解</h3><p>上面，我们介绍了线性可分条件下的最大硬间隔的推导求解方法。在很多时候，我们还会遇到下面这种情况。你可以发现，在实心点和空心点中各混入了零星的不同类别的数据点。对于这种情况，数据集就变成了严格意义上的线性不可分。但是，造成这种线性不可分的原因往往是因为包含「噪声」数据，它同样可以被看作是不严格条件下的线性可分。</p>
<p>当我们使用支持向量机求解这类问题时，就会把最大间隔称之为最大「软间隔」，而软间隔就意味着可以容许零星噪声数据被误分类。</p>
<p><img width='300px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/3.png"></img></p>
<p>当出现上图所示的样本点不是严格线性可分的情况时，某些样本点 $(x_i,y_i)$ 就不能满足函数间隔 $\geqslant 1$ 的约束条件，即公式（3b）中的约束条件。为了解决这个问题，可以对每个样本点 $(x_i,y_i)$ 引入一个松弛变量 $\xi_i \geq 0$，使得函数间隔加上松弛变量 $\geqslant 1$，即约束条件转化为：<br>$$<br>y_i(wx_i+b) \geq 1-\xi_i \tag{8}<br>$$<br>同时，对每个松弛变量 $\xi_i$ 支付一个代价 $\xi_i$，目标函数由原来的 $\frac{1}{2}||w||^2$ 变成：<br>$$<br>\frac{1}{2}\left | w \right |^2+C\sum\limits_{j=1}^{N}\xi_i \tag{9}<br>$$<br>这里，$C&gt;0$ 称为惩罚参数，一般根据实际情况确定。$C$ 值越大对误分类的惩罚增大，最优化问题即为：<br>$$<br>\min\limits_{w,b,\xi} \frac{1}{2}\left | w \right |^2+C\sum\limits_{i=1}^{N}\xi_i \tag{10a}<br>$$</p>
<p>$$<br>s.t. y_i(wx_i+b) \geq 1-\xi_i,i=1,2,…,N \tag{10b}<br>$$</p>
<p>$$<br>\xi_i\geq 0,i=1,2,…,N \tag{10c}<br>$$</p>
<p>这就是软间隔支持向量机的表示过程。同理，我们可以使用拉格朗日乘子法将其转换为对偶问题求解：<br>$$<br>\max\limits_{\alpha}  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i*x_j)-\sum\limits_{i=1}^{N}\alpha_i \tag{11a}<br>$$</p>
<p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_iy_i=0 \tag{11b}<br>$$</p>
<p>$$<br>0 \leq \alpha_i \leq C ,i=1,2,…,N\tag{11c}<br>$$</p>
<p>解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{12}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{13}<br>$$</p>
<h3 id="线性支持向量机分类实现"><a href="#线性支持向量机分类实现" class="headerlink" title="线性支持向量机分类实现"></a>线性支持向量机分类实现</h3><p>上面，我们对硬间隔和软间隔支持向量机的求解过程进行了推演，推导过程比较复杂不需要完全掌握，但至少要知道硬间隔和软间隔区别。接下来，我们就使用 Python 对支持向量机找寻最大间隔的过程进行实战。由于支持向量机纯 Python 实现太过复杂，所以本次直接使用 scikit-learn 完成。</p>
<p>scikit-learn 中的支持向量机分类器对应的类及参数为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto&#x27;</span>, coef0=<span class="number">0.0</span>, shrinking=<span class="literal">True</span>, probability=<span class="literal">False</span>, tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, verbose=<span class="literal">False</span>, max_iter=<span class="number">-1</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>主要的参数如下：</p>
<ul>
<li><code>C</code>: 软间隔支持向量机对应的惩罚参数，详见公式（9）.</li>
<li><code>kernel</code>: 核函数，linear, poly, rbf, sigmoid, precomputed 可选，下文详细介绍。</li>
<li><code>degree</code>: poly 多项式核函数的指数。</li>
<li><code>tol</code>: 收敛停止的容许值。</li>
</ul>
<p>这里，我们还是使用上面生成的示例数据训练支持向量机模型。由于是线性可分数据，<code>kernel</code> 参数指定为 <code>linear</code> 即可。</p>
<p>首先，训练支持向量机线性分类模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">linear_svc = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">linear_svc.fit(x, y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,</span><br><span class="line">  decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto_deprecated&#x27;</span>,</span><br><span class="line">  kernel=<span class="string">&#x27;linear&#x27;</span>, max_iter=<span class="number">-1</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">  shrinking=<span class="literal">True</span>, tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>对于训练完成的模型，我们可以通过 <code>support_vectors_</code> 属性输出它对应的支持向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear_svc.support_vectors_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ <span class="number">2.57325754</span>, <span class="number">-3.92687452</span>],</span><br><span class="line">       [ <span class="number">2.49156506</span>, <span class="number">-5.96321164</span>],</span><br><span class="line">       [ <span class="number">4.62473719</span>, <span class="number">-6.02504452</span>]])</span><br></pre></td></tr></table></figure>

<p>可以看到，一共有 <code>3</code> 个支持向量。如果你输出 <code>x, y</code> 的坐标值，就能看到这 <code>3</code> 个支持向量所对应的数据。</p>
<p>接下来，我们可以使用 Matplotlib 绘制出训练完成的支持向量机对于的分割线和间隔。为了方便后文重复使用，这里将绘图操作写入到 <code>svc_plot()</code> 函数中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svc_plot</span>(<span class="params">model</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取到当前 Axes 子图数据，并为绘制分割线做准备</span></span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    x = np.linspace(ax.get_xlim()[<span class="number">0</span>], ax.get_xlim()[<span class="number">1</span>], <span class="number">50</span>)</span><br><span class="line">    y = np.linspace(ax.get_ylim()[<span class="number">0</span>], ax.get_ylim()[<span class="number">1</span>], <span class="number">50</span>)</span><br><span class="line">    Y, X = np.meshgrid(y, x)</span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用轮廓线方法绘制分割线</span></span><br><span class="line">    ax.contour(X, Y, P, colors=<span class="string">&#x27;green&#x27;</span>, levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], linestyles=[<span class="string">&#x27;--&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;--&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 标记出支持向量的位置</span></span><br><span class="line">    ax.scatter(model.support_vectors_[:, <span class="number">0</span>], model.support_vectors_[:, <span class="number">1</span>], c=<span class="string">&#x27;green&#x27;</span>, s=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制最大间隔支持向量图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">svc_plot(linear_svc)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_03.png"></p>
<p>如上图所示，绿色实线代表最终找到的分割线，绿色虚线之间的间隔也就是最大间隔。同时，绿色实心点即代表 <code>3</code> 个支持向量的位置。</p>
<p>上面的数据点可以被线性可分，所以得到的也就是硬间隔支持向量机的分类结果。那么，如果我们加入噪声使得数据集变成不完美线性可分，结果会怎么样呢？</p>
<p>接下来，我们就来还原软间隔支持向量机的分类过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向原数据集中加入噪声点</span></span><br><span class="line">x = np.concatenate((x, np.array([[<span class="number">3</span>, <span class="number">-4</span>], [<span class="number">4</span>, <span class="number">-3.8</span>], [<span class="number">2.5</span>, <span class="number">-6.3</span>], [<span class="number">3.3</span>, <span class="number">-5.8</span>]])))</span><br><span class="line">y = np.concatenate((y, np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_04.png"></p>
<p>可以看到，此时的红蓝数据团中各混入了两个噪声点。</p>
<p>训练软间隔支持向量机模型并绘制成分割线和最大间隔：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">linear_svc.fit(x, y) <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">svc_plot(linear_svc)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_05.png"></p>
<p>由于噪声点的混入，此时支持向量的数量由原来的 <code>3</code> 个变成了 <code>13</code> 个。</p>
<p>前面的实验中，我们提到了惩罚系数 $C$，下面可以通过更改 $C$ 的取值来观察支持向量的变化过程。与此同时，我们要引入一个可以在 Notebook 中实现交互操作的模块。你可以通过选择不同的 $C$ 查看最终绘图的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact</span><br><span class="line"><span class="keyword">import</span> ipywidgets <span class="keyword">as</span> widgets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_c</span>(<span class="params">c</span>):</span></span><br><span class="line">    linear_svc.C = c</span><br><span class="line">    linear_svc.fit(x, y)</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">    svc_plot(linear_svc)</span><br><span class="line">    </span><br><span class="line">interact(change_c, c=[<span class="number">1</span>, <span class="number">10000</span>, <span class="number">1000000</span>])</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_06.png"></p>
<h2 id="非线性分类支持向量机"><a href="#非线性分类支持向量机" class="headerlink" title="非线性分类支持向量机"></a>非线性分类支持向量机</h2><p>上面的内容中，我们假设样本是线性可分或不严格线性可分，然后通过支持向量机建立最大硬间隔或软间隔实现样本分类。然而，线性可分的样本往往只是理想情况，现实中的原始样本大多数情况下是线性不可分。此时，还能用支持向量机吗？</p>
<p>其实，对于线性不可分的数据集，我们也可以通过支持向量机去完成分类。但是，这里需要增加一个技巧把线性不可分数据转换为线性可分数据之后，再完成分类。</p>
<p>与此同时，<strong>我们把这种数据转换的技巧称作「核技巧」，实现数据转换的函数称之为「核函数」</strong>。</p>
<h3 id="核技巧与核函数"><a href="#核技巧与核函数" class="headerlink" title="核技巧与核函数"></a>核技巧与核函数</h3><p>根据上面的介绍，我们提到一个思路就是核技巧，即先把线性不可分数据转换为线性可分数据，然后再使用支持向量机去完成分类。那么，具体是怎样操作呢？</p>
<div style="text-align:center;color:blue;"><i>核技巧的关键在于空间映射，即将低维数据映射到高维空间中，使得数据集在高维空间能被线性可分。</i></div>

<div style="color: #999;font-size: 12px;font-style: italic;">* 核技巧是一种数学方法，仅针对于其在支持向量机中的应用场景进行讲解。</div>

<p><img width='500px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/5.png"></img></p>
<p>如上图所示，假设我们在二维空间中有蓝色和红色代表的两类数据点，很明显无法使用一条直线把这两类数据分开。此时，如果我们使用核技巧将其映射到三维空间中，就变成了可以被平面线性可分的状态。</p>
<p>对于「映射」过程，我们还可以这样理解：分布在二维桌面上的红蓝小球无法被线性分开，此时将手掌拍向桌面（好疼），小球在力的作用下跳跃到三维空间中，这也就是一个直观的映射过程。</p>
<p>同时，「映射」的过程也就是通过核函数转换的过程。这里需要补充说明一点，那就是将数据点从低维度空间转换到高维度空间的方法有很多，但往往涉及到庞大的计算量，而数学家们从中发现了几种特殊的函数，这类函数能大大降低计算的复杂度，于是被命名为「核函数」。也就是说，核技巧是一种特殊的「映射」技巧，而核函数是核技巧的实现方法。</p>
<p>下面，我们就认识几种常见的核函数：</p>
<h4 id="线性核函数"><a href="#线性核函数" class="headerlink" title="线性核函数"></a>线性核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=x_i*x_j \tag{14}<br>$$</p>
<h4 id="多项式核函数"><a href="#多项式核函数" class="headerlink" title="多项式核函数"></a>多项式核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=\left ( x_i*x_j \right )^d, d \geq 1 \tag{15}<br>$$</p>
<h4 id="高斯径向基核函数"><a href="#高斯径向基核函数" class="headerlink" title="高斯径向基核函数"></a>高斯径向基核函数</h4><p>$$<br>k\left ( x_i, x_j \right ) = \exp \left(-{\frac  {\left |{\mathbf  {x_i}}-{\mathbf  {x_j}}\right |<em>{2}^{2}}{2\sigma ^{2}}}\right)=exp\left ( -\gamma * \left | x_i-x_j \right |</em>{2} ^2 \right ), \gamma&gt;0 \tag{16}<br>$$</p>
<h4 id="Sigmoid-核函数"><a href="#Sigmoid-核函数" class="headerlink" title="Sigmoid 核函数"></a>Sigmoid 核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=tanh\left ( \beta * x_ix_j+\theta \right ), \beta &gt; 0 , \theta &lt; 0 \tag{17}<br>$$</p>
<p>这 <code>4</code> 个核函数也就分别对应着上文介绍 <code>sklearn</code> 中 <code>SVC</code> 方法中 <code>kernel</code> 参数的 <code>linear, poly, rbf, sigmoid</code> 等 <code>4</code> 种不同取值。</p>
<p>此外，核函数还可以通过函数组合得到，例如：</p>
<p>若 $k_1$ 和 $k_2$ 是核函数，那么对于任意正数 $\lambda_1,\lambda_2$，其线性组合：<br>$$<br>\lambda_1 k_1+\lambda_2 k_2 \tag{18}<br>$$</p>
<h3 id="引入核函数的间隔表示及求解"><a href="#引入核函数的间隔表示及求解" class="headerlink" title="引入核函数的间隔表示及求解"></a>引入核函数的间隔表示及求解</h3><p>我们通过直接引入核函数 $k(x_i,x_j)$，而不需要显式的定义高维特征空间和映射函数，就可以利用解线性分类问题的方法来求解非线性分类问题的支持向量机。引入核函数以后，对偶问题就变为：<br>$$<br>\max\limits_{\alpha}  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_jy_iy_jk(x_i*x_j)-\sum\limits_{i=1}^{N}\alpha_i \tag{19a}<br>$$</p>
<p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_iy_i=0 \tag{19b}<br>$$</p>
<p>$$<br>0 \leq \alpha_i \leq C ,i=1,2,…,N \tag{19c}<br>$$</p>
<p>同样，解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{20}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{21}<br>$$</p>
<h3 id="非线性支持向量机分类实现"><a href="#非线性支持向量机分类实现" class="headerlink" title="非线性支持向量机分类实现"></a>非线性支持向量机分类实现</h3><p>同样，我们使用 scikit-learn 中提供的 SVC 类来构建非线性支持向量机模型，并绘制决策边界。</p>
<p>首先，实验需要生成一组示例数据。上面我们使用了 <code>make_blobs</code> 生成一组线性可分数据，这里使用 <code>make_circles</code> 生成一组线性不可分数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x2, y2 = samples_generator.make_circles(<span class="number">150</span>, factor=<span class="number">.5</span>, noise=<span class="number">.1</span>, random_state=<span class="number">30</span>) <span class="comment"># 生成示例数据</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>)) <span class="comment"># 绘图</span></span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_07.png"></p>
<p>上图明显是一组线性不可分数据，当我们训练支持向量机模型时就需要引入核技巧。例如，我们这里使用下式做一个简单的非线性映射：<br>$$<br>k\left ( x_i, x_j \right )=x_i^2 + x_j^2 \tag{22}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_function</span>(<span class="params">xi, xj</span>):</span></span><br><span class="line">    poly = xi**<span class="number">2</span> + xj**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> poly</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact, fixed</span><br><span class="line"></span><br><span class="line">r = kernel_function(x2[:,<span class="number">0</span>], x2[:,<span class="number">1</span>])</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">ax = plt.subplot(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.scatter3D(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], r, c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;r&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_08.png"></p>
<p>上面展示了二维空间点映射到效果维空间的效果。接下来，我们使用 sklearn 中 SVC 方法提供的 RBF 高斯径向基核函数完成实验。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbf_svc = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>)</span><br><span class="line">rbf_svc.fit(x2, y2)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,</span><br><span class="line">  decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto_deprecated&#x27;</span>,</span><br><span class="line">  kernel=<span class="string">&#x27;rbf&#x27;</span>, max_iter=<span class="number">-1</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">  shrinking=<span class="literal">True</span>, tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line">svc_plot(rbf_svc)</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_09.png"></p>
<p>同样，我们可以挑战不同的惩罚系数 $C$，看一看决策边界和支持向量的变化情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_c</span>(<span class="params">c</span>):</span></span><br><span class="line">    rbf_svc.C = c</span><br><span class="line">    rbf_svc.fit(x2, y2)</span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">    svc_plot(rbf_svc)</span><br><span class="line">    </span><br><span class="line">interact(change_c, c=[<span class="number">1</span>, <span class="number">100</span>, <span class="number">10000</span>])</span><br></pre></td></tr></table></figure>

<p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_10.png"></p>
<h2 id="多分类支持向量机"><a href="#多分类支持向量机" class="headerlink" title="多分类支持向量机"></a>多分类支持向量机</h2><p>支持向量机最初是为二分类问题设计的，当我们面对多分类问题时，其实同样可以使用支持向量机解决。而解决的方法就是通过组合多个二分类器来实现多分类器的构造。根据构造的方式又分为 2 种方法：</p>
<ul>
<li><strong>一对多法</strong>：即训练时依次把某个类别的样本归为一类，剩余的样本归为另一类，这样 $k$ 个类别的样本就构造出了 $k$ 个支持向量机。</li>
<li><strong>一对一法</strong>：即在任意两类样本之间构造一个支持向量机，因此 $k$ 个类别的样本就需要设计 $k(k-1) \div 2$ 个支持向量机。</li>
</ul>
<p>而在 scikit-learn，实现多分类支持向量机通过设定参数 <code>decision_function_shape</code> 来确定，其中：</p>
<ul>
<li><code>decision_function_shape=&#39;ovo&#39;</code>：代表一对一法。</li>
<li><code>decision_function_shape=&#39;ovr&#39;</code>：代表一对多法。</li>
</ul>
<p>由于这里只需要修改参数，所以就不再赘述了。</p>
<hr>
<p><strong>拓展阅读：</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-hans/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">支持向量机 - 维基百科</a></p>
</li>
<li><p>[知乎上关于支持向量机的问题讨论](<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/21094489">https://www.zhihu.com/question/21094489</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://cuijiahua.com/blog/2017/11/ml_8_svm_1.html">机器学习实战教程（八）：支持向量机原理篇之手撕线性SVM</a></p>
</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>机器学习| 支持向量机详解</p><p><a href="http://www.laugh12321.cn/blog/2019/01/20/support_vector_machine/">http://www.laugh12321.cn/blog/2019/01/20/support_vector_machine/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Laugh</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-01-20</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-10-21</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Support-Vector-Machine/">Support Vector Machine</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5f902f1aa809f500123872bf&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/alipay-reward-image.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/wechat-reward-image.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/01/23/ubuntu16.04_setting/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">阿里云服务器ECS Ubuntu16.04 初次使用配置教程(图形界面安装)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/01/19/naive_bayes_basic/"><span class="level-item">机器学习| 朴素贝叶斯详解</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.css"><script src="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "8b30d0f0ff4287aee2516f9cb37e70d0",
            repo: "blog",
            owner: "laugh12321",
            clientID: "f831b7db637971edc8c9",
            clientSecret: "9767a6cb27b89b67e2875e8915671499b75a6436",
            admin: ["laugh12321"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars0.githubusercontent.com/u/38065710?s=400&amp;u=80ee4d360562451484188e6f1677fa442720891b&amp;v=4" alt="Laugh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Laugh</p><p class="is-size-6 is-block">The life I want, there is no shortcut.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Northern Hemisphere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/blog/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/blog/categories"><p class="title">15</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/blog/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/laugh12321" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/blog/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#线性分类支持向量机"><span class="level-left"><span class="level-item">1</span><span class="level-item">线性分类支持向量机</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#支持向量机分类特点"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">支持向量机分类特点</span></span></a></li><li><a class="level is-mobile" href="#支持向量机分类演示"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">支持向量机分类演示</span></span></a></li><li><a class="level is-mobile" href="#硬间隔表示及求解"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">硬间隔表示及求解</span></span></a></li><li><a class="level is-mobile" href="#软间隔表示及求解"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">软间隔表示及求解</span></span></a></li><li><a class="level is-mobile" href="#线性支持向量机分类实现"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">线性支持向量机分类实现</span></span></a></li></ul></li><li><a class="level is-mobile" href="#非线性分类支持向量机"><span class="level-left"><span class="level-item">2</span><span class="level-item">非线性分类支持向量机</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#核技巧与核函数"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">核技巧与核函数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#线性核函数"><span class="level-left"><span class="level-item">2.1.1</span><span class="level-item">线性核函数</span></span></a></li><li><a class="level is-mobile" href="#多项式核函数"><span class="level-left"><span class="level-item">2.1.2</span><span class="level-item">多项式核函数</span></span></a></li><li><a class="level is-mobile" href="#高斯径向基核函数"><span class="level-left"><span class="level-item">2.1.3</span><span class="level-item">高斯径向基核函数</span></span></a></li><li><a class="level is-mobile" href="#Sigmoid-核函数"><span class="level-left"><span class="level-item">2.1.4</span><span class="level-item">Sigmoid 核函数</span></span></a></li></ul></li><li><a class="level is-mobile" href="#引入核函数的间隔表示及求解"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">引入核函数的间隔表示及求解</span></span></a></li><li><a class="level is-mobile" href="#非线性支持向量机分类实现"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">非线性支持向量机分类实现</span></span></a></li></ul></li><li><a class="level is-mobile" href="#多分类支持向量机"><span class="level-left"><span class="level-item">3</span><span class="level-item">多分类支持向量机</span></span></a></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Algorithm/Sorting-algorithm/"><span class="level-start"><span class="level-item">Sorting algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Decision-Tree/"><span class="level-start"><span class="level-item">Decision Tree</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/K-Nearest-Neighbors/"><span class="level-start"><span class="level-item">K-Nearest Neighbors</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Linear-Regression/"><span class="level-start"><span class="level-item">Linear Regression</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Naive-Bayes/"><span class="level-start"><span class="level-item">Naive Bayes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Polynomial-Regression/"><span class="level-start"><span class="level-item">Polynomial Regression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Support-Vector-Machine/"><span class="level-start"><span class="level-item">Support Vector Machine</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Ubuntu/"><span class="level-start"><span class="level-item">Ubuntu</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/"><span class="level-start"><span class="level-item">ECS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/Seafile/"><span class="level-start"><span class="level-item">Seafile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Blog/"><span class="tag">Blog</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ECS/"><span class="tag">ECS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GitHub-Pages/"><span class="tag">GitHub Pages</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Jekyll/"><span class="tag">Jekyll</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Nearest-Neighbors/"><span class="tag">K-Nearest Neighbors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAE/"><span class="tag">MAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAPE/"><span class="tag">MAPE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MSE/"><span class="tag">MSE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Next/"><span class="tag">Next</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Seafile/"><span class="tag">Seafile</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Sorting-algorithm/"><span class="tag">Sorting algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Support-Vector-Machine/"><span class="tag">Support Vector Machine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Laugh</a><p class="is-size-7"><span>&copy; 2020 Laugh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>