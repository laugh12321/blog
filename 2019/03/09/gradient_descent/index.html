<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>译|Gradient Descent in Python - Laugh&#039;s blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Laugh&#039;s blog"><meta name="msapplication-TileImage" content="https://avatars2.githubusercontent.com/u/38065710?s=400&amp;u=b32cf1cc11129ac10e78dd72a9e3dba0aa0d58bd&amp;v=4"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Laugh&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="当你初次涉足机器学习时，你学习的第一个基本算法就是 梯度下降 (Gradient Descent), 可以说梯度下降法是机器学习算法的支柱。 在这篇文章中，我尝试使用 python$python$ 解释梯度下降法的基本原理。一旦掌握了梯度下降法，很多问题就会变得容易理解，并且利于理解不同的算法。"><meta property="og:type" content="blog"><meta property="og:title" content="译|Gradient Descent in Python"><meta property="og:url" content="http://www.laugh12321.cn/blog/2019/03/09/gradient_descent/"><meta property="og:site_name" content="Laugh&#039;s blog"><meta property="og:description" content="当你初次涉足机器学习时，你学习的第一个基本算法就是 梯度下降 (Gradient Descent), 可以说梯度下降法是机器学习算法的支柱。 在这篇文章中，我尝试使用 python$python$ 解释梯度下降法的基本原理。一旦掌握了梯度下降法，很多问题就会变得容易理解，并且利于理解不同的算法。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/13.jpg"><meta property="article:published_time" content="2019-03-09T00:00:00.000Z"><meta property="article:modified_time" content="2020-10-23T08:47:45.933Z"><meta property="article:author" content="Laugh"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="GD"><meta property="article:tag" content="SGD"><meta property="article:tag" content="MBGD"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/13.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.laugh12321.cn/blog"},"headline":"Laugh's blog","image":[],"datePublished":"2019-03-09T00:00:00.000Z","dateModified":"2020-10-23T08:47:45.933Z","author":{"@type":"Person","name":"laugh12321"},"description":"当你初次涉足机器学习时，你学习的第一个基本算法就是 梯度下降 (Gradient Descent), 可以说梯度下降法是机器学习算法的支柱。 在这篇文章中，我尝试使用 python$python$ 解释梯度下降法的基本原理。一旦掌握了梯度下降法，很多问题就会变得容易理解，并且利于理解不同的算法。"}</script><link rel="canonical" href="http://www.laugh12321.cn/blog/2019/03/09/gradient_descent/"><link rel="icon" href="https://avatars2.githubusercontent.com/u/38065710?s=400&amp;u=b32cf1cc11129ac10e78dd72a9e3dba0aa0d58bd&amp;v=4"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-180999768-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-180999768-1');</script><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><link rel="alternate" href="/blog/atom.xml" title="Laugh's blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Laugh</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">首页</a><a class="navbar-item" href="/blog/archives">归档</a><a class="navbar-item" href="/blog/categories">分类</a><a class="navbar-item" href="/blog/tags">标签</a><a class="navbar-item" href="/blog/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/13.jpg" alt="译|Gradient Descent in Python"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-03-09T00:00:00.000Z" title="2019-03-09T00:00:00.000Z">2019-03-09</time>发表</span><span class="level-item"><time dateTime="2020-10-23T08:47:45.933Z" title="2020-10-23T08:47:45.933Z">2020-10-23</time>更新</span><span class="level-item"><a class="link-muted" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> / </span><a class="link-muted" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">梯度下降</a></span><span class="level-item">17 分钟读完 (大约2478个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">译|Gradient Descent in Python</h1><div class="content"><p>当你初次涉足机器学习时，你学习的第一个基本算法就是 <strong>梯度下降 (Gradient Descent)</strong>, 可以说<strong>梯度下降法</strong>是机器学习算法的支柱。 在这篇文章中，我尝试使用 python$python$ 解释<strong>梯度下降法</strong>的基本原理。一旦掌握了<strong>梯度下降法</strong>，很多问题就会变得容易理解，并且利于理解不同的算法。</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use([<span class="string">&#x27;ggplot&#x27;</span>])</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<p>如果你想尝试自己实现<strong>梯度下降法</strong>， 你需要加载基本的 p$python$ $packages$ —— $numpy$ and $matplotlib$  </p>
<p>首先， 我们将创建包含着噪声的线性数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机创建一些噪声</span></span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>接下来通过 <code>matplotlib</code> 可视化数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.plot(X, y, <span class="string">&#x27;b.&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;$x$&quot;</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;$y$&quot;</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])</span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_7_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_7_1.png"></a></p>
<p>显然， y$y$ 与 x$x$ 具有良好的线性关系，这个数据非常简单，只有一个自变量 x$x$.</p>
<p>我们可以将其表示为简单的线性关系：</p>
<p>y=b+mx</p>
<p>$$y=b+mx$$</p>
<p>并求出 b$b$ , m$m$。</p>
<p>这种被称为解方程的分析方法并没有什么不妥，但机器学习是涉及矩阵计算的，因此我们使用矩阵法（向量法）进行分析。</p>
<p>我们将 y$y$ 替换成 J(θ)$J(θ)$， b$b$ 替换成 θ0$θ0$， m$m$ 替换成 θ1$θ1$。<br>得到如下表达式：</p>
<p>J(θ)=θ0+θ1x</p>
<p>$$J(θ)=θ0+θ1x$$</p>
<p><strong>注意：</strong> 本例中 θ0=4$θ0=4$， θ1=3$θ1=3$</p>
<p>求解 θ0$θ0$ 和 θ1$θ1$ 的分析方法，代码如下：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X\_b &#x3D; np.c\_\[np.ones((100, 1)), X\] # 为X添加了一个偏置单位，对于X中的每个向量都是1  </span><br><span class="line">theta\_best &#x3D; np.linalg.inv(X\_b.T.dot(X\_b)).dot(X\_b.T).dot(y)  </span><br><span class="line">theta\_best  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[3.86687149],</span><br><span class="line">       [3.12408839]]) </span><br></pre></td></tr></table></figure>

<p>不难发现这个值接近真实的 θ0$θ0$，θ1$θ1$，由于我在数据中引入了噪声，所以存在误差。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X\_new &#x3D; np.array(\[\[0\], \[2\]\])  </span><br><span class="line">X\_new\_b &#x3D; np.c\_\[np.ones((2, 1)), X\_new\]  </span><br><span class="line">y\_predict &#x3D; X\_new\_b.dot(theta\_best)  </span><br><span class="line">y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[ 3.86687149],</span><br><span class="line">       [10.11504826]]) </span><br></pre></td></tr></table></figure>

<h2 id="梯度下降法-（Gradient-Descent）"><a href="#梯度下降法-（Gradient-Descent）" class="headerlink" title="梯度下降法 （Gradient Descent）"></a><a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%EF%BC%88Gradient-Descent%EF%BC%89" title="梯度下降法 （Gradient Descent）"></a>梯度下降法 （Gradient Descent）</h2><h3 id="Cost-Function-amp-Gradients"><a href="#Cost-Function-amp-Gradients" class="headerlink" title="Cost Function &amp; Gradients"></a><a href="#Cost-Function-amp-Gradients" title="Cost Function &amp; Gradients"></a>Cost Function &amp; Gradients</h3><p>计算代价函数和梯度的公式如下所示。</p>
<p><strong>注意：</strong>代价函数用于线性回归，对于其他算法，代价函数是不同的，梯度必须从代价函数中推导出来。</p>
<p><strong>Cost</strong></p>
<p>J(θ)=1/2mm∑i=1(h(θ)(i)−y(i))2</p>
<p>$$J(θ)=1/2m∑i=1m(h(θ)(i)−y(i))2$$</p>
<p><strong>Gradient</strong></p>
<p>∂J(θ)∂θj=1/mm∑i=1(h(θ(i)−y(i)).X(i)j</p>
<p>$$∂J(θ)∂θj=1/m∑i=1m(h(θ(i)−y(i)).Xj(i)$$</p>
<p><strong>Gradients</strong></p>
<p>θ0:=θ0−α.(1/m.m∑i=1(h(θ(i)−y(i)).X(i)0)</p>
<p>$$θ0:=θ0−α.(1/m.∑i=1m(h(θ(i)−y(i)).X0(i))$$</p>
<p>θ1:=θ1−α.(1/m.m∑i=1(h(θ(i)−y(i)).X(i)1)</p>
<p>$$θ1:=θ1−α.(1/m.∑i=1m(h(θ(i)−y(i)).X1(i))$$</p>
<p>θ2:=θ2−α.(1/m.m∑i=1(h(θ(i)−y(i)).X(i)2)</p>
<p>$$θ2:=θ2−α.(1/m.∑i=1m(h(θ(i)−y(i)).X2(i))$$</p>
<p>θj:=θj−α.(1/m.m∑i=1(h(θ(i)−y(i)).X(i)0)</p>
<p>$$θj:=θj−α.(1/m.∑i=1m(h(θ(i)−y(i)).X0(i))$$</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def cal\_cost(theta, X, y):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    Calculates the cost for given X and Y. The following shows and example of a single dimensional X  </span><br><span class="line">    theta &#x3D; Vector of thetas   </span><br><span class="line">    X     &#x3D; Row of X&#39;s np.zeros((2,j))  </span><br><span class="line">    y     &#x3D; Actual y&#39;s np.zeros((2,1))  </span><br><span class="line">      </span><br><span class="line">    where:  </span><br><span class="line">        j is the no of features  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    m &#x3D; len(y)  </span><br><span class="line">      </span><br><span class="line">    predictions &#x3D; X.dot(theta)  </span><br><span class="line">    cost &#x3D; (1&#x2F;2\*m) \* np.sum(np.square(predictions - y))  </span><br><span class="line">      </span><br><span class="line">    return cost  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |<br>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def gradient\_descent(X, y, theta, learning\_rate &#x3D; 0.01, iterations &#x3D; 100):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    X    &#x3D; Matrix of X with added bias units  </span><br><span class="line">    y    &#x3D; Vector of Y  </span><br><span class="line">    theta&#x3D;Vector of thetas np.random.randn(j,1)  </span><br><span class="line">    learning\_rate   </span><br><span class="line">    iterations &#x3D; no of iterations  </span><br><span class="line">      </span><br><span class="line">    Returns the final theta vector and array of cost history over no of iterations      </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    m &#x3D; len(y)  </span><br><span class="line">    # learning\_rate &#x3D; 0.01  </span><br><span class="line">    # iterations &#x3D; 100  </span><br><span class="line">      </span><br><span class="line">    cost\_history &#x3D; np.zeros(iterations)  </span><br><span class="line">    theta\_history &#x3D; np.zeros((iterations, 2))  </span><br><span class="line">    for i in range(iterations):  </span><br><span class="line">        prediction &#x3D; np.dot(X, theta)  </span><br><span class="line">          </span><br><span class="line">        theta &#x3D; theta - (1&#x2F;m) \* learning\_rate \* (X.T.dot((prediction - y)))  </span><br><span class="line">        theta\_history\[i, :\] &#x3D; theta.T  </span><br><span class="line">        cost\_history\[i\] &#x3D; cal\_cost(theta, X, y)  </span><br><span class="line">          </span><br><span class="line">    return theta, cost\_history, theta\_history  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |<br>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">\# 从1000次迭代开始，学习率为0.01。从高斯分布的θ开始  </span><br><span class="line">lr &#x3D;0.01  </span><br><span class="line">n\_iter &#x3D; 1000  </span><br><span class="line">theta &#x3D; np.random.randn(2, 1)  </span><br><span class="line">X\_b &#x3D; np.c\_\[np.ones((len(X), 1)), X\]  </span><br><span class="line">theta, cost\_history, theta\_history &#x3D; gradient\_descent(X\_b, y, theta, lr, n\_iter)  </span><br><span class="line">  </span><br><span class="line">print(&#39;Theta0:          &#123;:0.3f&#125;,\\nTheta1:          &#123;:0.3f&#125;&#39;.format(theta\[0\]\[0\],theta\[1\]\[0\]))  </span><br><span class="line">print(&#39;Final cost&#x2F;MSE:  &#123;:0.3f&#125;&#39;.format(cost\_history\[-1\]))  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta0:          3.867,</span><br><span class="line">Theta1:          3.124</span><br><span class="line">Final cost&#x2F;MSE:  5457.747 </span><br></pre></td></tr></table></figure>

<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\# 绘制迭代的成本图  </span><br><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(12,8))  </span><br><span class="line">  </span><br><span class="line">ax.set\_ylabel(&#39;J(Theta)&#39;)  </span><br><span class="line">ax.set\_xlabel(&#39;Iterations&#39;)  </span><br><span class="line">ax.plot(range(1000), cost\_history, &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_27_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_27_1.png"></a></p>
<p>在大约 <code>150</code> 次迭代之后代价函数趋于稳定，因此放大到迭代200，看看曲线</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(10,8))  </span><br><span class="line">ax.plot(range(200), cost\_history\[:200\], &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_29_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_29_1.png"></a></p>
<p>值得注意的是，最初成本下降得更快，然后成本降低的收益就不那么多了。 我们可以尝试使用不同的学习速率和迭代组合，并得到不同学习率和迭代的效果会如何。</p>
<p>让我们建立一个函数，它可以显示效果，也可以显示梯度下降实际上是如何工作的。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def plot\_GD(n\_iter, lr, ax, ax1&#x3D;None):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    n\_iter &#x3D; no of iterations  </span><br><span class="line">    lr &#x3D; Learning Rate  </span><br><span class="line">    ax &#x3D; Axis to plot the Gradient Descent  </span><br><span class="line">    ax1 &#x3D; Axis to plot cost\_history vs Iterations plot  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    ax.plot(X, y, &#39;b.&#39;)  </span><br><span class="line">    theta &#x3D; np.random.randn(2, 1)  </span><br><span class="line">      </span><br><span class="line">    tr &#x3D; 0.1  </span><br><span class="line">    cost\_history &#x3D; np.zeros(n\_iter)  </span><br><span class="line">    for i in range(n\_iter):  </span><br><span class="line">        pred\_prev &#x3D; X\_b.dot(theta)  </span><br><span class="line">        theta, h, \_ &#x3D; gradient\_descent(X\_b, y, theta, lr, 1)  </span><br><span class="line">        pred &#x3D; X\_b.dot(theta)  </span><br><span class="line">          </span><br><span class="line">        cost\_history\[i\] &#x3D; h\[0\]  </span><br><span class="line">          </span><br><span class="line">        if ((i % 25 &#x3D;&#x3D; 0)):  </span><br><span class="line">            ax.plot(X, pred, &#39;r-&#39;, alpha&#x3D;tr)  </span><br><span class="line">            if tr &lt; 0.8:  </span><br><span class="line">                tr +&#x3D; 0.2  </span><br><span class="line">      </span><br><span class="line">    if not ax1 &#x3D;&#x3D; None:  </span><br><span class="line">        ax1.plot(range(n\_iter), cost\_history, &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |<br>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">\# 绘制不同迭代和学习率组合的图  </span><br><span class="line">fig &#x3D; plt.figure(figsize&#x3D;(30,25), dpi&#x3D;200)  </span><br><span class="line">fig.subplots\_adjust(hspace&#x3D;0.4, wspace&#x3D;0.4)  </span><br><span class="line">  </span><br><span class="line">it\_lr &#x3D; \[(2000, 0.001), (500, 0.01), (200, 0.05), (100, 0.1)\]  </span><br><span class="line">count &#x3D; 0  </span><br><span class="line">for n\_iter, lr in it\_lr:  </span><br><span class="line">    count +&#x3D; 1  </span><br><span class="line">      </span><br><span class="line">    ax &#x3D; fig.add\_subplot(4, 2, count)  </span><br><span class="line">    count +&#x3D; 1  </span><br><span class="line">     </span><br><span class="line">    ax1 &#x3D; fig.add\_subplot(4, 2, count)  </span><br><span class="line">      </span><br><span class="line">    ax.set\_title(&quot;lr:&#123;&#125;&quot; .format(lr))  </span><br><span class="line">    ax1.set\_title(&quot;Iterations:&#123;&#125;&quot; .format(n\_iter))  </span><br><span class="line">    plot\_GD(n\_iter, lr, ax, ax1)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_32_0.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_32_0.png"></a></p>
<p>通过观察发现，以较小的学习速率收集解决方案需要很长时间，而学习速度越大，学习速度越快。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\_, ax &#x3D; plt.subplots(figsize&#x3D;(14, 10))  </span><br><span class="line">plot\_GD(100, 0.1, ax)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_34_0.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_34_0.png"></a></p>
<h2 id="随机梯度下降法（Stochastic-Gradient-Descent）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent）"></a><a href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Stochastic-Gradient-Descent%EF%BC%89" title="随机梯度下降法（Stochastic Gradient Descent）"></a>随机梯度下降法（Stochastic Gradient Descent）</h2><p>随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的 m$m$ 个样本的数据，而是仅仅选取一个样本 j$j$ 来求梯度。对应的更新公式是：</p>
<p>θi=θi−α(hθ(x(j)0,x(j)1,…x(j)n)−yj)x(j)i</p>
<p>$$θi=θi−α(hθ(x0(j),x1(j),…xn(j))−yj)xi(j)$$</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def stocashtic\_gradient\_descent(X, y, theta, learning\_rate&#x3D;0.01, iterations&#x3D;10):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    X    &#x3D; Matrix of X with added bias units  </span><br><span class="line">    y    &#x3D; Vector of Y  </span><br><span class="line">    theta&#x3D;Vector of thetas np.random.randn(j,1)  </span><br><span class="line">    learning\_rate   </span><br><span class="line">    iterations &#x3D; no of iterations  </span><br><span class="line">      </span><br><span class="line">    Returns the final theta vector and array of cost history over no of iterations  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    m &#x3D; len(y)  </span><br><span class="line">    cost\_history &#x3D; np.zeros(iterations)  </span><br><span class="line">      </span><br><span class="line">    for it in range(iterations):  </span><br><span class="line">        cost &#x3D; 0.0  </span><br><span class="line">        for i in range(m):  </span><br><span class="line">            rand\_ind &#x3D; np.random.randint(0, m)  </span><br><span class="line">            X\_i &#x3D; X\[rand\_ind, :\].reshape(1, X.shape\[1\])  </span><br><span class="line">            y\_i &#x3D; y\[rand\_ind, :\].reshape(1, 1)  </span><br><span class="line">            prediction &#x3D; np.dot(X\_i, theta)  </span><br><span class="line">              </span><br><span class="line">            theta -&#x3D; (1&#x2F;m) \* learning\_rate \* (X\_i.T.dot((prediction - y\_i)))  </span><br><span class="line">            cost +&#x3D; cal\_cost(theta, X\_i, y\_i)  </span><br><span class="line">        cost\_history\[it\] &#x3D; cost  </span><br><span class="line">          </span><br><span class="line">    return theta, cost\_history  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |<br>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">lr &#x3D; 0.5  </span><br><span class="line">n\_iter &#x3D; 50  </span><br><span class="line">theta &#x3D; np.random.randn(2,1)  </span><br><span class="line">X\_b &#x3D; np.c\_\[np.ones((len(X),1)), X\]  </span><br><span class="line">theta, cost\_history &#x3D; stocashtic\_gradient\_descent(X\_b, y, theta, lr, n\_iter)  </span><br><span class="line">  </span><br><span class="line">print(&#39;Theta0:          &#123;:0.3f&#125;,\\nTheta1:          &#123;:0.3f&#125;&#39; .format(theta\[0\]\[0\],theta\[1\]\[0\]))  </span><br><span class="line">print(&#39;Final cost&#x2F;MSE:  &#123;:0.3f&#125;&#39; .format(cost\_history\[-1\]))  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta0:          3.762,</span><br><span class="line">Theta1:          3.159</span><br><span class="line">Final cost&#x2F;MSE:  46.964 </span><br></pre></td></tr></table></figure>

<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(10,8))  </span><br><span class="line">  </span><br><span class="line">ax.set\_ylabel(&#39;$J(\\Theta)$&#39; ,rotation&#x3D;0)  </span><br><span class="line">ax.set\_xlabel(&#39;$Iterations$&#39;)  </span><br><span class="line">theta &#x3D; np.random.randn(2,1)  </span><br><span class="line">  </span><br><span class="line">ax.plot(range(n\_iter), cost\_history, &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_39_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_39_1.png"></a></p>
<h2 id="小批量梯度下降法（Mini-batch-Gradient-Descent）"><a href="#小批量梯度下降法（Mini-batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a><a href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Mini-batch-Gradient-Descent%EF%BC%89" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a>小批量梯度下降法（Mini-batch Gradient Descent）</h2><p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于 m$m$ 个样本，我们采用 x$x$ 个样子来迭代，1&lt;x&lt;m$1&lt;x&lt;m$。一般可以取 x=10$x=10$，当然根据样本的数据，可以调整这个 x$x$ 的值。对应的更新公式是：</p>
<p>θi=θi−αt+x−1∑j=t(hθ(x(j)0,x(j)1,…x(j)n)−yj)x(j)i</p>
<p>$$θi=θi−α∑j=tt+x−1(hθ(x0(j),x1(j),…xn(j))−yj)xi(j)$$</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def minibatch\_gradient\_descent(X, y, theta, learning\_rate&#x3D;0.01, iterations&#x3D;10, batch\_size&#x3D;20):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    X    &#x3D; Matrix of X without added bias units  </span><br><span class="line">    y    &#x3D; Vector of Y  </span><br><span class="line">    theta&#x3D;Vector of thetas np.random.randn(j,1)  </span><br><span class="line">    learning\_rate   </span><br><span class="line">    iterations &#x3D; no of iterations  </span><br><span class="line">      </span><br><span class="line">    Returns the final theta vector and array of cost history over no of iterations  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    m &#x3D; len(y)  </span><br><span class="line">    cost\_history &#x3D; np.zeros(iterations)  </span><br><span class="line">    n\_batches &#x3D; int(m &#x2F; batch\_size)  </span><br><span class="line">      </span><br><span class="line">    for it in range(iterations):  </span><br><span class="line">        cost &#x3D; 0.0  </span><br><span class="line">        indices &#x3D; np.random.permutation(m)  </span><br><span class="line">        X &#x3D; X\[indices\]  </span><br><span class="line">        y &#x3D; y\[indices\]  </span><br><span class="line">        for i in range(0, m, batch\_size):  </span><br><span class="line">            X\_i &#x3D; X\[i: i+batch\_size\]  </span><br><span class="line">            y\_i &#x3D; y\[i: i+batch\_size\]  </span><br><span class="line">              </span><br><span class="line">            X\_i &#x3D; np.c\_\[np.ones(len(X\_i)), X\_i\]  </span><br><span class="line">            prediction &#x3D; np.dot(X\_i, theta)  </span><br><span class="line">              </span><br><span class="line">            theta -&#x3D; (1&#x2F;m) \* learning\_rate \* (X\_i.T.dot((prediction - y\_i)))  </span><br><span class="line">            cost +&#x3D; cal\_cost(theta, X\_i, y\_i)  </span><br><span class="line">        cost\_history\[it\] &#x3D; cost  </span><br><span class="line">      </span><br><span class="line">    return theta, cost\_history  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |<br>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lr &#x3D; 0.1  </span><br><span class="line">n\_iter &#x3D; 200  </span><br><span class="line">theta &#x3D; np.random.randn(2, 1)  </span><br><span class="line">theta, cost\_history &#x3D; minibatch\_gradient\_descent(X, y, theta, lr, n\_iter)  </span><br><span class="line">  </span><br><span class="line">print(&#39;Theta0:          &#123;:0.3f&#125;,\\nTheta1:          &#123;:0.3f&#125;&#39; .format(theta\[0\]\[0\], theta\[1\]\[0\]))  </span><br><span class="line">print(&#39;Final cost&#x2F;MSE:  &#123;:0.3f&#125;&#39; .format(cost\_history\[-1\]))  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta0:          3.842,</span><br><span class="line">Theta1:          3.146</span><br><span class="line">Final cost&#x2F;MSE:  1090.518 </span><br></pre></td></tr></table></figure>

<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(10,8))  </span><br><span class="line">  </span><br><span class="line">ax.set\_ylabel(&#39;$J(\\Theta)$&#39;, rotation&#x3D;0)  </span><br><span class="line">ax.set\_xlabel(&#39;$Iterations$&#39;)  </span><br><span class="line">theta &#x3D; np.random.randn(2, 1)  </span><br><span class="line">  </span><br><span class="line">ax.plot(range(n\_iter), cost\_history, &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_44_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_44_1.png"></a></p>
<p>_参考_：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f">Gradient Descent in Python</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/5970503.html">梯度下降（Gradient Descent）小结</a></li>
</ul>
<p># <a href="/tags/Gradient-Descent/">Gradient Descent</a>, <a href="/tags/machine-learning/">machine learning</a>, <a href="/tags/python/">python</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>译|Gradient Descent in Python</p><p><a href="http://www.laugh12321.cn/blog/2019/03/09/gradient_descent/">http://www.laugh12321.cn/blog/2019/03/09/gradient_descent/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Laugh</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-03-09</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-10-23</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/GD/">GD</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/SGD/">SGD</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/MBGD/">MBGD</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5f902f1aa809f500123872bf&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/alipay-reward-image.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/wechat-reward-image.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/03/12/gradient_descent_plus/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">机器学习|梯度下降详解</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/02/11/hierarchical_clustering_method/"><span class="level-item">机器学习|层次聚类方法</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.css"><script src="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "15061642d6c31ca955dc88c4f4eda96e",
            repo: "blog",
            owner: "laugh12321",
            clientID: "f831b7db637971edc8c9",
            clientSecret: "9767a6cb27b89b67e2875e8915671499b75a6436",
            admin: ["laugh12321"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars0.githubusercontent.com/u/38065710?s=400&amp;u=80ee4d360562451484188e6f1677fa442720891b&amp;v=4" alt="Laugh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Laugh</p><p class="is-size-6 is-block">The life I want, there is no shortcut.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Northern Hemisphere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/blog/archives"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/blog/categories"><p class="title">29</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/blog/tags"><p class="title">50</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/laugh12321" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/laugh12321"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="BiliBili" href="https://space.bilibili.com/86034462"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/laugh12321"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Steam" href="https://steamcommunity.com/id/laugh12321/"><i class="fab fa-steam"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#梯度下降法-（Gradient-Descent）"><span class="level-left"><span class="level-item">1</span><span class="level-item">梯度下降法 （Gradient Descent）</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Cost-Function-amp-Gradients"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Cost Function &amp; Gradients</span></span></a></li></ul></li><li><a class="level is-mobile" href="#随机梯度下降法（Stochastic-Gradient-Descent）"><span class="level-left"><span class="level-item">2</span><span class="level-item">随机梯度下降法（Stochastic Gradient Descent）</span></span></a></li><li><a class="level is-mobile" href="#小批量梯度下降法（Mini-batch-Gradient-Descent）"><span class="level-left"><span class="level-item">3</span><span class="level-item">小批量梯度下降法（Mini-batch Gradient Descent）</span></span></a></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Ubuntu/"><span class="level-start"><span class="level-item">Ubuntu</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><span class="level-start"><span class="level-item">环境配置</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E5%8D%9A%E5%AE%A2/"><span class="level-start"><span class="level-item">博客</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E5%8D%9A%E5%AE%A2/%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96/"><span class="level-start"><span class="level-item">主题美化</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">15</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">K-近邻算法</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-start"><span class="level-item">人工神经网络</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/"><span class="level-start"><span class="level-item">决策树</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB/"><span class="level-start"><span class="level-item">划分聚类</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB/K-Means/"><span class="level-start"><span class="level-item">K-Means</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92/"><span class="level-start"><span class="level-item">多项式回归</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/"><span class="level-start"><span class="level-item">层次聚类</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA/"><span class="level-start"><span class="level-item">感知机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8F%90%E5%8D%87/"><span class="level-start"><span class="level-item">提升</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"><span class="level-start"><span class="level-item">支持向量机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"><span class="level-start"><span class="level-item">朴素贝叶斯</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><span class="level-start"><span class="level-item">梯度下降</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><span class="level-start"><span class="level-item">线性回归</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A3%85%E8%A2%8B/"><span class="level-start"><span class="level-item">装袋</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"><span class="level-start"><span class="level-item">推荐系统</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"><span class="level-start"><span class="level-item">矩阵分解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"><span class="level-start"><span class="level-item">论文翻译</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"><span class="level-start"><span class="level-item">计算机视觉</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-start"><span class="level-item">卷积神经网络</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">迁移学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"><span class="level-start"><span class="level-item">风格迁移</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">排序算法</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Artificial-Neural-Network/"><span class="tag">Artificial Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/BGD/"><span class="tag">BGD</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/BPR/"><span class="tag">BPR</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Bagging/"><span class="tag">Bagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Blog/"><span class="tag">Blog</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/BoosTing/"><span class="tag">BoosTing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/CV/"><span class="tag">CV</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Cluster/"><span class="tag">Cluster</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ECS/"><span class="tag">ECS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GD/"><span class="tag">GD</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GitHub-Pages/"><span class="tag">GitHub Pages</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Hierarchical-Clustering/"><span class="tag">Hierarchical Clustering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Jekyll/"><span class="tag">Jekyll</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Means/"><span class="tag">K-Means</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Nearest-Neighbors/"><span class="tag">K-Nearest Neighbors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/KNN/"><span class="tag">KNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Kreas/"><span class="tag">Kreas</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAE/"><span class="tag">MAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAPE/"><span class="tag">MAPE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MBGD/"><span class="tag">MBGD</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MF/"><span class="tag">MF</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MSE/"><span class="tag">MSE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/NN/"><span class="tag">NN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Style-Transfer/"><span class="tag">Neural Style Transfer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Next/"><span class="tag">Next</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/RS/"><span class="tag">RS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Recommender-System/"><span class="tag">Recommender System</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SGD/"><span class="tag">SGD</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SVM/"><span class="tag">SVM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Seafile/"><span class="tag">Seafile</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Transfer-Learning/"><span class="tag">Transfer Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/VGG19/"><span class="tag">VGG19</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/"><span class="tag">冒泡排序</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9/"><span class="tag">图像压缩</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"><span class="tag">快速排序</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"><span class="tag">排序算法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/"><span class="tag">插入排序</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><span class="tag">环境配置</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/"><span class="tag">选择排序</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Laugh</a><p class="is-size-7"><span>&copy; 2020 Laugh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>