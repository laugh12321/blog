<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>机器学习|决策树详解 - Laugh&#039;s blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Laugh&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Laugh&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习|决策树详解"><meta property="og:url" content="http://www.laugh12321.cn/blog/2019/02/01/decision_tree/"><meta property="og:site_name" content="Laugh&#039;s blog"><meta property="og:description" content="决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/08.jpg"><meta property="article:published_time" content="2019-02-01T00:00:00.000Z"><meta property="article:modified_time" content="2020-10-21T10:18:01.341Z"><meta property="article:author" content="Laugh"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Decision Tree"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/08.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.laugh12321.cn/blog/2019/02/01/decision_tree/"},"headline":"Laugh's blog","image":["https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/photo/08.jpg"],"datePublished":"2019-02-01T00:00:00.000Z","dateModified":"2020-10-21T10:18:01.341Z","author":{"@type":"Person","name":"Laugh"},"description":"决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。"}</script><link rel="canonical" href="http://www.laugh12321.cn/blog/2019/02/01/decision_tree/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Laugh</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">首页</a><a class="navbar-item" href="/blog/archives">归档</a><a class="navbar-item" href="/blog/categories">分类</a><a class="navbar-item" href="/blog/tags">标签</a><a class="navbar-item" href="/blog/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/08.jpg" alt="机器学习|决策树详解"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-02-01T00:00:00.000Z" title="2019-02-01T00:00:00.000Z">2019-02-01</time>发表</span><span class="level-item"><time dateTime="2020-10-21T10:18:01.341Z" title="2020-10-21T10:18:01.341Z">2020-10-21</time>更新</span><span class="level-item"><a class="link-muted" href="/blog/categories/Machine-Learning/">Machine Learning</a><span> / </span><a class="link-muted" href="/blog/categories/Machine-Learning/Decision-Tree/">Decision Tree</a></span><span class="level-item">1 小时读完 (大约6926个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习|决策树详解</h1><div class="content"><p>决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。  </p>
<a id="more"></a>

<p>举一个通俗的例子，假设在B站工作多年仍然单身的小B和他母亲在给他介绍对象时的一段对话：</p>
<blockquote>
<p>母亲：小B，你都 28 了还是单身，明天亲戚家要来个姑娘要不要去见见。<br>小B：多大年纪？<br>母亲：26。<br>小B：有多高？<br>母亲：165厘米。<br>小B：长的好看不。<br>母亲：还行，比较朴素。<br>小B：温柔不？<br>母亲：看起来挺温柔的，很有礼貌。<br>小B：好，去见见。</p>
</blockquote>
<p>作为程序员的小B的思考逻辑就是典型的决策树分类逻辑，将年龄，身高，长相，是否温柔作为特征，并最后对见或者不见进行决策。其决策逻辑如图所示：</p>
<center>
<img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/1.png">
</center>

<h2 id="决策树算法实现"><a href="#决策树算法实现" class="headerlink" title="决策树算法实现"></a>决策树算法实现</h2><p>其实决策树算法如同上面场景一样，其思想非常容易理解，具体的算法流程为：</p>
<ul>
<li><p><strong>第 1 步</strong>: 数据准备：通过数据清洗和数据处理，将数据整理为没有缺省值的向量。</p>
</li>
<li><p><strong>第 2 步</strong>: 寻找最佳特征：遍历每个特征的每一种划分方式，找到最好的划分特征。</p>
</li>
<li><p><strong>第 3 步</strong>: 生成分支：划分成两个或多个节点。</p>
</li>
<li><p><strong>第 4 步</strong>: 生成决策树：对分裂后的节点分别继续执行2-3步，直到每个节点只有一种类别。</p>
</li>
<li><p><strong>第 5 步</strong>: 决策分类：根据训练决策树模型，将预测数据进行分类。</p>
</li>
</ul>
<h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><p>下面我们依照决策树的算法流程，用 <code>python</code> 来实现决策树构建和分类。首先生成一组数据，数据包含两个类别 <code>man</code> 和 <code>woman</code>,特征分别为:</p>
<ul>
<li><code>hair</code>:头发长短(<code>long</code>:长,<code>short</code>:短)</li>
<li><code>voice</code>:声音粗细(<code>thick</code>:粗,<code>thin</code>:细)</li>
<li><code>height</code>:身高</li>
<li><code>ear_stud</code>:是否带有耳钉(<code>yes</code>:是,<code>no</code>:没有)</li>
</ul>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;生成示例数据  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import numpy as np  </span><br><span class="line">import pandas as pd  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def create\_data():  </span><br><span class="line">    data\_value &#x3D; np.array(  </span><br><span class="line">        \[\[&#39;long&#39;, &#39;thick&#39;, 175, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 168, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 178, &#39;yes&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thick&#39;, 172, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;medium&#39;, 163, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thick&#39;, 180, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 173, &#39;yes&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 174, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 164, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;medium&#39;, 158, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 161, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 166, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 158, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 163, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 161, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 164, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 172, &#39;yes&#39;, &#39;woman&#39;\]\])  </span><br><span class="line">    columns &#x3D; np.array(\[&#39;hair&#39;, &#39;voice&#39;, &#39;height&#39;, &#39;ear\_stud&#39;, &#39;labels&#39;\])  </span><br><span class="line">    data &#x3D; pd.DataFrame(data\_value.reshape(17, 5), columns&#x3D;columns)  </span><br><span class="line">    return data  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>在创建好数据之后，加载并打印出这些数据</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; create\_data()  </span><br><span class="line">data  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a><a href="#%E5%88%92%E5%88%86%E9%80%89%E6%8B%A9" title="划分选择"></a>划分选择</h3><p>在得到数据后，根据算法流程，接下来需要寻找最优的划分特征，随着划分的不断进行，我们尽可能的将划分的分支所包含的样本归于同一类别，即结点的“纯度”越来越高。而常用的特征划分方式为信息增益和增益率。</p>
<h4 id="信息增益（ID3）"><a href="#信息增益（ID3）" class="headerlink" title="信息增益（ID3）"></a><a href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%EF%BC%88ID3%EF%BC%89" title="信息增益（ID3）"></a>信息增益（ID3）</h4><p>在介绍信息增益之前，先引入“信息熵”的概念。“信息熵”是度量样本纯度最常用的一种指标，其公式为：</p>
<p>Ent(D)=−|y|∑k=1pklog2pk(1)</p>
<p>$$(1)Ent(D)=−∑k=1|y|pklog2pk$$</p>
<p>其中 D$D$ 表示样本集合，pk$pk$ 表示第 k$k$ 类样本所占的比例。其中 Ent(D)$Ent(D)$ 的值越小，则 D$D$ 的纯度越高。根据以上数据，在计算数据集的“信息熵”时，|y|$|y|$ 显然只有 <code>man</code>,<code>woman</code> 共 2 种，其中为 <code>man</code> 的概率为 817$817$, <code>woman</code> 的概率为 917$917$,则根据公式(1)得到数据集的纯度为：</p>
<p>Ent(data)=−2∑k=1pklog2pk=−(817log2817+917log2917)=0.9975</p>
<p>$$Ent(data)=−∑k=12pklog2pk=−(817log2817+917log2917)=0.9975$$</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算信息熵  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import math  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_Ent(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    Ent -- 信息熵  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    num\_sample &#x3D; len(data)  # 样本个数  </span><br><span class="line">    label\_counts &#x3D; &#123;&#125;  # 初始化标签统计字典  </span><br><span class="line">    for i in range(num\_sample):  </span><br><span class="line">        each\_data &#x3D; data.iloc\[i, :\]  </span><br><span class="line">        current\_label &#x3D; each\_data\[&quot;labels&quot;\]  # 得到当前元素的标签（label）  </span><br><span class="line">  </span><br><span class="line">        # 如果标签不在当前字典中，添加该类标签并初始化 value&#x3D;0,否则该类标签 value+1  </span><br><span class="line">        if current\_label not in label\_counts.keys():  </span><br><span class="line">            label\_counts\[current\_label\] &#x3D; 0  </span><br><span class="line">        label\_counts\[current\_label\] +&#x3D; 1  </span><br><span class="line">      </span><br><span class="line">    Ent &#x3D; 0.0  # 初始化信息熵  </span><br><span class="line">    for key in label\_counts:  </span><br><span class="line">        prob &#x3D; float(label\_counts\[key\])&#x2F;num\_sample  </span><br><span class="line">        Ent -&#x3D; prob \* math.log(prob, 2)  # 应用信息熵公式计算信息熵  </span><br><span class="line">    return Ent  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>通过计算信息熵函数，计算根节点的信息熵：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">base\_ent &#x3D; get\_Ent(data)  </span><br><span class="line">base\_ent  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9975025463691153 </span><br></pre></td></tr></table></figure>

<p><strong>信息增益</strong> 就是建立在信息熵的基础上，在离散特征 x$x$ 有 M$M$ 个取值，如果用 x$x$ 对样本 D$D$ 来进行划分，就会产生 M$M$ 个分支，其中第 m$m$ 个分支包含了集合 D$D$ 的所有在特征 x$x$ 上取值为 m$m$ 的样本，记为 Dm$Dm$（例如：根据以上生成数据，如果我们用 <code>hair</code> 进行划分，则会产生<code>long</code>，<code>short</code>两个分支，每一个分支中分别包含了整个集合中属于 <code>long</code> 或者 <code>short</code> 的数据）。</p>
<p>考虑到不同分支节点包含样本数不同，给分支赋予权重 |Dm||D|$|Dm||D|$ ,使得样本越多的分支节点影响越大，则 <strong>信息增益</strong> 的公式就可以得到：</p>
<p>Gain(D,x)=Ent(D)−M∑m=1|Dm||D|Ent(Dm)(2)</p>
<p>$$(2)Gain(D,x)=Ent(D)−∑m=1M|Dm||D|Ent(Dm)$$</p>
<p>一般情况下，信息增益越大，则说明用 x$x$ 来划分样本集合 D$D$ 的纯度越高。以 <code>hair</code> 为例，其中它有 <code>short</code> 和 <code>long</code> 两个可能取值，则分别用 D1$D1$ (hair = long) 和 `D^{2} (hair = short)来表示。</p>
<p>其中为 D1$D1$ 的数据编号为 {0，4，6，8，9，10，12，14，15}${0，4，6，8，9，10，12，14，15}$ 共 9 个，在这之中为 <code>man</code> 的有 {0，4，6} 共3 个占比为39$39$，为 <code>woman</code> 的有{8, 9，10，12，14，15}共 6 个占比为69$69$; 同样 D2$D2$ 编号为{1，2，3，5，7，11，13, 16}共 8 个，其中为 <code>man</code> 的有{1，2，3，5，7}共 5 个占比58$58$,为 <code>woman</code> 的有{11，13, 16}共 3 个占比 38$38$,若按照 <code>hair</code> 进行划分，则两个分支点的信息熵为：</p>
<p>Ent(D1)=−(39log239+69log269)=0.449</p>
<p>$$Ent(D1)=−(39log239+69log269)=0.449$$</p>
<p>Ent(D2)=−(58log258+38log238)=0.486</p>
<p>$$Ent(D2)=−(58log258+38log238)=0.486$$</p>
<p>根据信息增益的公式可以计算出 <code>hair</code> 的信息增益为：</p>
<p>Gain(D,hair)=Ent(D)−2∑m=1|Dm||D|Ent(Dm)=0.9975−(917∗0.449+817∗0.486)=0.062</p>
<p>$$Gain(D,hair)=Ent(D)−∑m=12|Dm||D|Ent(Dm)=0.9975−(917∗0.449+817∗0.486)=0.062$$</p>
<p>下面我们用 <code>python</code> 来实现<strong>信息增益（ID3）</strong>算法：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算信息增益  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_gain(data, base\_ent, feature):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    base\_ent -- 根节点的信息熵  </span><br><span class="line">    feature -- 计算信息增益的特征  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    Ent -- 信息熵  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">    feature\_list &#x3D; data\[feature\]  # 得到一个特征的全部取值  </span><br><span class="line">    unique\_value &#x3D; set(feature\_list)  # 特征取值的类别  </span><br><span class="line">    feature\_ent &#x3D; 0.0  </span><br><span class="line">  </span><br><span class="line">    for each\_feature in unique\_value:  </span><br><span class="line">        temp\_data &#x3D; data\[data\[feature\] &#x3D;&#x3D; each\_feature\]  </span><br><span class="line">        weight &#x3D; len(temp\_data)&#x2F;len(feature\_list)  # 计算该特征的权重值  </span><br><span class="line">        temp\_ent &#x3D; weight\*get\_Ent(temp\_data)  </span><br><span class="line">        feature\_ent &#x3D; feature\_ent+temp\_ent  </span><br><span class="line">  </span><br><span class="line">    gain &#x3D; base\_ent - feature\_ent  # 信息增益  </span><br><span class="line">    return gain  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>完成 <strong>信息增益</strong> 函数后，尝试计算特征 <code>hair</code> 的信息增益值。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_gain(data,base\_ent,&#39;hair&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.062200515199107964 </span><br></pre></td></tr></table></figure>

<h4 id="信息增益率（C4-5）"><a href="#信息增益率（C4-5）" class="headerlink" title="信息增益率（C4.5）"></a><a href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87%EF%BC%88C4-5%EF%BC%89" title="信息增益率（C4.5）"></a>信息增益率（C4.5）</h4><p>信息增益也存在许多不足之处，经过大量的实验发现，当信息增益作为标准时，易偏向于取值较多的特征，为了避免这种偏好给预测结果带来的不好影响，可以使用<strong>增益率</strong>来选择最优划分。<strong>增益率</strong>的公式定义为:</p>
<p>GainRatio(D,a)=Gain(D,a)IV(a)(3)</p>
<p>$$(3)GainRatio(D,a)=Gain(D,a)IV(a)$$</p>
<p>其中：</p>
<p>IV(a)=−M∑m=1|Dm||D|log2|Dm||D|(4)</p>
<p>$$(4)IV(a)=−∑m=1M|Dm||D|log2|Dm||D|$$</p>
<p>IV(a)$IV(a)$ 称为特征 a$a$ 的固有值，当 a$a$ 的取值数目越多，则 IV(a)$IV(a)$ 的值通常会比较大。例如：</p>
<p>IV(hair)=−917log2917−817log2817=0.998</p>
<p>$$IV(hair)=−917log2917−817log2817=0.998$$</p>
<p>IV(voice)=−717log2717−517log2517−517log2517=1.566</p>
<p>$$IV(voice)=−717log2717−517log2517−517log2517=1.566$$</p>
<h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a><a href="#%E8%BF%9E%E7%BB%AD%E5%80%BC%E5%A4%84%E7%90%86" title="连续值处理"></a>连续值处理</h3><p>在前面介绍的特征选择中，都是对离散型数据进行处理，但在实际的生活中数据常常会出现连续值的情况，如生成数据中的身高，当数据较少时，可以将每一个值作为一个类别，但当数据量大时，这样是不可取的，在 <strong>C4.5</strong> 算法中采用二分法对连续值进行处理。</p>
<p>对于连续的属性 X$X$ 假设共出现了 n 个不同的取值，将这些取值从小到大排序{x1,x2,x3,…,xn}${x1,x2,x3,…,xn}$ ，其中找一点作为划分点 t ，则将数据划分为两类，大于 t 的为一类，小于 t 的为另一类。而 t 的取值通常为相邻两点的平均数 t=xi+xi+12$t=xi+xi+12$。</p>
<p>则在 n 个连续值之中，可以作为划分点的 t 有 n-1 个。通过遍历可以像离散型一样来考察这些划分点。</p>
<p>Gain(D,X)=Ent(D)−|D&lt;t||D|Ent(D&lt;t)−|D&gt;t||D|Ent(D&gt;t)(5)</p>
<p>$$(5)Gain(D,X)=Ent(D)−|D&lt;t||D|Ent(D&lt;t)−|D&gt;t||D|Ent(D&gt;t)$$</p>
<p>其中得到样本 D$D$ 基于划分点 t 二分后的信息增益，于是我们可以选择使得 Ga∈(D,X)$Ga∈(D,X)$ 值最大的划分点。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line">33  </span><br><span class="line">34  </span><br><span class="line">35  </span><br><span class="line">36  </span><br><span class="line">37  </span><br><span class="line">38  </span><br><span class="line">39  </span><br><span class="line">40  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算连续值的划分点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_splitpoint(data, base\_ent, feature):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    base\_ent -- 根节点的信息熵  </span><br><span class="line">    feature -- 需要划分的连续特征  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    final\_t -- 连续值最优划分点  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    # 将连续值进行排序并转化为浮点类型  </span><br><span class="line">    continues\_value &#x3D; data\[feature\].sort\_values().astype(np.float64)  </span><br><span class="line">    continues\_value &#x3D; \[i for i in continues\_value\]  # 不保留原来的索引  </span><br><span class="line">    t\_set &#x3D; \[\]  </span><br><span class="line">    t\_ent &#x3D; &#123;&#125;  </span><br><span class="line">  </span><br><span class="line">    # 得到划分点 t 的集合  </span><br><span class="line">    for i in range(len(continues\_value)-1):  </span><br><span class="line">        temp\_t &#x3D; (continues\_value\[i\]+continues\_value\[i+1\])&#x2F;2  </span><br><span class="line">        t\_set.append(temp\_t)  </span><br><span class="line">  </span><br><span class="line">    # 计算最优划分点  </span><br><span class="line">    for each\_t in t\_set:  </span><br><span class="line">        # 将大于划分点的分为一类  </span><br><span class="line">        temp1\_data &#x3D; data\[data\[feature\].astype(np.float64) &gt; each\_t\]  </span><br><span class="line">        # 将小于划分点的分为一类  </span><br><span class="line">        temp2\_data &#x3D; data\[data\[feature\].astype(np.float64) &lt; each\_t\]  </span><br><span class="line">        weight1 &#x3D; len(temp1\_data)&#x2F;len(data)  </span><br><span class="line">        weight2 &#x3D; len(temp2\_data)&#x2F;len(data)  </span><br><span class="line">        # 计算每个划分点的信息增益  </span><br><span class="line">        temp\_ent &#x3D; base\_ent-weight1 \* \\  </span><br><span class="line">            get\_Ent(temp1\_data)-weight2\*get\_Ent(temp2\_data)  </span><br><span class="line">        t\_ent\[each\_t\] &#x3D; temp\_ent  </span><br><span class="line">    print(&quot;t\_ent:&quot;, t\_ent)  </span><br><span class="line">    final\_t &#x3D; max(t\_ent, key&#x3D;t\_ent.get)  </span><br><span class="line">    return final\_t  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>实现连续值最优划分点的函数后，寻找 <code>height</code> 连续特征值的划分点。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">final\_t &#x3D; get\_splitpoint(data, base\_ent, &#39;height&#39;)  </span><br><span class="line">final\_t  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t_ent: &#123;158.0: 0.1179805181500242, 159.5: 0.1179805181500242, 161.0: 0.2624392604045631, 162.0: 0.2624392604045631, 163.0: 0.3856047022157598, 163.5: 0.15618502398692893, 164.0: 0.3635040117533678, 165.0: 0.33712865788827096, 167.0: 0.4752766311586692, 170.0: 0.32920899348970845, 172.0: 0.5728389611412551, 172.5: 0.4248356349861979, 173.5: 0.3165383509071513, 174.5: 0.22314940393447813, 176.5: 0.14078143361499595, 179.0: 0.06696192680347068&#125;</span><br><span class="line"></span><br><span class="line">172.0 </span><br></pre></td></tr></table></figure>

<h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a><a href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="算法实现"></a>算法实现</h3><p>在对决策树中最佳特征选择和连续值处理之后，接下来就是对决策树的构建。</p>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86" title="数据预处理"></a>数据预处理</h4><p>首先我们将连续值进行处理，在找到最佳划分点之后，将 &lt;t$&lt;t$ 的值设为 0，将 &gt;=t$&gt;=t$ 的值设为 1。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def choice\_1(x, t):  </span><br><span class="line">    if x &gt; t:  </span><br><span class="line">        return &quot;&gt;&#123;&#125;&quot;.format(t)  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;&lt;&#123;&#125;&quot;.format(t)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">deal\_data &#x3D; data.copy()  </span><br><span class="line">\# 使用lambda和map函数将 height 按照final\_t划分为两个类别  </span><br><span class="line">deal\_data\[&quot;height&quot;\] &#x3D; pd.Series(  </span><br><span class="line">    map(lambda x: choice\_1(int(x), final\_t), deal\_data\[&quot;height&quot;\]))  </span><br><span class="line">deal\_data  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<h4 id="选择最优划分特征"><a href="#选择最优划分特征" class="headerlink" title="选择最优划分特征"></a><a href="#%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%88%92%E5%88%86%E7%89%B9%E5%BE%81" title="选择最优划分特征"></a>选择最优划分特征</h4><p>将数据进行预处理之后，接下来就是选择最优的划分特征。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;选择最优划分特征  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def choose\_feature(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    best\_feature -- 最优的划分特征  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    num\_features &#x3D; len(data.columns) - 1  # 特征数量  </span><br><span class="line">    base\_ent &#x3D; get\_Ent(data)  </span><br><span class="line">    best\_gain &#x3D; 0.0  # 初始化信息增益  </span><br><span class="line">    best\_feature &#x3D; data.columns\[0\]  </span><br><span class="line">    for i in range(num\_features):  # 遍历所有特征  </span><br><span class="line">        temp\_gain &#x3D; get\_gain(data, base\_ent, data.columns\[i\])    # 计算信息增益  </span><br><span class="line">        if (temp\_gain &gt; best\_gain):  # 选择最大的信息增益  </span><br><span class="line">            best\_gain &#x3D; temp\_gain  </span><br><span class="line">            best\_feature &#x3D; data.columns\[i\]  </span><br><span class="line">    return best\_feature  # 返回最优特征  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>完成函数之后，我们首先看看数据集中信息增益值最大的特征是什么？</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">choose\_feature(deal\_data)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;height&#39; </span><br></pre></td></tr></table></figure>

<h4 id="构建决策树"><a href="#构建决策树" class="headerlink" title="构建决策树"></a><a href="#%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91" title="构建决策树"></a>构建决策树</h4><p>在将所有的子模块构建好之后，最后就是对核心决策树的构建，本次实验采用<strong>信息增益（ID3）</strong>的方式构建决策树。在构建的过程中，根据算法流程，我们反复遍历数据集，计算每一个特征的信息增益，通过比较将最好的特征作为父节点，根据特征的值确定分支子节点，然后重复以上操作，直到某一个分支全部属于同一类别，或者遍历完所有的数据特征，当遍历到最后一个特征时，若分支数据依然“不纯”，就将其中数量较多的类别作为子节点。</p>
<p>因此最好采用递归的方式来构建决策树。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;构建决策树  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def create\_tree(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    tree -- 以字典的形式返回决策树  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    feature\_list &#x3D; data.columns\[:-1\].tolist()  </span><br><span class="line">    label\_list &#x3D; data.iloc\[:, -1\]  </span><br><span class="line">    if len(data\[&quot;labels&quot;\].value\_counts()) &#x3D;&#x3D; 1:  </span><br><span class="line">        leaf\_node &#x3D; data\[&quot;labels&quot;\].mode().values  </span><br><span class="line">        return leaf\_node            # 第一个递归结束条件：所有的类标签完全相同  </span><br><span class="line">    if len(feature\_list) &#x3D;&#x3D; 1:  </span><br><span class="line">        leaf\_node &#x3D; data\[&quot;labels&quot;\].mode().values  </span><br><span class="line">        return leaf\_node   # 第二个递归结束条件：用完了所有特征  </span><br><span class="line">    best\_feature &#x3D; choose\_feature(data)   # 最优划分特征  </span><br><span class="line">    tree &#x3D; &#123;best\_feature: &#123;&#125;&#125;  </span><br><span class="line">    feat\_values &#x3D; data\[best\_feature\]  </span><br><span class="line">    unique\_value &#x3D; set(feat\_values)  </span><br><span class="line">    for value in unique\_value:  </span><br><span class="line">        temp\_data &#x3D; data\[data\[best\_feature\] &#x3D;&#x3D; value\]  </span><br><span class="line">        temp\_data &#x3D; temp\_data.drop(\[best\_feature\], axis&#x3D;1)  </span><br><span class="line">        tree\[best\_feature\]\[value\] &#x3D; create\_tree(temp\_data)  </span><br><span class="line">    return tree  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>完成创建决策树函数后，接下来对我们第一棵树进行创建。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tree &#x3D; create\_tree(deal\_data)  </span><br><span class="line">tree  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;height&#39;: &#123;&#39;&lt;172.0&#39;: &#123;&#39;ear_stud&#39;: &#123;&#39;no&#39;: &#123;&#39;voice&#39;: &#123;&#39;thick&#39;: array([&#39;man&#39;], dtype&#x3D;object),</span><br><span class="line">      &#39;medium&#39;: array([&#39;man&#39;], dtype&#x3D;object),</span><br><span class="line">      &#39;thin&#39;: array([&#39;woman&#39;], dtype&#x3D;object)&#125;&#125;,</span><br><span class="line">    &#39;yes&#39;: array([&#39;woman&#39;], dtype&#x3D;object)&#125;&#125;,</span><br><span class="line">  &#39;&gt;172.0&#39;: array([&#39;man&#39;], dtype&#x3D;object)&#125;&#125; </span><br></pre></td></tr></table></figure>

<p>通过字典的方式表示构建好的树，可以通过图像的方式更加直观的了解。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/2.png"></a></p>
<p>通过图形可以看出，在构建决策树时不一定每一个特征都会成为树的节点（如同 <code>hair</code>）。</p>
<h4 id="决策分类"><a href="#决策分类" class="headerlink" title="决策分类"></a><a href="#%E5%86%B3%E7%AD%96%E5%88%86%E7%B1%BB" title="决策分类"></a>决策分类</h4><p>在构建好决策树之后，最终就可以使用未知样本进行预测分类。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;决策分类  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def classify(tree, test):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    test -- 需要测试的数据  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    class\_label -- 分类结果  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    first\_feature &#x3D; list(tree.keys())\[0\]  # 获取根节点  </span><br><span class="line">    feature\_dict &#x3D; tree\[first\_feature\]  # 根节点下的树  </span><br><span class="line">    labels &#x3D; test.columns.tolist()  </span><br><span class="line">    value &#x3D; test\[first\_feature\]\[0\]  </span><br><span class="line">    for key in feature\_dict.keys():  </span><br><span class="line">        if value &#x3D;&#x3D; key:  </span><br><span class="line">            if type(feature\_dict\[key\]).\_\_name\_\_ &#x3D;&#x3D; &#39;dict&#39;:  # 判断该节点是否为叶节点  </span><br><span class="line">                class\_label &#x3D; classify(feature\_dict\[key\], test)  # 采用递归直到遍历到叶节点  </span><br><span class="line">            else:  </span><br><span class="line">                class\_label &#x3D; feature\_dict\[key\]  </span><br><span class="line">    return class\_label  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>在分类函数完成之后，接下来我们尝试对未知数据进行分类。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test &#x3D; pd.DataFrame(&#123;&quot;hair&quot;: \[&quot;long&quot;\], &quot;voice&quot;: \[&quot;thin&quot;\], &quot;height&quot;: \[163\], &quot;ear\_stud&quot;: \[&quot;yes&quot;\]&#125;)  </span><br><span class="line">test  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>对连续值进行预处理。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test\[&quot;height&quot;\] &#x3D; pd.Series(map(lambda x: choice\_1(int(x), final\_t), test\[&quot;height&quot;\]))  </span><br><span class="line">test  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>分类预测。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classify(tree,test)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([&#39;woman&#39;], dtype&#x3D;object) </span><br></pre></td></tr></table></figure>

<p>一个身高 163 厘米，长发，带着耳钉且声音纤细的人，在我们构建的决策树判断后预测为一名女性。</p>
<p>上面的实验中，我们没有考虑 <code>=划分点</code> 的情况，你可以自行尝试将 <code>&gt;=划分点</code> 或 <code>&lt;=划分点</code> 归为一类，看看结果又有哪些不同？</p>
<h3 id="预剪枝和后剪枝"><a href="#预剪枝和后剪枝" class="headerlink" title="预剪枝和后剪枝"></a><a href="#%E9%A2%84%E5%89%AA%E6%9E%9D%E5%92%8C%E5%90%8E%E5%89%AA%E6%9E%9D" title="预剪枝和后剪枝"></a>预剪枝和后剪枝</h3><p>在决策树的构建过程中，特别在数据特征非常多时，为了尽可能正确的划分每一个训练样本，结点的划分就会不停的重复，则一棵决策树的分支就非常多。对于训练集而言，拟合出来的模型是非常完美的。但是，这种完美就使得整体模型的复杂度变高，同时对其他数据集的预测能力下降，也就是我们常说的过拟合使得模型的泛化能力变弱。为了避免过拟合问题的出现，在决策树中最常见的两种方法就是预剪枝和后剪枝。</p>
<h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a><a href="#%E9%A2%84%E5%89%AA%E6%9E%9D" title="预剪枝"></a>预剪枝</h4><p>预剪枝，顾名思义预先减去枝叶，在构建决策树模型的时候，每一次对数据划分之前进行估计，如果当前节点的划分不能带来决策树泛化的提升，则停止划分并将当前节点标记为叶节点。例如前面构造的决策树，按照决策树的构建原则，通过 <code>height</code> 特征进行划分后 <code>&lt;172</code> 分支中又按照 <code>ear_stud</code> 特征值进行继续划分。如果应用预剪枝，则当通过 <code>height</code> 进行特征划分之后，对 <code>&lt;172</code> 分支是否进行 <code>ear_stud</code> 特征进行划分时计算划分前后的准确度，如果划分后的更高则按照 <code>ear_stud</code> 继续划分，如果更低则停止划分。</p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a><a href="#%E5%90%8E%E5%89%AA%E6%9E%9D" title="后剪枝"></a>后剪枝</h4><p>跟预剪枝在构建决策树的过程中判断是否继续特征划分所不同的是，后剪枝在决策树构建好之后对树进行修剪。如果说预剪枝是自顶向下的修剪，那么后剪枝就是自底向上进行修剪。后剪枝将最后的分支节点替换为叶节点，判断是否带来决策树泛化的提升，是则进行修剪，并将该分支节点替换为叶节点，否则不进行修剪。例如在前面构建好决策树之后，<code>&gt;172</code>分支的 <code>voice</code> 特征，将其替换为叶节点如（<code>man</code>），计算替换前后划分准确度，如果替换后准确度更高则进行修剪（用叶节点替换分支节点），否则不修剪。</p>
<h2 id="预测分类"><a href="#预测分类" class="headerlink" title="预测分类"></a><a href="#%E9%A2%84%E6%B5%8B%E5%88%86%E7%B1%BB" title="预测分类"></a>预测分类</h2><p>在前面我们使用 <code>python</code> 将决策树的特征选择，连续值处理和预测分类做了详细的讲解。接下来我们应用决策树模型对真实的数据进行分类预测。</p>
<h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a><a href="#%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE" title="导入数据"></a>导入数据</h3><p>本次应用到的数据为学生成绩数据集 <code>course-13-student.csv</code>，一共有 395 条数据，26 个特征。</p>
<blockquote>
<p>数据集下载 👉 <a target="_blank" rel="noopener" href="http://labfile.oss.aliyuncs.com/courses/1081/course-13-student.csv">传送门</a></p>
</blockquote>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;导入数据集并预览  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import pandas as pd  </span><br><span class="line">  </span><br><span class="line">stu\_grade &#x3D; pd.read\_csv(&#39;course-13-student.csv&#39;)  </span><br><span class="line">stu\_grade.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>由于特征过多，我们选择部分特征作为决策树模型的分类特征,分别为：</p>
<ul>
<li><code>school</code>：学生所读学校(<code>GP</code>，<code>MS</code>)</li>
<li><code>sex</code>: 性别(<code>F</code>：女，<code>M</code>：男)</li>
<li><code>address</code>: 家庭住址(<code>U</code>：城市，<code>R</code>：郊区)</li>
<li><code>Pstatus</code>: 父母状态(<code>A</code>：同居，<code>T</code>：分居)</li>
<li><code>Pedu</code>: 父母学历由低到高</li>
<li><code>reason</code>: 选择这所学校的原因(<code>home</code>：家庭,<code>course</code>：课程设计，<code>reputation</code>：学校地位，<code>other</code>：其他)</li>
<li><code>guardian</code>: 监护人(<code>mother</code>：母亲，<code>father</code>：父亲，<code>other</code>：其他)</li>
<li><code>studytime</code>: 周末学习时长</li>
<li><code>schoolsup</code>: 额外教育支持(<code>yes</code>：有，<code>no</code>：没有)</li>
<li><code>famsup</code>: 家庭教育支持(<code>yes</code>：有，<code>no</code>：没有)</li>
<li><code>paid</code>: 是否上补习班(<code>yes</code>：是，<code>no</code>：否)</li>
<li><code>higher</code>: 是否想受更好的教育(<code>yes</code>：是，<code>no</code>：否)</li>
<li><code>internet</code>: 是否家里联网(<code>yes</code>：是，<code>no</code>：否)</li>
<li><code>G1</code>: 一阶段测试成绩</li>
<li><code>G2</code>: 二阶段测试成绩</li>
<li><code>G3</code>: 最终成绩</li>
</ul>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new\_data &#x3D; stu\_grade.iloc\[:, \[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 15, 24, 25, 26\]\]  </span><br><span class="line">new\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1" title="数据预处理"></a>数据预处理</h3><p>首先我们将成绩 <code>G1</code>，<code>G2</code>，<code>G3</code> 根据分数进行等级划分，将 <code>0-4</code> 划分为 <code>bad</code>，<code>5-9</code> 划分为 <code>medium</code> ，</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def choice\_2(x):  </span><br><span class="line">    x &#x3D; int(x)  </span><br><span class="line">    if x &lt; 5:  </span><br><span class="line">        return &quot;bad&quot;  </span><br><span class="line">    elif x &gt;&#x3D; 5 and x &lt; 10:  </span><br><span class="line">        return &quot;medium&quot;  </span><br><span class="line">    elif x &gt;&#x3D; 10 and x &lt; 15:  </span><br><span class="line">        return &quot;good&quot;  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;excellent&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">stu\_data &#x3D; new\_data.copy()  </span><br><span class="line">stu\_data\[&quot;G1&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G1&quot;\]))  </span><br><span class="line">stu\_data\[&quot;G2&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G2&quot;\]))  </span><br><span class="line">stu\_data\[&quot;G3&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G3&quot;\]))  </span><br><span class="line">stu\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>同样我们对 <code>Pedu</code> （父母教育程度）也进行划分</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def choice\_3(x):  </span><br><span class="line">    x &#x3D; int(x)  </span><br><span class="line">    if x &gt; 3:  </span><br><span class="line">        return &quot;high&quot;  </span><br><span class="line">    elif x &gt; 1.5:  </span><br><span class="line">        return &quot;medium&quot;  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;low&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">stu\_data\[&quot;Pedu&quot;\] &#x3D; pd.Series(map(lambda x: choice\_3(x), stu\_data\[&quot;Pedu&quot;\]))  </span><br><span class="line">stu\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>在等级划分之后，为遵循 <code>scikit-learn</code> 函数的输入规范，需要将数据特征进行替换。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;特征值替换  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def replace\_feature(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    data -- 将特征值替换后的数据集  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    for each in data.columns:  # 遍历每一个特征名称  </span><br><span class="line">        feature\_list &#x3D; data\[each\]  </span><br><span class="line">        unique\_value &#x3D; set(feature\_list)  </span><br><span class="line">        i &#x3D; 0  </span><br><span class="line">        for fea\_value in unique\_value:  </span><br><span class="line">            data\[each\] &#x3D; data\[each\].replace(fea\_value, i)  </span><br><span class="line">            i +&#x3D; 1  </span><br><span class="line">    return data  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>将特征值进行替换后展示。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stu\_data &#x3D; replace\_feature(stu\_data)  </span><br><span class="line">stu\_data.head(10)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<h3 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a><a href="#%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86" title="数据划分"></a>数据划分</h3><p>加载好预处理的数据集之后，为了实现决策树算法，同样我们需要将数据集分为 <strong>训练集</strong>和<strong>测试集</strong>，依照经验：<strong>训练集</strong>占比为 70%，<strong>测试集</strong>占 30%。</p>
<p>同样在此我们使用 <code>scikit-learn</code> 模块的 <code>train_test_split</code> 函数完成数据集切分。  </p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model\_selection import train\_test\_split  </span><br><span class="line">  </span><br><span class="line">x\_train,x\_test, y\_train, y\_test &#x3D;train\_test\_split(train\_data,train\_target,test\_size&#x3D;0.4, random\_state&#x3D;0)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>其中：</p>
<ul>
<li><code>x_train</code>,<code>x_test</code>, <code>y_train</code>, <code>y_test</code> 分别表示，切分后的 特征的训练集，特征的测试集，标签的训练集，标签的测试集；其中特征和标签的值是一一对应的。</li>
<li><code>train_data</code>,<code>train_target</code>分别表示为待划分的特征集和待划分的标签集。</li>
<li><code>test_size</code>：测试样本所占比例。</li>
<li><code>random_state</code>：随机数种子,在需要重复实验时，保证在随机数种子一样时能得到一组一样的随机数。</li>
</ul>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model\_selection import train\_test\_split  </span><br><span class="line">  </span><br><span class="line">x\_train, x\_test, y\_train, y\_test &#x3D; train\_test\_split(stu\_data.iloc\[:, :-1\], stu\_data\[&quot;G3&quot;\],   </span><br><span class="line">                                                    test\_size&#x3D;0.3, random\_state&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">x\_test  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<h3 id="决策树构建"><a href="#决策树构建" class="headerlink" title="决策树构建"></a><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA" title="决策树构建"></a>决策树构建</h3><p>在划分好数据集之后，接下来就是进行预测。在前面的实验中我们采用 <code>python</code> 对决策树算法进行实现，下面我们通过 <code>scikit-learn</code> 来对其进行实现。 <code>scikit-learn</code> 决策树类及常用参数如下：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(criterion&#x3D;’gini’，random\_state&#x3D;None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>其中：</p>
<ul>
<li><code>criterion</code> 表示特征划分方法选择，默认为 <code>gini</code> (在后面会讲到)，可选择为 <code>entropy</code> (信息增益)。</li>
<li><code>ramdom_state</code> 表示随机数种子，当特征特别多时 <code>scikit-learn</code> 为了提高效率，随机选取部分特征来进行特征选择，即找到所有特征中较优的特征。</li>
</ul>
<p>常用方法:</p>
<ul>
<li><code>fit(x,y)</code>训练决策树。</li>
<li><code>predict(X)</code> 对数据集进行预测返回预测结果。</li>
</ul>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier  </span><br><span class="line">  </span><br><span class="line">dt\_model &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;entropy&#39;, random\_state&#x3D;34)  </span><br><span class="line">dt\_model.fit(x\_train,y\_train) # 使用训练集训练模型  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(class_weight&#x3D;None, criterion&#x3D;&#39;entropy&#39;, max_depth&#x3D;None,</span><br><span class="line">            max_features&#x3D;None, max_leaf_nodes&#x3D;None,</span><br><span class="line">            min_impurity_decrease&#x3D;0.0, min_impurity_split&#x3D;None,</span><br><span class="line">            min_samples_leaf&#x3D;1, min_samples_split&#x3D;2,</span><br><span class="line">            min_weight_fraction_leaf&#x3D;0.0, presort&#x3D;False, random_state&#x3D;34,</span><br><span class="line">            splitter&#x3D;&#39;best&#39;) </span><br></pre></td></tr></table></figure>

<h3 id="决策树可视化"><a href="#决策树可视化" class="headerlink" title="决策树可视化"></a><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96" title="决策树可视化"></a>决策树可视化</h3><p>在构建好决策树之后，我们需要对创建好的决策树进行可视化展示，引入 <code>export_graphviz</code> 进行画图。由于环境中没有函数需要进行安装。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\# Linux  </span><br><span class="line">!apt-get install --yes graphviz # 安装所需模块  </span><br><span class="line">!pip install graphviz  </span><br><span class="line">  </span><br><span class="line">\# windows Anaconda  </span><br><span class="line">conda install graphviz  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>下面开始生成决策树图像，其中生成决策树较大需要拖动滑动条进行查看。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import export\_graphviz  </span><br><span class="line">import graphviz  </span><br><span class="line">  </span><br><span class="line">img &#x3D; export\_graphviz(  </span><br><span class="line">    dt\_model, out\_file&#x3D;None,  </span><br><span class="line">    feature\_names&#x3D;stu\_data.columns\[:-1\].values.tolist(),  # 传入特征名称  </span><br><span class="line">    class\_names&#x3D;np.array(\[&quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;, &quot;excellent&quot;\]),  # 传入类别值  </span><br><span class="line">    filled&#x3D;True, node\_ids&#x3D;True,  </span><br><span class="line">    rounded&#x3D;True)  </span><br><span class="line">  </span><br><span class="line">graphviz.Source(img)  # 展示决策树  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/output_157_0.svg"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/output_157_0.svg" alt="svg">svg</a></p>
<h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a><a href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B" title="模型预测"></a>模型预测</h3><p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y\_predict &#x3D; dt\_model.predict(x\_test) # 使用模型对测试集进行预测  </span><br><span class="line">y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 1, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 3, 0, 3, 2, 0, 3, 2, 2, 0,</span><br><span class="line">       3, 2, 2, 0, 3, 3, 2, 2, 0, 0, 0, 2, 1, 0, 1, 2, 3, 0, 3, 3, 2, 2,</span><br><span class="line">       0, 2, 2, 2, 2, 2, 3, 2, 2, 0, 3, 0, 2, 2, 2, 1, 3, 0, 2, 2, 2, 2,</span><br><span class="line">       3, 3, 2, 0, 0, 0, 1, 2, 2, 0, 0, 3, 0, 3, 2, 3, 2, 2, 3, 1, 0, 0,</span><br><span class="line">       0, 2, 1, 2, 2, 2, 2, 3, 0, 0, 3, 0, 0, 2, 3, 2, 1, 2, 2, 0, 0, 2,</span><br><span class="line">       0, 2, 0, 3, 2, 2, 2, 3, 2], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure>

<h3 id="分类准确率计算"><a href="#分类准确率计算" class="headerlink" title="分类准确率计算"></a><a href="#%E5%88%86%E7%B1%BB%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97" title="分类准确率计算"></a>分类准确率计算</h3><p>当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。</p>
<p>accur=∑Ni=1I(¯yi=yi)N(6)</p>
<p>$$(6)accur=∑i=1NI(yi¯=yi)N$$</p>
<p>公式(6)中 N$N$ 表示数据总条数，¯yi$yi¯$ 表示第 i$i$ 条数据的种类预测值，yi$yi$ 表示第 i$i$ 条数据的种类真实值，I$I$ 同样是指示函数，表示 ¯yi$yi¯$ 和 yi$yi$ 相同的个数。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;准确率计算  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def get\_accuracy(test\_labels, pred\_labels):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    test\_labels -- 测试集的真实值  </span><br><span class="line">    pred\_labels -- 测试集的预测值  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    accur -- 准确率  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    correct &#x3D; np.sum(test\_labels &#x3D;&#x3D; pred\_labels)  # 计算预测正确的数据个数  </span><br><span class="line">    n &#x3D; len(test\_labels)  # 总测试集数据个数  </span><br><span class="line">    accur &#x3D; correct&#x2F;n  </span><br><span class="line">    return accur  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">get\_accuracy(y\_test, y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.6974789915966386 </span><br></pre></td></tr></table></figure>

<h2 id="CART-决策树"><a href="#CART-决策树" class="headerlink" title="CART 决策树"></a><a href="#CART-%E5%86%B3%E7%AD%96%E6%A0%91" title="CART 决策树"></a>CART 决策树</h2><p>分类与回归树（classification and regression tree, CART）同样也是应用广泛的决策树学习算法，CART 算法是按照特征划分，由树的生成和树的剪枝构成，既可以进行分类又可以用于回归，按照作用将其分为决策树和回归树，由于本实验设计为决策树的概念，所以回归树的部分有兴趣的同学可以自己查找相关资料进一步学习。</p>
<p>CART决策树的构建和常见的 <strong>ID3</strong> 和 <strong>C4.5</strong> 算法的流程相似，但在特征划分选择上CART选择了 <strong>基尼指数</strong> 作为划分标准。数据集 D$D$ 的纯度可用基尼值来度量：</p>
<p>Gini(D)=|y|∑y=1∑k′≠kpkp′k(7)</p>
<p>$$(7)Gini(D)=∑y=1|y|∑k′≠kpkpk′$$</p>
<p><strong>基尼指数</strong>表示随机抽取两个样本，两个样本类别不一致的概率，<strong>基尼指数</strong>越小则数据集的纯度越高。同样对于每一个特征值的基尼指数计算，其和 <strong>ID3</strong> 、 <strong>C4.5</strong> 相似，定义为：</p>
<p>GiniValue(D,a)=M∑m=1|Dm||D|Gini(Dm)(8)</p>
<p>$$(8)GiniValue(D,a)=∑m=1M|Dm||D|Gini(Dm)$$</p>
<p>在进行特征划分的时候，选择特征中基尼值最小的作为最优特征划分点。</p>
<p>实际上，在应用过程中，更多的会使用 <strong>基尼指数</strong> 对特征划分点进行决策，最重要的原因是计算复杂度相较于 <strong>ID3</strong> 和 <strong>C4.5</strong> 小很多（没有对数运算）。</p>
<p><strong>拓展阅读：</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-hans/%E5%86%B3%E7%AD%96%E6%A0%91">决策树- 维基百科</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>机器学习|决策树详解</p><p><a href="http://www.laugh12321.cn/blog/2019/02/01/decision_tree/">http://www.laugh12321.cn/blog/2019/02/01/decision_tree/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Laugh</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-02-01</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-10-21</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Decision-Tree/">Decision Tree</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/alipay-reward-image.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/wechat-reward-image.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/01/31/neural_network/"><span class="level-item">机器学习|感知机和人工神经网络详解</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars0.githubusercontent.com/u/38065710?s=400&amp;u=80ee4d360562451484188e6f1677fa442720891b&amp;v=4" alt="Laugh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Laugh</p><p class="is-size-6 is-block">The life I want, there is no shortcut.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Northern Hemisphere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/blog/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/blog/categories"><p class="title">15</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/blog/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/laugh12321" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/blog/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#决策树算法实现"><span class="level-left"><span class="level-item">1</span><span class="level-item">决策树算法实现</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#数据生成"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">数据生成</span></span></a></li><li><a class="level is-mobile" href="#划分选择"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">划分选择</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#信息增益（ID3）"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">信息增益（ID3）</span></span></a></li><li><a class="level is-mobile" href="#信息增益率（C4-5）"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">信息增益率（C4.5）</span></span></a></li></ul></li><li><a class="level is-mobile" href="#连续值处理"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">连续值处理</span></span></a></li><li><a class="level is-mobile" href="#算法实现"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">算法实现</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#数据预处理"><span class="level-left"><span class="level-item">1.4.1</span><span class="level-item">数据预处理</span></span></a></li><li><a class="level is-mobile" href="#选择最优划分特征"><span class="level-left"><span class="level-item">1.4.2</span><span class="level-item">选择最优划分特征</span></span></a></li><li><a class="level is-mobile" href="#构建决策树"><span class="level-left"><span class="level-item">1.4.3</span><span class="level-item">构建决策树</span></span></a></li><li><a class="level is-mobile" href="#决策分类"><span class="level-left"><span class="level-item">1.4.4</span><span class="level-item">决策分类</span></span></a></li></ul></li><li><a class="level is-mobile" href="#预剪枝和后剪枝"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">预剪枝和后剪枝</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#预剪枝"><span class="level-left"><span class="level-item">1.5.1</span><span class="level-item">预剪枝</span></span></a></li><li><a class="level is-mobile" href="#后剪枝"><span class="level-left"><span class="level-item">1.5.2</span><span class="level-item">后剪枝</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#预测分类"><span class="level-left"><span class="level-item">2</span><span class="level-item">预测分类</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#导入数据"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">导入数据</span></span></a></li><li><a class="level is-mobile" href="#数据预处理-1"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">数据预处理</span></span></a></li><li><a class="level is-mobile" href="#数据划分"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">数据划分</span></span></a></li><li><a class="level is-mobile" href="#决策树构建"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">决策树构建</span></span></a></li><li><a class="level is-mobile" href="#决策树可视化"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">决策树可视化</span></span></a></li><li><a class="level is-mobile" href="#模型预测"><span class="level-left"><span class="level-item">2.6</span><span class="level-item">模型预测</span></span></a></li><li><a class="level is-mobile" href="#分类准确率计算"><span class="level-left"><span class="level-item">2.7</span><span class="level-item">分类准确率计算</span></span></a></li></ul></li><li><a class="level is-mobile" href="#CART-决策树"><span class="level-left"><span class="level-item">3</span><span class="level-item">CART 决策树</span></span></a></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Algorithm/Sorting-algorithm/"><span class="level-start"><span class="level-item">Sorting algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Decision-Tree/"><span class="level-start"><span class="level-item">Decision Tree</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/K-Nearest-Neighbors/"><span class="level-start"><span class="level-item">K-Nearest Neighbors</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Linear-Regression/"><span class="level-start"><span class="level-item">Linear Regression</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Naive-Bayes/"><span class="level-start"><span class="level-item">Naive Bayes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Polynomial-Regression/"><span class="level-start"><span class="level-item">Polynomial Regression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Support-Vector-Machine/"><span class="level-start"><span class="level-item">Support Vector Machine</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Ubuntu/"><span class="level-start"><span class="level-item">Ubuntu</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/"><span class="level-start"><span class="level-item">ECS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/Seafile/"><span class="level-start"><span class="level-item">Seafile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Blog/"><span class="tag">Blog</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ECS/"><span class="tag">ECS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GitHub-Pages/"><span class="tag">GitHub Pages</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Jekyll/"><span class="tag">Jekyll</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Nearest-Neighbors/"><span class="tag">K-Nearest Neighbors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAE/"><span class="tag">MAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAPE/"><span class="tag">MAPE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MSE/"><span class="tag">MSE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Next/"><span class="tag">Next</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Seafile/"><span class="tag">Seafile</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Sorting-algorithm/"><span class="tag">Sorting algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Support-Vector-Machine/"><span class="tag">Support Vector Machine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Laugh</a><p class="is-size-7"><span>&copy; 2020 Laugh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>