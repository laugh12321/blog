<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>机器学习|划分聚类之 K-Means 详解 - Laugh&#039;s blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Laugh&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Laugh&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。 在划分的过程中，首先由用户确定划分子集的个数 k$k$，然后随机选定 k$k$ 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 k$k$ 个子集，即将数据划分为 k$k$ 类。 而评估划分的好坏标准就是：保证同一"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习|划分聚类之 K-Means 详解"><meta property="og:url" content="http://www.laugh12321.cn/blog/2019/02/09/cluster_of_k-means/"><meta property="og:site_name" content="Laugh&#039;s blog"><meta property="og:description" content="划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。 在划分的过程中，首先由用户确定划分子集的个数 k$k$，然后随机选定 k$k$ 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 k$k$ 个子集，即将数据划分为 k$k$ 类。 而评估划分的好坏标准就是：保证同一"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/10.jpg"><meta property="article:published_time" content="2019-02-09T00:00:00.000Z"><meta property="article:modified_time" content="2020-10-22T08:04:02.957Z"><meta property="article:author" content="Laugh"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="K-Means"><meta property="article:tag" content="Cluster"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/10.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.laugh12321.cn/blog"},"headline":"Laugh's blog","image":["https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/photo/10.jpg"],"datePublished":"2019-02-09T00:00:00.000Z","dateModified":"2020-10-22T08:04:02.957Z","author":{"@type":"Person","name":"laugh12321"},"description":"划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。 在划分的过程中，首先由用户确定划分子集的个数 k$k$，然后随机选定 k$k$ 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 k$k$ 个子集，即将数据划分为 k$k$ 类。 而评估划分的好坏标准就是：保证同一"}</script><link rel="canonical" href="http://www.laugh12321.cn/blog/2019/02/09/cluster_of_k-means/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-180999768-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-180999768-1');</script><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><link rel="alternate" href="/blog/atom.xml" title="Laugh's blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Laugh</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">首页</a><a class="navbar-item" href="/blog/archives">归档</a><a class="navbar-item" href="/blog/categories">分类</a><a class="navbar-item" href="/blog/tags">标签</a><a class="navbar-item" href="/blog/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/10.jpg" alt="机器学习|划分聚类之 K-Means 详解"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-02-09T00:00:00.000Z" title="2019-02-09T00:00:00.000Z">2019-02-09</time>发表</span><span class="level-item"><time dateTime="2020-10-22T08:04:02.957Z" title="2020-10-22T08:04:02.957Z">2020-10-22</time>更新</span><span class="level-item"><a class="link-muted" href="/blog/categories/Machine-Learning/">Machine Learning</a><span> / </span><a class="link-muted" href="/blog/categories/Machine-Learning/Cluster/">Cluster</a><span> / </span><a class="link-muted" href="/blog/categories/Machine-Learning/Cluster/K-Means/">K-Means</a></span><span class="level-item">1 小时读完 (大约7494个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习|划分聚类之 K-Means 详解</h1><div class="content"><p>划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。</p>
<p>在划分的过程中，首先由用户确定划分子集的个数 k$k$，然后随机选定 k$k$ 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 k$k$ 个子集，即将数据划分为 k$k$ 类。</p>
<p>而评估划分的好坏标准就是：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。</p>
<a id="more"></a>

<h2 id="K-Means-聚类方法"><a href="#K-Means-聚类方法" class="headerlink" title="K-Means 聚类方法"></a>K-Means 聚类方法</h2><p>在划分聚类中，K-Means 是最具有代表性的算法，下面用图片的方式演示 K-Means 的基本算法流程。希望大家能通过简单的图文演示，对 K-Means 方法的原理过程产生大致的印象。</p>
<p><strong>[1] 对于未聚类数据集，首先随机初始化 K 个（代表拟聚类簇个数）中心点，如图红色五角星所示。</strong></p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/1.png"></a></p>
<p><strong>[2] 每一个样本按照距离自身最近的中心点进行聚类，等效于通过两中心点连线的中垂线划分区域。</strong></p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/2.png"></a></p>
<p><strong>[3] 依据上次聚类结果，移动中心点到个簇的质心位置，并将此质心作为新的中心点</strong></p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/3.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/3.png"></a></p>
<p><strong>[4] 反复迭代，直至中心点的变化满足收敛条件（变化很小或几乎不变化），最终得到聚类结果。</strong></p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/4.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/4.png"></a></p>
<p>在对 K-Means 有了一个直观了解后，下面我们用 Python 来进行实现。</p>
<h3 id="生成示例数据"><a href="#生成示例数据" class="headerlink" title="生成示例数据"></a><a href="#%E7%94%9F%E6%88%90%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE" title="生成示例数据"></a>生成示例数据</h3><p>首先通过 <code>scikit-learn</code> 模块的 <code>make_blobs()</code> 函数生成本次实验所需的示例数据。该方法可以按照我们的要求，生成特定的团状数据。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data,label &#x3D; sklearn.datasets.make\_blobs(n\_samples&#x3D;100,n\_features&#x3D;2,centers&#x3D;3,center\_box&#x3D;(-10.0,10.0),random\_state&#x3D;None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>其中参数为：</p>
<ul>
<li><code>n_samples</code>：表示生成数据总个数,默认为 100 个。</li>
<li><code>n_features</code>：表示每一个样本的特征个数，默认为 2 个。</li>
<li><code>centers</code>：表示中心点的个数，默认为 3 个。</li>
<li><code>center_box</code>：表示每一个中心的边界,默认为 -10.0到10.0。</li>
<li><code>random_state</code>：表示生成数据的随机数种子。</li>
</ul>
<p>返回值为：</p>
<ul>
<li><code>data</code>：表示数据信息。</li>
<li><code>label</code>：表示数据类别。</li>
</ul>
<p>根据上面函数，在 0.0 到 10.0 上生成 200 条数据，大致包含 3 个中心。由于是用于演示聚类效果，数据标签就不是必须的了，在生成数据时赋值给 <code>_</code>，后面也不会使用到。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;构造数据  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">from sklearn.datasets import make\_blobs  </span><br><span class="line">  </span><br><span class="line">blobs, \_ &#x3D; make\_blobs(n\_samples&#x3D;200, centers&#x3D;3, random\_state&#x3D;18)  </span><br><span class="line">blobs\[:10\] # 打印出前 10 条数据的信息  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[ 8.28390539,  4.98011149],</span><br><span class="line">       [ 7.05638504,  7.00948082],</span><br><span class="line">       [ 7.43101466, -6.56941148],</span><br><span class="line">       [ 8.20192526, -6.4442691 ],</span><br><span class="line">       [ 3.15614247,  0.46193832],</span><br><span class="line">       [ 7.7037692 ,  6.14317389],</span><br><span class="line">       [ 5.62705611, -0.35067953],</span><br><span class="line">       [ 7.53828533, -4.86595492],</span><br><span class="line">       [ 8.649291  ,  3.98488194],</span><br><span class="line">       [ 7.91651636,  4.54935348]]) </span><br></pre></td></tr></table></figure>

<h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a><a href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96" title="数据可视化"></a>数据可视化</h3><p>为了更加直观的查看数据分布情况，使用 <code>matplotlib</code> 将生成数据绘画出来。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;数据展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">%matplotlib inline  </span><br><span class="line">import matplotlib.pyplot as plt  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20);  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_21_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_21_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd80877b8&gt;"></a></p>
<h3 id="随机初始化中心点"><a href="#随机初始化中心点" class="headerlink" title="随机初始化中心点"></a><a href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%AD%E5%BF%83%E7%82%B9" title="随机初始化中心点"></a>随机初始化中心点</h3><p>当我们得到数据时，依照划分聚类方法的思想，首先需要随机选取 k$k$ 个点作为每一个子集的中心点。从图像中，通过肉眼很容易的发现该数据集有 <code>3</code> 个子集。接下来，用 <code>numpy</code> 模块随机生成 <code>3</code> 个中心点，为了更方便展示，这里我们加入了随机数种子以便每一次运行结果相同。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;初始化中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import numpy as np  </span><br><span class="line">  </span><br><span class="line">def random\_k(k, data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    init\_centers -- 初始化中心点  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    prng &#x3D; np.random.RandomState(27) # 定义随机种子  </span><br><span class="line">    num\_feature&#x3D;np.shape(data)\[1\]  </span><br><span class="line">    init\_centers &#x3D; prng.randn(k, num\_feature)\*5 # 由于初始化的随机数是从-1到1，为了更加贴近数据集这里乘了一个 5  </span><br><span class="line">    return init\_centers  </span><br><span class="line">  </span><br><span class="line">init\_centers&#x3D;random\_k(3, blobs)  </span><br><span class="line">init\_centers  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 6.42802708, -1.51776689],</span><br><span class="line">       [ 3.09537831,  1.97999275],</span><br><span class="line">       [ 1.11702824, -0.27169709]]) </span><br></pre></td></tr></table></figure>

<p>在随机生成好中心点之后，将其在图像中表示出来，这里同样使用红色五角星表示。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;初始中心点展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20);  </span><br><span class="line">plt.scatter(init\_centers\[:,0\], init\_centers\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_26_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_26_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd849b128&gt;"></a></p>
<h3 id="计算样本与中心点的距离"><a href="#计算样本与中心点的距离" class="headerlink" title="计算样本与中心点的距离"></a><a href="#%E8%AE%A1%E7%AE%97%E6%A0%B7%E6%9C%AC%E4%B8%8E%E4%B8%AD%E5%BF%83%E7%82%B9%E7%9A%84%E8%B7%9D%E7%A6%BB" title="计算样本与中心点的距离"></a>计算样本与中心点的距离</h3><p>为了找到最合适的中心点位置，需要计算每一个样本和中心点的距离，从而根据距离更新中心点位置。常见的距离计算方法有欧几里得距离和余弦相似度，本实验采用更常见且更易于理解的欧几里得距离（欧式距离）。</p>
<p>欧式距离源自 N$N$ 维欧氏空间中两点之间的距离公式。表达式如下:</p>
<p>deuc= ⎷N∑i=1(Xi−Yi)2(1)</p>
<p>$$(1)deuc=∑i=1N(Xi−Yi)2$$</p>
<p>其中：</p>
<ul>
<li>X$X$, Y$Y$ ：两个数据点</li>
<li>N$N$：每个数据中有 N$N$ 个特征值，</li>
<li>Xi$Xi$ ：数据 X$X$ 的第 i$i$ 个特征值</li>
</ul>
<p>将两个数据 X$X$ 和 Y$Y$ 中的每一个对应的特征值之间差值的平方，再求和，最后开平方，便是欧式距离。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算欧氏距离  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def d\_euc(x, y):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    x -- 数据 a  </span><br><span class="line">    y -- 数据 b  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    d -- 数据 a 和 b 的欧氏距离  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    d &#x3D; np.sqrt(np.sum(np.square(x - y)))  </span><br><span class="line">    return d  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<h3 id="最小化-SSE，更新聚类中心"><a href="#最小化-SSE，更新聚类中心" class="headerlink" title="最小化 SSE，更新聚类中心"></a><a href="#%E6%9C%80%E5%B0%8F%E5%8C%96-SSE%EF%BC%8C%E6%9B%B4%E6%96%B0%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83" title="最小化 SSE，更新聚类中心"></a>最小化 SSE，更新聚类中心</h3><p>和第一章的回归算法通过减小目标函数（如：损失函数）的值拟合数据集一样，聚类算法通常也是优化一个目标函数，从而提高聚类的质量。在聚类算法中，常常使用误差的平方和 SSE（Sum of squared errors）作为度量聚类效果的标准，当 SSE 越小表示聚类效果越好。其中 SSE 表示为：</p>
<p>SSE(C)=K∑k=1∑xi∈Ck|xi−ck|2(2)</p>
<p>$$(2)SSE(C)=∑k=1K∑xi∈Ck|xi−ck|2$$</p>
<p>其中数据集 D={x1,x2,…,xn}$D={x1,x2,…,xn}$，xi$xi$表示每一个样本值，C$C$ 表示通过 K-Means 聚类分析后的产生类别集合 C={C1,C2,…,CK}$C={C1,C2,…,CK}$ ，ck$ck$ 是类别 Ck$Ck$ 的中心点，其中 ck$ck$ 计算方式为：</p>
<p>ck=∑xi∈CkxiI(Ck)(3)</p>
<p>$$(3)ck=∑xi∈CkxiI(Ck)$$</p>
<p>I(Ck)$I(Ck)$ 表示在第 k$k$ 个集合 Ck$Ck$ 中数据的个数。</p>
<p>当然，我们希望同最小化损失函数一样，最小化 SSE 函数，从而找出最优化的聚类模型，但是求其最小值并不容易，是一个 NP 难（非确定性多项式）的问题，其中 NP 难问题是一个经典图论问题，至今也没有找到一个完美且有效的算法。</p>
<p>下面我们对中心点的更新用代码的方式进行实现：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;中心点的更新  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def update\_center(clusters, data, centers):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    clusters -- 每一点分好的类别  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    centers -- 中心点集合  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    new\_centers.reshape(num\_centers,num\_features) -- 新中心点集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">    num\_centers &#x3D; np.shape(centers)\[0\]  # 中心点的个数  </span><br><span class="line">    num\_features &#x3D; np.shape(centers)\[1\]  # 每一个中心点的特征数  </span><br><span class="line">    container &#x3D; \[\]  </span><br><span class="line">    for x in range(num\_centers):  </span><br><span class="line">        each\_container &#x3D; \[\]  </span><br><span class="line">        container.append(each\_container)  # 首先创建一个容器,将相同类别数据存放到一起  </span><br><span class="line">  </span><br><span class="line">    for i, cluster in enumerate(clusters):  </span><br><span class="line">        container\[cluster\].append(data\[i\])  </span><br><span class="line">  </span><br><span class="line">    # 为方便计算，将 list 类型转换为 np.array 类型  </span><br><span class="line">    container &#x3D; np.array(list(map(lambda x: np.array(x), container)))  </span><br><span class="line">  </span><br><span class="line">    new\_centers &#x3D; np.array(\[\])  # 创建一个容器，存放中心点的坐标  </span><br><span class="line">    for i in range(len(container)):  </span><br><span class="line">        each\_center &#x3D; np.mean(container\[i\], axis&#x3D;0)  # 计算每一子集中数据均值作为中心点  </span><br><span class="line">        new\_centers &#x3D; np.append(new\_centers, each\_center)  </span><br><span class="line">  </span><br><span class="line">    return new\_centers.reshape(num\_centers, num\_features)  # 以矩阵的方式返回中心点坐标  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<h3 id="K-Means-聚类算法实现"><a href="#K-Means-聚类算法实现" class="headerlink" title="K-Means 聚类算法实现"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="K-Means 聚类算法实现"></a>K-Means 聚类算法实现</h3><p>K-Means 算法则采用的是迭代算法，避开优化 SSE 函数，通过不断移动中心点的距离，最终达到聚类的效果。</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><a href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B" title="算法流程"></a>算法流程</h4><ol>
<li>初始化中心点：判断数据集可能被分为 k$k$ 个子集，随机生成 k$k$ 个随机点作为每一个子集的中心点。</li>
<li>距离计算，类别标记：样本和每一个中心点进行距离计算，将距离最近的中心点所代表的类别标记为该样本的类别。</li>
<li>中心点位置更新：计算每一个类别中的所有样本的均值，作为新的中心点位置。</li>
<li>重复 2，3 步骤，直到中心点位置不再变化。</li>
</ol>
<h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a><a href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="算法实现"></a>算法实现</h4><p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line">33  </span><br><span class="line">34  </span><br><span class="line">35  </span><br><span class="line">36  </span><br><span class="line">37  </span><br><span class="line">38  </span><br><span class="line">39  </span><br><span class="line">40  </span><br><span class="line">41  </span><br><span class="line">42  </span><br><span class="line">43  </span><br><span class="line">44  </span><br><span class="line">45  </span><br><span class="line">46  </span><br><span class="line">47  </span><br><span class="line">48  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;K-Means 聚类  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def kmeans\_cluster(data, init\_centers, k):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    init\_centers -- 初始化中心点集合  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    centers\_container -- 每一次更新中心点的集合  </span><br><span class="line">    cluster\_container -- 每一次更新类别的集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    max\_step &#x3D; 50  # 定义最大迭代次数，中心点最多移动的次数。  </span><br><span class="line">    epsilon &#x3D; 0.001  # 定义一个足够小的数，通过中心点变化的距离是否小于该数，判断中心点是否变化。  </span><br><span class="line">  </span><br><span class="line">    old\_centers &#x3D; init\_centers  </span><br><span class="line">  </span><br><span class="line">    centers\_container &#x3D; \[\]  # 建立一个中心点容器，存放每一次变化后的中心点，以便后面的绘图。  </span><br><span class="line">    cluster\_container &#x3D; \[\]  # 建立一个分类容器，存放每一次中心点变化后数据的类别  </span><br><span class="line">    centers\_container.append(old\_centers)  </span><br><span class="line">  </span><br><span class="line">    for step in range(max\_step):  </span><br><span class="line">        cluster &#x3D; np.array(\[\], dtype&#x3D;int)  </span><br><span class="line">        for each\_data in data:  </span><br><span class="line">            distances &#x3D; np.array(\[\])  </span><br><span class="line">            for each\_center in old\_centers:  </span><br><span class="line">                temp\_distance &#x3D; d\_euc(each\_data, each\_center)  # 计算样本和中心点的欧式距离  </span><br><span class="line">                distances &#x3D; np.append(distances, temp\_distance)  </span><br><span class="line">            lab &#x3D; np.argmin(distances)  # 返回距离最近中心点的索引，即按照最近中心点分类  </span><br><span class="line">            cluster &#x3D; np.append(cluster, lab)  </span><br><span class="line">        cluster\_container.append(cluster)  </span><br><span class="line">  </span><br><span class="line">        new\_centers &#x3D; update\_center(cluster, data, old\_centers)  # 根据子集分类更新中心点  </span><br><span class="line">  </span><br><span class="line">        # 计算每个中心点更新前后之间的欧式距离  </span><br><span class="line">        difference &#x3D; \[\]  </span><br><span class="line">        for each\_old\_center, each\_new\_center in zip(old\_centers, new\_centers):  </span><br><span class="line">            difference.append(d\_euc(each\_old\_center, each\_new\_center))  </span><br><span class="line">          </span><br><span class="line">        if (np.array(difference) &lt; epsilon).all():  # 判断每个中心点移动是否均小于 epsilon  </span><br><span class="line">            return centers\_container, cluster\_container  </span><br><span class="line">  </span><br><span class="line">        centers\_container.append(new\_centers)  </span><br><span class="line">        old\_centers &#x3D; new\_centers  </span><br><span class="line">  </span><br><span class="line">    return centers\_container, cluster\_container  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>完成 K-Means 聚类函数后，接下来用函数得到最终中心点的位置。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算最终中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">centers\_container, cluster\_container &#x3D; kmeans\_cluster(blobs, init\_centers, 3)  </span><br><span class="line">final\_center &#x3D; centers\_container\[-1\]  </span><br><span class="line">final\_cluster &#x3D; cluster\_container\[-1\]  </span><br><span class="line">final\_center  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 7.67007252, -6.44697348],</span><br><span class="line">       [ 6.83832746,  4.98604668],</span><br><span class="line">       [ 3.28477676,  0.15456871]]) </span><br></pre></td></tr></table></figure>

<p>最后，我们把聚类得到的中心绘制到原图中看一看聚类效果。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;可视化展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;final\_cluster);  </span><br><span class="line">plt.scatter(final\_center\[:,0\], final\_center\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_50_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_50_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd89e8438&gt;"></a></p>
<h3 id="中心点移动过程可视化"><a href="#中心点移动过程可视化" class="headerlink" title="中心点移动过程可视化"></a><a href="#%E4%B8%AD%E5%BF%83%E7%82%B9%E7%A7%BB%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96" title="中心点移动过程可视化"></a>中心点移动过程可视化</h3><p>截止上小节，已经完成了 K-Means 聚类的流程。为了帮助大家理解，我们尝试将 K-Means 聚类过程中，中心点移动变化的过程绘制出来。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num\_axes &#x3D; len(centers\_container)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(1, num\_axes, figsize&#x3D;(20, 4))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[0\])  </span><br><span class="line">axes\[0\].scatter(init\_centers\[:, 0\], init\_centers\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[0\].set\_title(&quot;initial center&quot;)  </span><br><span class="line">  </span><br><span class="line">for i in range(1, num\_axes-1):  </span><br><span class="line">    axes\[i\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[i\])  </span><br><span class="line">    axes\[i\].scatter(centers\_container\[i\]\[:, 0\],  </span><br><span class="line">                    centers\_container\[i\]\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">    axes\[i\].set\_title(&quot;step &#123;&#125;&quot;.format(i))  </span><br><span class="line">  </span><br><span class="line">axes\[-1\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[-1\])  </span><br><span class="line">axes\[-1\].scatter(final\_center\[:, 0\], final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[-1\].set\_title(&quot;final center&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_53_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_53_1.png" alt="Text(0.5, 1.0, &#39;final center&#39;)">Text(0.5, 1.0, ‘final center’)</a></p>
<p>你会惊讶的发现，对于示例数据集，虽然我们先前将最大迭代次数 <code>max_step</code> 设为了 <code>50</code>，但实际上 K-Means 迭代 3 次即收敛。原因主要有 2 点：</p>
<ul>
<li>初始化中心点的位置很好，比较均匀分布在了数据范围中。如果初始化中心点集中分布在某一角落，迭代次数肯定会增加。</li>
<li>示例数据分布规整和简单，使得无需迭代多次就能收敛。</li>
</ul>
<h3 id="K-Means-算法聚类中的-K-值选择"><a href="#K-Means-算法聚类中的-K-值选择" class="headerlink" title="K-Means 算法聚类中的 K 值选择"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E8%81%9A%E7%B1%BB%E4%B8%AD%E7%9A%84-K-%E5%80%BC%E9%80%89%E6%8B%A9" title="K-Means 算法聚类中的 K 值选择"></a>K-Means 算法聚类中的 K 值选择</h3><p>不知道你是否还记得，前面在学习分类算法 K-近邻的时候，我们讲到了 K 值的选择。而在使用 K-Means 算法聚类时，由于要提前确定随机初始化中心点的数量，同样面临着 K 值选择问题。</p>
<p>在前面寻找 K 值时，我们通过肉眼观察认为应该聚为 3 类。那么，如果我们设定聚类为 5 类呢？</p>
<p>这一次，我们尝试通过 <code>scikit-learn</code> 模块中的 K-Means 算法完成聚类。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import k\_means  </span><br><span class="line">  </span><br><span class="line">k\_means(X, n\_clusters)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>其中参数为：</p>
<ul>
<li><code>X</code>：表示需要聚类的数据。</li>
<li><code>n_clusters</code>：表示聚类的个数，也就是 K 值。</li>
</ul>
<p>返回值包含：</p>
<ul>
<li><code>centroid</code>：表示中心点坐标。</li>
<li><code>label</code>：表示聚类后每一个样本的类别。</li>
<li><code>inertia</code>：每一个样本与最近中心点距离的平方和，即 SSE。</li>
</ul>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;用 scikit-learn 聚类并绘图  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">from sklearn.cluster import k\_means  </span><br><span class="line">model &#x3D; k\_means(blobs, n\_clusters&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">centers &#x3D; model\[0\]  </span><br><span class="line">clusters\_info &#x3D; model\[1\]  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;clusters\_info)  </span><br><span class="line">plt.scatter(centers\[:, 0\], centers\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_60_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_60_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdab66e10&gt;"></a></p>
<p>从图片上来看，聚为 5 类效果明显不如聚为 3 类的好。当然，我们提前用肉眼就能看出数据大致为 3 团。</p>
<p>实际的应用过程中，如果通过肉眼无法判断数据应该聚为几类？或者是高维数据无法可视化展示。面对这样的情况，我们就要从数值计算的角度去判断 K 值的大小。</p>
<p><strong>接下来，将介绍一种启发式学习算法，被称之为 肘部法则，可以帮助我们选取 K 值。</strong></p>
<p>使用 K-Means 算法聚类时，我们可以计算出按不同 K 值聚类后，每一个样本距离最近中心点距离的平方和 SSE。</p>
<p>随着 K 值增加时，也就是类别增加时，每个类别中的类内相似性也随之增加，由此造成的 SSE 的变化是单调减小的。可以想象一下，聚类类别的数量和样本的总数相同时，也就是说一个样本就代表一个类别时，这个数值会变成 0。</p>
<p>下面我们通过代码将不同的数量的聚类下，样本和最近中心点的距离和绘制出来。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">index &#x3D; \[\] # 横坐标数组  </span><br><span class="line">inertia &#x3D; \[\] # 纵坐标数组  </span><br><span class="line">  </span><br><span class="line">\# K 从 1~ 6 聚类  </span><br><span class="line">for i in range(6):  </span><br><span class="line">    model &#x3D; k\_means(blobs, n\_clusters&#x3D;i + 1)  </span><br><span class="line">    index.append(i + 1)  </span><br><span class="line">    inertia.append(model\[2\])  </span><br><span class="line">  </span><br><span class="line">\# 绘制折线图  </span><br><span class="line">plt.plot(index, inertia, &quot;-o&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_63_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_63_1.png" alt="&lt;matplotlib.lines.Line2D at 0x2dbdac2fdd8&gt;"></a></p>
<p>通过上图可以看到，和预想的一样，样本距离最近中心点距离的总和会随着 K 值的增大而降低。</p>
<p>现在，回想本实验划分聚类中所讲评估划分的好坏标准：「保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大」。</p>
<p>当 K 值越大时，越满足「同一划分的样本之间的差异尽可能的小」。而当 K 值越小时，越满足「不同划分中的样本差异尽可能的大畸变程度最大」。那么如何做到两端的平衡呢？</p>
<p>于是，<strong>我们通过 SSE 所绘制出来的图，将畸变程度最大的点称之为「肘部」</strong>。从图中可以看到，这里的「肘部」是 K = 3（内角最小，弯曲度最大）。这也说明，将样本聚为 3 类是最佳选择（K = 2 比较接近）。这就是所谓的「肘部法则」，你明白了吗？</p>
<h2 id="K-Means-聚类算法"><a href="#K-Means-聚类算法" class="headerlink" title="K-Means++ 聚类算法"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95" title="K-Means++ 聚类算法"></a>K-Means++ 聚类算法</h2><h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a><a href="#%E9%97%AE%E9%A2%98%E5%BC%95%E5%85%A5" title="问题引入"></a>问题引入</h3><p>随着数据量的增长，分类数目增多时，由于 K-Means 中初始化中心点是随机的，常常会出现：一个较大子集有多个中心点，而其他多个较小子集公用一个中心点的问题。即算法陷入局部最优解而不是达到全局最优解的问题。</p>
<p>造成这种问题主要原因就是：一部分中心点在初始化时离的太近。下面我们通过例子来进一步了解。</p>
<h3 id="生成示例数据-1"><a href="#生成示例数据-1" class="headerlink" title="生成示例数据"></a><a href="#%E7%94%9F%E6%88%90%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE-1" title="生成示例数据"></a>生成示例数据</h3><p>同样，我们先使用 <code>scikit-learn</code> 模块的 <code>make_blobs</code> 函数生成本次实验所需数据，本次生成 <code>800</code> 条数据，共 5 堆。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;生成数据并展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">blobs\_plus, \_ &#x3D; make\_blobs(n\_samples&#x3D;800, centers&#x3D;5, random\_state&#x3D;18)  # 生成数据  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20)  # 将数据可视化展示  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_71_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_71_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdae88278&gt;"></a></p>
<h3 id="随机初始化中心点-1"><a href="#随机初始化中心点-1" class="headerlink" title="随机初始化中心点"></a><a href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%AD%E5%BF%83%E7%82%B9-1" title="随机初始化中心点"></a>随机初始化中心点</h3><p>从数据点分布中可以很容易的观测出聚类数量应该为 5 类，我们先用 K-Means 中随机初始中心点的方法完成聚类：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">km\_init\_center&#x3D;random\_k(5, blobs\_plus)  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20);  </span><br><span class="line">plt.scatter(km\_init\_center\[:,0\], km\_init\_center\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_74_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_74_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdabfea90&gt;"></a></p>
<h3 id="K-Means-聚类"><a href="#K-Means-聚类" class="headerlink" title="K-Means 聚类"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB" title="K-Means 聚类"></a>K-Means 聚类</h3><p>用传统的 K-Means 算法，将数据集进行聚类，聚类数量为 5。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">km\_centers, km\_clusters &#x3D; kmeans\_cluster(blobs\_plus, km\_init\_center, 5)  </span><br><span class="line">km\_final\_center &#x3D; km\_centers\[-1\]  </span><br><span class="line">km\_final\_cluster &#x3D; km\_clusters\[-1\]  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;km\_final\_cluster)  </span><br><span class="line">plt.scatter(km\_final\_center\[:, 0\], km\_final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_77_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_77_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb2a92b0&gt;"></a></p>
<p>通过传统 K-Means 算法聚类后，你会发现聚类效果和我们预想不同，我们预想的结果应该是下面这样的：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;\_)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_79_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_79_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb2e6978&gt;"></a></p>
<p>对比 K-Means 聚类和预想聚类的两张图，可以直观的看出 K-Means 算法显然没有达到最优的聚类效果，出现了本章开头所提到的局部最优解的问题。</p>
<p>对于局部最优问题是可以通过 SSE 来解决的，即在同一数据集上运行多次 K-Means 算法聚类，之后选取 SSE 最小的那次作为最终的聚类结果。虽然通过 SSE 找到最优解十分困难，但通过 SSE 判断最优解是十分容易的。</p>
<p>但当遇到更大的数据集，每一次 K-Means 算法会花费大量时间时，如果使用多次运行通过 SSE 来判断最优解，显然不是好的选择。是否有一种方法在初始化中心点时，就能有效避免局部最优问题的出现呢？</p>
<p>在 K-Means 的基础上，D.Arthur 等人在 2007 年提出了 K-Means++ 算法。其中 K-Means++ 算法主要针对初始化中心点问题进行改进，这样就可以从源头上解决局部最优解的问题。</p>
<h3 id="K-Means-算法流程"><a href="#K-Means-算法流程" class="headerlink" title="K-Means++ 算法流程"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B" title="K-Means++ 算法流程"></a>K-Means++ 算法流程</h3><p>K-Means++ 相较于 K-Means 在初始化中心点上做了改进，在其他方面和 K-Means 相同。</p>
<ol>
<li>在数据集中随机选择一个样本点作为第一个初始化的聚类中心。</li>
<li>计算样本中的非中心点与最近中心点之间的距离 D(x)$D(x)$ 并保存于一个数组里，将数组中的这些距离加起来得到 Sum(D(x))$Sum(D(x))$。</li>
<li>取一个落在 Sum(D(x))$Sum(D(x))$范围中的随机值 R$R$ ，重复计算 R=R−D(x)$R=R−D(x)$ 直至得到 R≤0$R≤0$ ，选取此时的点作为下一个中心点。</li>
<li>重复 2,3 步骤，直到 K$K$ 个聚类中心都被确定。</li>
<li>对 K$K$ 个初始化的聚类中心，利用 K-Means 算法计算最终的聚类中心。</li>
</ol>
<p>看完整个算法流程，可能会出现一个疑问：为避免初始点距离太近，直接选取距离最远的点不就好了，为什么要引入一个随机值 R$R$ 呢？</p>
<p>其实当采用直接选取距离最远的点作为初始点的方法，会容易受到数据集中离群点的干扰。采用引入随机值 R$R$ 的方法避免数据集中所包含的离群点对算法思想中要选择相距最远的中心点的目标干扰。</p>
<p>相对于正常的数据点，离群点所计算得出的 D(x)$D(x)$ 距离一定比较大，这样在选取的过程中，它被选中的概率也就相对较大，但是离群点在整个数据集中只占一小部分，大部分依然是正常的点，这样离群点由于距离大而造成的概率大，就被正常点的数量大给平衡掉。从而保证了整个算法的平衡性。</p>
<h3 id="K-Means-算法实现"><a href="#K-Means-算法实现" class="headerlink" title="K-Means++ 算法实现"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="K-Means++ 算法实现"></a>K-Means++ 算法实现</h3><p>K-Means++ 在初始化样本点之后，计算其他样本与其最近的中心点距离之和，以备下一个中心点的选择，下面用 Python 来进行实现：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def get\_sum\_dis(centers, data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    centers -- 中心点集合  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    np.sum(dis\_container) -- 样本距离最近中心点的距离之和  </span><br><span class="line">    dis\_container -- 样本距离最近中心点的距离集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    dis\_container &#x3D; np.array(\[\])  </span><br><span class="line">    for each\_data in data:  </span><br><span class="line">        distances &#x3D; np.array(\[\])  </span><br><span class="line">        for each\_center in centers:  </span><br><span class="line">            temp\_distance &#x3D; d\_euc(each\_data, each\_center)  # 计算样本和中心点的欧式距离  </span><br><span class="line">            distances &#x3D; np.append(distances, temp\_distance)  </span><br><span class="line">        lab &#x3D; np.min(distances)  </span><br><span class="line">        dis\_container &#x3D; np.append(dis\_container, lab)  </span><br><span class="line">    return np.sum(dis\_container), dis\_container  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>接下来，我们初始化中心点：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;K-Means++ 初始化中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def get\_init\_center(data, k):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    np.array(center\_container) -- 初始化中心点集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    seed &#x3D; np.random.RandomState(20)  </span><br><span class="line">    p &#x3D; seed.randint(0, len(data))  </span><br><span class="line">    first\_center &#x3D; data\[p\]  </span><br><span class="line">  </span><br><span class="line">    center\_container &#x3D; \[\]  </span><br><span class="line">    center\_container.append(first\_center)  </span><br><span class="line">  </span><br><span class="line">    for i in range(k-1):  </span><br><span class="line">        sum\_dis, dis\_con &#x3D; get\_sum\_dis(center\_container, data)  </span><br><span class="line">        r &#x3D; np.random.randint(0, sum\_dis)  </span><br><span class="line">        for j in range(len(dis\_con)):  </span><br><span class="line">            r &#x3D; r - dis\_con\[j\]  </span><br><span class="line">            if r &lt;&#x3D; 0:  </span><br><span class="line">                center\_container.append(data\[j\])  </span><br><span class="line">                break  </span><br><span class="line">            else:  </span><br><span class="line">                pass  </span><br><span class="line">  </span><br><span class="line">    return np.array(center\_container)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>实现 K-Means++ 初始化中心点函数之后，根据生成数据，得到初始化的中心点坐标。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plus\_init\_center &#x3D; get\_init\_center(blobs\_plus, 5)  </span><br><span class="line">plus\_init\_center  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[ 4.1661903 ,  0.81807492],</span><br><span class="line">       [ 8.9161603 ,  5.58757202],</span><br><span class="line">       [ 7.62699601,  2.3492678 ],</span><br><span class="line">       [-3.42049424, -9.57117787],</span><br><span class="line">       [ 3.35681598, -0.54000802]]) </span><br></pre></td></tr></table></figure>

<p>为了让你更清晰的看到 K-Means++ 初始化中心点的过程，我们用 <code>matplotlib</code> 进行展示。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num &#x3D; len(plus\_init\_center)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(1, num, figsize&#x3D;(25, 4))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">axes\[0\].scatter(plus\_init\_center\[0, 0\], plus\_init\_center\[0, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[0\].set\_title(&quot;first center&quot;)  </span><br><span class="line">  </span><br><span class="line">for i in range(1, num):  </span><br><span class="line">    axes\[i\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">    axes\[i\].scatter(plus\_init\_center\[:i+1, 0\],  </span><br><span class="line">                    plus\_init\_center\[:i+1, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">    axes\[i\].set\_title(&quot;step&#123;&#125;&quot;.format(i))  </span><br><span class="line">  </span><br><span class="line">axes\[-1\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">axes\[-1\].scatter(plus\_init\_center\[:, 0\], plus\_init\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[-1\].set\_title(&quot;final center&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_96_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_96_1.png" alt="Text(0.5, 1.0, &#39;final center&#39;)">Text(0.5, 1.0, ‘final center’)</a></p>
<p>通过上图可以看到点的变化，即除了最初随机选择点之外，之后的每一个点都是尽可能选择远一些的点。这样就很好的保证初始中心点的分散。</p>
<p>通过多次执行代码可以看到，使用 K-Means++ 同样可能出现两个中心点较近的情况，因此，在极端情况也可能出现局部最优的问题。但相比于 K-Means 算法的随机选取，K-Means++ 的初始化中心点会在很大程度上降低局部最优问题出现的概率。</p>
<p>在通过 K-Means++ 算法初始化中心点后，下面我们通过 K-Means 算法对数据进行聚类。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plus\_centers, plus\_clusters &#x3D; kmeans\_cluster(blobs\_plus, plus\_init\_center, 5)  </span><br><span class="line">plus\_final\_center &#x3D; plus\_centers\[-1\]  </span><br><span class="line">plus\_final\_cluster &#x3D; plus\_clusters\[-1\]  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;plus\_final\_cluster)  </span><br><span class="line">plt.scatter(plus\_final\_center\[:, 0\], plus\_final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_100_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_100_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb3e3630&gt;"></a></p>
<p>在 K-Means++ 算法中，我们依旧无法完全避免随机选择中心点带来的不稳定性，所以偶尔也会得到不太好的结果。当然，K-Means++ 算法得到不太好的聚类的概率远小于 K-Means 算法。所以，如果你并没有得到一个较好的聚类效果，可以再次初始化中心点尝试。</p>
<h2 id="Mini-Batch-K-Means-聚类算法"><a href="#Mini-Batch-K-Means-聚类算法" class="headerlink" title="Mini-Batch K-Means 聚类算法"></a><a href="#Mini-Batch-K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95" title="Mini-Batch K-Means 聚类算法"></a>Mini-Batch K-Means 聚类算法</h2><p>在「大数据」如此火的时代，K-Means 算法是否还能一如既往优秀的处理大数据呢？现在我们重新回顾下 K-Means 的算法原理：首先，计算每一个样本同所有中心点的距离，通过比较找到最近的中心点，将距离最近中心点的距离进行存储并归类。然后通过相同类别样本的特征值，更新中心点的位置。至此完成一次迭代，经过多次迭代后最终进行聚类。</p>
<p>通过上面的表述，你是否感觉到不断计算距离的过程，涉及到的计算量有多大呢？那么，设想一下数据量达到十万，百万，千万级别，且如果每一条数据有上百个特征，这将会消耗大量的计算资源。</p>
<p>为了解决大规模数据的聚类问题，我们就可以使用 K-Means 的另外一个变种 Mini Batch K-Means 来完成。</p>
<p>其算法原理也十分简单：在每一次迭代过程中，从数据集中随机抽取一部分数据形成小批量数据集，用该部分数据集进行距离计算和中心点的更新。由于每一次都是随机抽取，所以每一次抽取的数据能很好的表现原本数据集的特性。</p>
<p>下面，我们生成一组测试数据，并测试 K-Means 算法和 Mini Batch K-Means 在同一组数据上聚类时间和 SSE 上的差异。由于 scikit-learn 中 <code>MiniBatchKMeans()</code> 和 <code>KMeans()</code> 方法的参数几乎一致，这里就不再赘述了。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import time  </span><br><span class="line">from sklearn.cluster import MiniBatchKMeans, KMeans  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">test\_data, \_ &#x3D; make\_blobs(2000, n\_features&#x3D;2, cluster\_std&#x3D;2, centers&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">km &#x3D; KMeans(n\_clusters&#x3D;5)  </span><br><span class="line">mini\_km &#x3D; MiniBatchKMeans(n\_clusters&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(nrows&#x3D;1, ncols&#x3D;2, figsize&#x3D;(10, 5))  </span><br><span class="line">  </span><br><span class="line">for i, model in enumerate(\[km, mini\_km\]):  </span><br><span class="line">    t0 &#x3D; time.time()  </span><br><span class="line">    model.fit(test\_data)  </span><br><span class="line">    t1 &#x3D; time.time()  </span><br><span class="line">    t &#x3D; t1 - t0  </span><br><span class="line">    sse &#x3D; model.inertia\_  </span><br><span class="line">    axes\[i\].scatter(test\_data\[:, 0\], test\_data\[:, 1\], c&#x3D;model.labels\_)  </span><br><span class="line">    axes\[i\].set\_xlabel(&quot;time: &#123;:.4f&#125; s&quot;.format(t))  </span><br><span class="line">    axes\[i\].set\_ylabel(&quot;SSE: &#123;:.4f&#125;&quot;.format(sse))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].set\_title(&quot;K-Means&quot;)  </span><br><span class="line">axes\[1\].set\_title(&quot;Mini Batch K-Means&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_106_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_106_1.png" alt="Text(0.5, 1.0, &#39;Mini Batch K-Means&#39;)">Text(0.5, 1.0, ‘Mini Batch K-Means’)</a></p>
<p>以上是对 2000 条数据分别用 K-Means 和 Mini Batch K-Means进行聚类，从图像中可以看出，Mini Batch K-Means 在训练时间上明显比 K-Means 快（大于 2 倍不等），且聚类得到的 SSE 值比较接近。</p>
<hr>
<p><strong>拓展阅读：</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-hans/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95">K-平均算法- 维基百科</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">The 5 Clustering Algorithms Data Scientists Need to Know</a></li>
<li><a target="_blank" rel="noopener" href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">Visualizing K-Means Clustering</a></li>
</ul>
<p># <a href="/tags/cluster/">cluster</a>, <a href="/tags/k-means/">k-means</a>, <a href="/tags/machine-learning/">machine learning</a>, <a href="/tags/python/">python</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>机器学习|划分聚类之 K-Means 详解</p><p><a href="http://www.laugh12321.cn/blog/2019/02/09/cluster_of_k-means/">http://www.laugh12321.cn/blog/2019/02/09/cluster_of_k-means/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Laugh</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-02-09</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-10-22</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/K-Means/">K-Means</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Cluster/">Cluster</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5f902f1aa809f500123872bf&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/alipay-reward-image.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/wechat-reward-image.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/02/09/image_compression_using_mini_batch_k-means/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">使用 Mini Batch K-Means 进行图像压缩</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/02/07/bagging_and_boosting/"><span class="level-item">机器学习|装袋和提升方法详解</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.css"><script src="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "483900b75d8c3c47c3603274607fcc36",
            repo: "blog",
            owner: "laugh12321",
            clientID: "f831b7db637971edc8c9",
            clientSecret: "9767a6cb27b89b67e2875e8915671499b75a6436",
            admin: ["laugh12321"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars0.githubusercontent.com/u/38065710?s=400&amp;u=80ee4d360562451484188e6f1677fa442720891b&amp;v=4" alt="Laugh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Laugh</p><p class="is-size-6 is-block">The life I want, there is no shortcut.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Northern Hemisphere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/blog/archives"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/blog/categories"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/blog/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/laugh12321" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/blog/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#K-Means-聚类方法"><span class="level-left"><span class="level-item">1</span><span class="level-item">K-Means 聚类方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#生成示例数据"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">生成示例数据</span></span></a></li><li><a class="level is-mobile" href="#数据可视化"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">数据可视化</span></span></a></li><li><a class="level is-mobile" href="#随机初始化中心点"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">随机初始化中心点</span></span></a></li><li><a class="level is-mobile" href="#计算样本与中心点的距离"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">计算样本与中心点的距离</span></span></a></li><li><a class="level is-mobile" href="#最小化-SSE，更新聚类中心"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">最小化 SSE，更新聚类中心</span></span></a></li><li><a class="level is-mobile" href="#K-Means-聚类算法实现"><span class="level-left"><span class="level-item">1.6</span><span class="level-item">K-Means 聚类算法实现</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#算法流程"><span class="level-left"><span class="level-item">1.6.1</span><span class="level-item">算法流程</span></span></a></li><li><a class="level is-mobile" href="#算法实现"><span class="level-left"><span class="level-item">1.6.2</span><span class="level-item">算法实现</span></span></a></li></ul></li><li><a class="level is-mobile" href="#中心点移动过程可视化"><span class="level-left"><span class="level-item">1.7</span><span class="level-item">中心点移动过程可视化</span></span></a></li><li><a class="level is-mobile" href="#K-Means-算法聚类中的-K-值选择"><span class="level-left"><span class="level-item">1.8</span><span class="level-item">K-Means 算法聚类中的 K 值选择</span></span></a></li></ul></li><li><a class="level is-mobile" href="#K-Means-聚类算法"><span class="level-left"><span class="level-item">2</span><span class="level-item">K-Means++ 聚类算法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#问题引入"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">问题引入</span></span></a></li><li><a class="level is-mobile" href="#生成示例数据-1"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">生成示例数据</span></span></a></li><li><a class="level is-mobile" href="#随机初始化中心点-1"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">随机初始化中心点</span></span></a></li><li><a class="level is-mobile" href="#K-Means-聚类"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">K-Means 聚类</span></span></a></li><li><a class="level is-mobile" href="#K-Means-算法流程"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">K-Means++ 算法流程</span></span></a></li><li><a class="level is-mobile" href="#K-Means-算法实现"><span class="level-left"><span class="level-item">2.6</span><span class="level-item">K-Means++ 算法实现</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Mini-Batch-K-Means-聚类算法"><span class="level-left"><span class="level-item">3</span><span class="level-item">Mini-Batch K-Means 聚类算法</span></span></a></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Algorithm/Sorting-algorithm/"><span class="level-start"><span class="level-item">Sorting algorithm</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">12</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Bagging-and-BoosTing/"><span class="level-start"><span class="level-item">Bagging and BoosTing</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Cluster/"><span class="level-start"><span class="level-item">Cluster</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Cluster/K-Means/"><span class="level-start"><span class="level-item">K-Means</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Decision-Tree/"><span class="level-start"><span class="level-item">Decision Tree</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/K-Means/"><span class="level-start"><span class="level-item">K-Means</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/K-Nearest-Neighbors/"><span class="level-start"><span class="level-item">K-Nearest Neighbors</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Linear-Regression/"><span class="level-start"><span class="level-item">Linear Regression</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Naive-Bayes/"><span class="level-start"><span class="level-item">Naive Bayes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Polynomial-Regression/"><span class="level-start"><span class="level-item">Polynomial Regression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Machine-Learning/Support-Vector-Machine/"><span class="level-start"><span class="level-item">Support Vector Machine</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/Ubuntu/"><span class="level-start"><span class="level-item">Ubuntu</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/"><span class="level-start"><span class="level-item">ECS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/ECS/Seafile/"><span class="level-start"><span class="level-item">Seafile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Bagging/"><span class="tag">Bagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Blog/"><span class="tag">Blog</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/BoosTing/"><span class="tag">BoosTing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Cluster/"><span class="tag">Cluster</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ECS/"><span class="tag">ECS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GitHub-Pages/"><span class="tag">GitHub Pages</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Jekyll/"><span class="tag">Jekyll</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Means/"><span class="tag">K-Means</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Nearest-Neighbors/"><span class="tag">K-Nearest Neighbors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAE/"><span class="tag">MAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAPE/"><span class="tag">MAPE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MSE/"><span class="tag">MSE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Next/"><span class="tag">Next</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Seafile/"><span class="tag">Seafile</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Sorting-algorithm/"><span class="tag">Sorting algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Support-Vector-Machine/"><span class="tag">Support Vector Machine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Laugh</a><p class="is-size-7"><span>&copy; 2020 Laugh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>