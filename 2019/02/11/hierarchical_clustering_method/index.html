<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>机器学习|层次聚类方法 - Laugh&#039;s blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Laugh&#039;s blog"><meta name="msapplication-TileImage" content="https://avatars2.githubusercontent.com/u/38065710?s=400&amp;u=b32cf1cc11129ac10e78dd72a9e3dba0aa0d58bd&amp;v=4"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Laugh&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="在之前的文章中，我们学习了划分聚类方法，并着重介绍了其中的 K-Means 算法。K-Means 算法可以说是用处非常广泛的聚类算法之一，它非常好用。但是，当你使用过这种算法之后，你就会发现一个比较让人「头疼」的问题，那就是我们需要手动指定 K 值，也就是聚类的类别数量。"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习|层次聚类方法"><meta property="og:url" content="http://www.laugh12321.cn/blog/2019/02/11/hierarchical_clustering_method/"><meta property="og:site_name" content="Laugh&#039;s blog"><meta property="og:description" content="在之前的文章中，我们学习了划分聚类方法，并着重介绍了其中的 K-Means 算法。K-Means 算法可以说是用处非常广泛的聚类算法之一，它非常好用。但是，当你使用过这种算法之后，你就会发现一个比较让人「头疼」的问题，那就是我们需要手动指定 K 值，也就是聚类的类别数量。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/12.jpg"><meta property="article:published_time" content="2019-02-11T00:00:00.000Z"><meta property="article:modified_time" content="2020-10-23T08:50:34.684Z"><meta property="article:author" content="Laugh"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Cluster"><meta property="article:tag" content="Hierarchical Clustering"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/12.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.laugh12321.cn/blog"},"headline":"Laugh's blog","image":[],"datePublished":"2019-02-11T00:00:00.000Z","dateModified":"2020-10-23T08:50:34.684Z","author":{"@type":"Person","name":"laugh12321"},"description":"在之前的文章中，我们学习了划分聚类方法，并着重介绍了其中的 K-Means 算法。K-Means 算法可以说是用处非常广泛的聚类算法之一，它非常好用。但是，当你使用过这种算法之后，你就会发现一个比较让人「头疼」的问题，那就是我们需要手动指定 K 值，也就是聚类的类别数量。"}</script><link rel="canonical" href="http://www.laugh12321.cn/blog/2019/02/11/hierarchical_clustering_method/"><link rel="icon" href="https://avatars2.githubusercontent.com/u/38065710?s=400&amp;u=b32cf1cc11129ac10e78dd72a9e3dba0aa0d58bd&amp;v=4"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-180999768-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-180999768-1');</script><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><link rel="alternate" href="/blog/atom.xml" title="Laugh's blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Laugh</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">首页</a><a class="navbar-item" href="/blog/archives">归档</a><a class="navbar-item" href="/blog/categories">分类</a><a class="navbar-item" href="/blog/tags">标签</a><a class="navbar-item" href="/blog/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/laugh12321"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/photo/12.jpg" alt="机器学习|层次聚类方法"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-02-11T00:00:00.000Z" title="2019-02-11T00:00:00.000Z">2019-02-11</time>发表</span><span class="level-item"><time dateTime="2020-10-23T08:50:34.684Z" title="2020-10-23T08:50:34.684Z">2020-10-23</time>更新</span><span class="level-item"><a class="link-muted" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> / </span><a class="link-muted" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/">层次聚类</a></span><span class="level-item">1 小时读完 (大约8661个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习|层次聚类方法</h1><div class="content"><p>在之前的文章中，我们学习了划分聚类方法，并着重介绍了其中的 K-Means 算法。K-Means 算法可以说是用处非常广泛的聚类算法之一，它非常好用。但是，当你使用过这种算法之后，你就会发现一个比较让人「头疼」的问题，那就是我们需要手动指定 K 值，也就是聚类的类别数量。</p>
<a id="more"></a>
<p>预先确定聚类的类别数量看起来是个小事情，但是在很多时候是比较麻烦的，因为我们可能在聚类前并不知道数据集到底要被聚成几类。例如，下面的示意图中，感觉聚成 2 类或者 4 类都是比较合理的。  </p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/1.png"></a></p>
<p>今天要学习的层次聚类方法与划分聚类最大的区别之一，就是我们<strong>无需提前指定需要聚类类别的数量</strong>。这听起来是非常诱人的，到底是怎样做到的呢？</p>
<p>简单来讲，层次聚类方法的特点在于通过计算数据集元素间的相似度来生成一颗具备层次结构的树。首先在这里强调一点，这里的「层次树」和之前学习的「决策树」是完全不同，不要混淆。</p>
<p>与此同时，当我们使用层次聚类方法时，可以根据创建树的方式分为两种情况：</p>
<ul>
<li><p><strong>自底向上层次聚类法</strong>：该方法的过程被称为「凝聚」(Agglomerative)，也就是把数据集中的每个元素看作是一个类别，然后进行迭代合并成为更大的类别，直到满足某个终止条件。</p>
</li>
<li><p><strong>自顶向下层次聚类法</strong>：该方法的过程被称为「分裂」(Divisive)，也就是凝聚的反向过程。首先，把数据集看作是一个类别，然后递归地划分为多个较小的子类，直到满足某个终止条件。</p>
</li>
</ul>
<h2 id="自底向上层次聚类法"><a href="#自底向上层次聚类法" class="headerlink" title="自底向上层次聚类法"></a><a href="#%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E6%B3%95" title="自底向上层次聚类法"></a>自底向上层次聚类法</h2><p>自底向上层次聚类法也就是 Agglomerative Clustering 算法。这种算法的主要特点在于，我们使用「自底向上」进行聚类的思路来帮助距离相近的样本被放在同一类别中。</p>
<h3 id="自底向上层次聚类流程"><a href="#自底向上层次聚类流程" class="headerlink" title="自底向上层次聚类流程"></a><a href="#%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E6%B5%81%E7%A8%8B" title="自底向上层次聚类流程"></a>自底向上层次聚类流程</h3><p>具体来讲，这种方法的主要步骤如下：</p>
<p>对于数据集 D$D$，D=(x1,x2,⋯,xn)$D=(x1,x2,⋯,xn)$：</p>
<ol>
<li>将数据集中每个样本标记为 <code>1</code> 类，即 $D$ 初始时包含的类别（Class）为 C$C$，C=(c1,c2,⋯,cn)$C=(c1,c2,⋯,cn)$。</li>
<li>计算并找出 C$C$ 中距离最近的 <code>2</code> 个类别，合并为 <code>1</code> 类。</li>
<li>依次合并直到最后仅剩下一个列表，即建立起一颗完整的层次树。</li>
</ol>
<p>我们通过下图来演示自底向上层次聚类法的过程，首先平面上有 5 个样本点，我们将每个样本点都单独划为 1 类。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/2.png"></a></p>
<p>接下来，我们可以计算元素间的距离，并将距离最近的合并为 1 类。于是，总类别变为 3 类。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/3.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/3.png"></a></p>
<p>重复上面的步骤，总类别变为 2 类。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/4.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/4.png"></a></p>
<p>最后，合并为 1 类，聚类终止。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/5.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/5.png"></a></p>
<p>我们将上面的聚类过程变为层次树就为：</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/6.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/6.png"></a></p>
<p>看完上面的演示过程，你会发现「自底向上层次聚类法」原来这么简单呀！</p>
<h3 id="距离计算方法"><a href="#距离计算方法" class="headerlink" title="距离计算方法"></a><a href="#%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95" title="距离计算方法"></a>距离计算方法</h3><p>虽然聚类过程看似简单，但不知道你是否意识到一个问题：<strong>当一个类别中包含多个元素时，类别与类别之间的距离是怎样确定的呢？</strong></p>
<p>也就是说，上面的演示过程中，为什么要把 5$5$ 归为 &lt;3,4&gt;$&lt;3,4&gt;$ 组成的那一类，而不是 &lt;1,2&gt;$&lt;1,2&gt;$ 组成的类呢？或者说，为什么不先把 &lt;1,2&gt;$&lt;1,2&gt;$ 与 &lt;3,4&gt;$&lt;3,4&gt;$ 合并，最后才合并 5$5$ 呢？</p>
<p>这就涉及到 Agglomerative 聚类过程中的距离计算方式。简单来讲，我们一般有 3 种不同的距离计算方式：</p>
<h4 id="单连接（Single-linkage）"><a href="#单连接（Single-linkage）" class="headerlink" title="单连接（Single-linkage）"></a><a href="#%E5%8D%95%E8%BF%9E%E6%8E%A5%EF%BC%88Single-linkage%EF%BC%89" title="单连接（Single-linkage）"></a>单连接（Single-linkage）</h4><p>单连接的计算方式是根据两种类别之间<strong>最近</strong>的元素间距离作为两类别之间的距离。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/7.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/7.png"></a></p>
<h4 id="全连接（Complete-linkage）"><a href="#全连接（Complete-linkage）" class="headerlink" title="全连接（Complete-linkage）"></a><a href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%EF%BC%88Complete-linkage%EF%BC%89" title="全连接（Complete-linkage）"></a>全连接（Complete-linkage）</h4><p>全连接的计算方式是根据两种类别之间<strong>最远</strong>的元素间距离作为两类别之间的距离。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/8.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/8.png"></a></p>
<h4 id="平均连接（Average-linkage）"><a href="#平均连接（Average-linkage）" class="headerlink" title="平均连接（Average-linkage）"></a><a href="#%E5%B9%B3%E5%9D%87%E8%BF%9E%E6%8E%A5%EF%BC%88Average-linkage%EF%BC%89" title="平均连接（Average-linkage）"></a>平均连接（Average-linkage）</h4><p>平均连接的计算方式是依次计算两种类别之间两两元素间距离，并最终求得平均值作为两类别之间的距离。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/9.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/9.png"></a></p>
<h4 id="中心连接（Center-linkage）"><a href="#中心连接（Center-linkage）" class="headerlink" title="中心连接（Center-linkage）"></a><a href="#%E4%B8%AD%E5%BF%83%E8%BF%9E%E6%8E%A5%EF%BC%88Center-linkage%EF%BC%89" title="中心连接（Center-linkage）"></a>中心连接（Center-linkage）</h4><p>平均连接虽然看起来更加合理，但是两两元素间的距离计算量往往非常庞大。有时候，也可以使用中心连接计算方法。即先计算类别中心，再以中心连线作为两类别之间的距离。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/10.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/10.png"></a></p>
<p>总之，上面 4 种距离计算方法中，<strong>一般常用「平均连接」和「中心连接」</strong>方法，因为「单连接」和「全连接」都相对极端，容易受到噪声点和分布不均匀数据造成的干扰。</p>
<h3 id="Agglomerative-聚类-Python-实现"><a href="#Agglomerative-聚类-Python-实现" class="headerlink" title="Agglomerative 聚类 Python 实现"></a><a href="#Agglomerative-%E8%81%9A%E7%B1%BB-Python-%E5%AE%9E%E7%8E%B0" title="Agglomerative 聚类 Python 实现"></a>Agglomerative 聚类 Python 实现</h3><p>下面，我们尝试通过 Python 实现自底向上层次聚类算法。首先导入实验必要的模块：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line">from sklearn import datasets  </span><br><span class="line">from matplotlib import pyplot as plt  </span><br><span class="line">%matplotlib inline  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>然后，通过 <code>make_blobs</code> 方法随机生成一组示例数据，且使得示例数据呈现出 2 类数据的趋势。这里，我们设定随机数种子 <code>random_state=10</code> 以保证你的结果和实验结果一致。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; datasets.make\_blobs(10, n\_features&#x3D;2, centers&#x3D;2, random\_state&#x3D;10)  </span><br><span class="line">data  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(array([[  6.04774884, -10.30504657],</span><br><span class="line">        [  2.90159483,   5.42121526],</span><br><span class="line">        [  4.1575017 ,   3.89627276],</span><br><span class="line">        [  1.53636249,   5.11121453],</span><br><span class="line">        [  3.88101257,  -9.59334486],</span><br><span class="line">        [  1.70789903,   6.00435173],</span><br><span class="line">        [  5.69192445,  -9.47641249],</span><br><span class="line">        [  5.4307043 ,  -9.75956122],</span><br><span class="line">        [  5.85943906,  -8.38192364],</span><br><span class="line">        [  0.69523642,   3.23270535]]), array([0, 1, 1, 1, 0, 1, 0, 0, 0, 1])) </span><br></pre></td></tr></table></figure>

<p>使用 Matplotlib 绘制示例数据结果，可以看到数据的确呈现出 <code>2</code> 种类别的趋势。其中 <code>data[1]</code> 的结果即为生成数据时预设的类别，当然接下来的聚类过程，我们是不知道数据的预设类别。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(data\[0\]\[:,0\], data\[0\]\[:,1\], c&#x3D;data\[1\], s&#x3D;60)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_46_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_46_1.png"></a></p>
<p>首先，我们实现欧式距离的计算函数：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;欧式距离  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def euclidean\_distance(a, b):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    a -- 数组 a  </span><br><span class="line">    b -- 数组 b  </span><br><span class="line">      </span><br><span class="line">    返回:  </span><br><span class="line">    dist -- a, b 间欧式距离  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    x &#x3D; float(a\[0\]) - float(b\[0\])  </span><br><span class="line">    x &#x3D; x \* x  </span><br><span class="line">    y &#x3D; float(a\[1\]) - float(b\[1\])  </span><br><span class="line">    y &#x3D; y \* y  </span><br><span class="line">    dist &#x3D; round(np.sqrt(x + y), 2)  </span><br><span class="line">    return dist  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>然后，实现 Agglomerative 聚类函数，这里使用中心连接的方法。为了更加具体的展示层次聚类的过程，这里在函数中添加一些多余的 <code>print()</code> 函数。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;Agglomerative 聚类计算过程  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def agglomerative\_clustering(data):  </span><br><span class="line">      </span><br><span class="line">    while len(data) &gt; 1:  </span><br><span class="line">        print(&quot;☞ 第 &#123;&#125; 次迭代\\n&quot;.format(10 - len(data) + 1))  </span><br><span class="line">        min\_distance &#x3D; float(&#39;inf&#39;) # 设定初始距离为无穷大  </span><br><span class="line">        for i in range(len(data)):  </span><br><span class="line">            print(&quot;---&quot;)  </span><br><span class="line">            for j in range(i + 1, len(data)):  </span><br><span class="line">                distance &#x3D; euclidean\_distance(data\[i\], data\[j\])  </span><br><span class="line">                print(&quot;计算 &#123;&#125; 与 &#123;&#125; 距离为 &#123;&#125;&quot;.format(data\[i\], data\[j\],distance))  </span><br><span class="line">                if distance &lt; min\_distance:  </span><br><span class="line">                    min\_distance &#x3D; distance  </span><br><span class="line">                    min\_ij &#x3D; (i, j)  </span><br><span class="line">        i, j &#x3D; min\_ij # 最近数据点序号  </span><br><span class="line">        data1 &#x3D; data\[i\]  </span><br><span class="line">        data2 &#x3D; data\[j\]  </span><br><span class="line">        data &#x3D; np.delete(data, j, 0) # 删除原数据  </span><br><span class="line">        data &#x3D; np.delete(data, i, 0) # 删除原数据  </span><br><span class="line">        b &#x3D; np.atleast\_2d(\[(data1\[0\] + data2\[0\]) &#x2F; 2, (data1\[1\] + data2\[1\]) &#x2F; 2\]) # 计算两点新中心  </span><br><span class="line">        data &#x3D; np.concatenate((data, b), axis&#x3D;0) # 将新数据点添加到迭代过程  </span><br><span class="line">        print(&quot;\\n最近距离:&#123;&#125; &amp; &#123;&#125; &#x3D; &#123;&#125;, 合并后中心:&#123;&#125;\\n&quot;.format(data1, data2, min\_distance, b))  </span><br><span class="line">          </span><br><span class="line">    return data  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |<br>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">agglomerative\_clustering(data\[0\])  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br></pre></td><td class="code"><pre><span class="line">☞ 第 1 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [2.90159483 5.42121526] 距离为 16.04</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [4.1575017  3.89627276] 距离为 14.33</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [1.53636249 5.11121453] 距离为 16.06</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 3.88101257 -9.59334486] 距离为 2.28</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [1.70789903 6.00435173] 距离为 16.88</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.69192445 -9.47641249] 距离为 0.9</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.4307043  -9.75956122] 距离为 0.82</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.85943906 -8.38192364] 距离为 1.93</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [0.69523642 3.23270535] 距离为 14.56</span><br><span class="line">---</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [4.1575017  3.89627276] 距离为 1.98</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.69192445 -9.47641249] 距离为 15.16</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.4307043  -9.75956122] 距离为 15.39</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11</span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.69192445 -9.47641249] 距离为 13.46</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.4307043  -9.75956122] 距离为 13.72</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">---</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.69192445 -9.47641249] 距离为 15.17</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.4307043  -9.75956122] 距离为 15.37</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.69192445 -9.47641249] 距离为 1.81</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.4307043  -9.75956122] 距离为 1.56</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">---</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.69192445 -9.47641249] 距离为 15.99</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.4307043  -9.75956122] 距离为 16.2</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95</span><br><span class="line">---</span><br><span class="line">计算 [ 5.69192445 -9.47641249] 与 [ 5.4307043  -9.75956122] 距离为 0.39</span><br><span class="line">计算 [ 5.69192445 -9.47641249] 与 [ 5.85943906 -8.38192364] 距离为 1.11</span><br><span class="line">计算 [ 5.69192445 -9.47641249] 与 [0.69523642 3.23270535] 距离为 13.66</span><br><span class="line">---</span><br><span class="line">计算 [ 5.4307043  -9.75956122] 与 [ 5.85943906 -8.38192364] 距离为 1.44</span><br><span class="line">计算 [ 5.4307043  -9.75956122] 与 [0.69523642 3.23270535] 距离为 13.83</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[ 5.69192445 -9.47641249] &amp; [ 5.4307043  -9.75956122] &#x3D; 0.39, 合并后中心:[[ 5.56131437 -9.61798686]]</span><br><span class="line"></span><br><span class="line">☞ 第 2 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [2.90159483 5.42121526] 距离为 16.04</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [4.1575017  3.89627276] 距离为 14.33</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [1.53636249 5.11121453] 距离为 16.06</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 3.88101257 -9.59334486] 距离为 2.28</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [1.70789903 6.00435173] 距离为 16.88</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.85943906 -8.38192364] 距离为 1.93</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [0.69523642 3.23270535] 距离为 14.56</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.56131437 -9.61798686] 距离为 0.84</span><br><span class="line">---</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [4.1575017  3.89627276] 距离为 1.98</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.56131437 -9.61798686] 距离为 15.27</span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.56131437 -9.61798686] 距离为 13.59</span><br><span class="line">---</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.56131437 -9.61798686] 距离为 15.27</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.56131437 -9.61798686] 距离为 1.68</span><br><span class="line">---</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.56131437 -9.61798686] 距离为 16.09</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [ 5.56131437 -9.61798686] 距离为 1.27</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.56131437 -9.61798686] 距离为 13.74</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[  6.04774884 -10.30504657] &amp; [ 5.56131437 -9.61798686] &#x3D; 0.84, 合并后中心:[[ 5.80453161 -9.96151671]]</span><br><span class="line"></span><br><span class="line">☞ 第 3 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [4.1575017  3.89627276] 距离为 1.98</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.80453161 -9.96151671] 距离为 15.65</span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96</span><br><span class="line">---</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.80453161 -9.96151671] 距离为 15.67</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96</span><br><span class="line">---</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.80453161 -9.96151671] 距离为 16.48</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[1.53636249 5.11121453] &amp; [1.70789903 6.00435173] &#x3D; 0.91, 合并后中心:[[1.62213076 5.55778313]]</span><br><span class="line"></span><br><span class="line">☞ 第 4 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [4.1575017  3.89627276] 距离为 1.98</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.80453161 -9.96151671] 距离为 15.65</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.62213076 5.55778313] 距离为 1.29</span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.62213076 5.55778313] 距离为 3.03</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [1.62213076 5.55778313] 距离为 15.32</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [1.62213076 5.55778313] 距离为 14.57</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [1.62213076 5.55778313] 距离为 2.5</span><br><span class="line">---</span><br><span class="line">计算 [ 5.80453161 -9.96151671] 与 [1.62213076 5.55778313] 距离为 16.07</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[2.90159483 5.42121526] &amp; [1.62213076 5.55778313] &#x3D; 1.29, 合并后中心:[[2.26186279 5.4894992 ]]</span><br><span class="line"></span><br><span class="line">☞ 第 5 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [2.26186279 5.4894992 ] 距离为 15.17</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [2.26186279 5.4894992 ] 距离为 14.33</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75</span><br><span class="line">---</span><br><span class="line">计算 [ 5.80453161 -9.96151671] 与 [2.26186279 5.4894992 ] 距离为 15.85</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[ 5.85943906 -8.38192364] &amp; [ 5.80453161 -9.96151671] &#x3D; 1.58, 合并后中心:[[ 5.83198533 -9.17172018]]</span><br><span class="line"></span><br><span class="line">☞ 第 6 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.83198533 -9.17172018] 距离为 13.17</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [2.26186279 5.4894992 ] 距离为 15.17</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.83198533 -9.17172018] 距离为 2.0</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.83198533 -9.17172018] 距离为 13.43</span><br><span class="line">---</span><br><span class="line">计算 [2.26186279 5.4894992 ] 与 [ 5.83198533 -9.17172018] 距离为 15.09</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[ 3.88101257 -9.59334486] &amp; [ 5.83198533 -9.17172018] &#x3D; 2.0, 合并后中心:[[ 4.85649895 -9.38253252]]</span><br><span class="line"></span><br><span class="line">☞ 第 7 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 4.85649895 -9.38253252] 距离为 13.3</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 4.85649895 -9.38253252] 距离为 13.28</span><br><span class="line">---</span><br><span class="line">计算 [2.26186279 5.4894992 ] 与 [ 4.85649895 -9.38253252] 距离为 15.1</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[4.1575017  3.89627276] &amp; [2.26186279 5.4894992 ] &#x3D; 2.48, 合并后中心:[[3.20968225 4.69288598]]</span><br><span class="line"></span><br><span class="line">☞ 第 8 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 4.85649895 -9.38253252] 距离为 13.28</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [3.20968225 4.69288598] 距离为 2.91</span><br><span class="line">---</span><br><span class="line">计算 [ 4.85649895 -9.38253252] 与 [3.20968225 4.69288598] 距离为 14.17</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[0.69523642 3.23270535] &amp; [3.20968225 4.69288598] &#x3D; 2.91, 合并后中心:[[1.95245933 3.96279567]]</span><br><span class="line"></span><br><span class="line">☞ 第 9 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [ 4.85649895 -9.38253252] 与 [1.95245933 3.96279567] 距离为 13.66</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[ 4.85649895 -9.38253252] &amp; [1.95245933 3.96279567] &#x3D; 13.66, 合并后中心:[[ 3.40447914 -2.70986843]]</span><br><span class="line"></span><br><span class="line">array([[ 3.40447914, -2.70986843]]) </span><br></pre></td></tr></table></figure>

<p>通过上面的计算过程，你应该能很清晰地看出 Agglomerative 聚类的完整过程了。我们将 <code>data</code> 数组的每行依次按 <code>0-9</code> 编号，并将计算过程绘制成层次聚类的二叉树结构如下：</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/11.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/11.png"></a></p>
<p>建立好树形结构后，如果我们想取类别为 <code>2</code>，就从顶部画一条横线就可以了。然后，沿着网络延伸到叶节点就能找到各自对应的类别。如下图所示：</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/12.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/12.png"></a></p>
<p>如果你对着 <code>data</code> 预设的类别，你会发现最终的聚类结果和 <code>data[1] = [0, 1, 1, 1, 0, 1, 0, 0, 0, 1]</code>完全一致。</p>
<p>至此，我们就完整实现了自底向上层次聚类法。</p>
<h3 id="使用-scikit-learn-完成-Agglomerative-聚类"><a href="#使用-scikit-learn-完成-Agglomerative-聚类" class="headerlink" title="使用 scikit-learn 完成 Agglomerative 聚类"></a><a href="#%E4%BD%BF%E7%94%A8-scikit-learn-%E5%AE%8C%E6%88%90-Agglomerative-%E8%81%9A%E7%B1%BB" title="使用 scikit-learn 完成 Agglomerative 聚类"></a>使用 scikit-learn 完成 Agglomerative 聚类</h3><p>scikit-learn 中也提供了 Agglomerative 聚类的类，相应的参数解释如下：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cluster.AgglomerativeClustering(n\_clusters&#x3D;2, affinity&#x3D;&#39;euclidean&#39;, memory&#x3D;None, connectivity&#x3D;None, compute\_full\_tree&#x3D;&#39;auto&#39;, linkage&#x3D;&#39;ward&#39;, pooling\_func&#x3D;&lt;function mean&gt;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>其中：</p>
<ul>
<li><code>n_clusters</code>: 表示最终要查找类别的数量，例如上面的 2 类。</li>
<li><code>affinity</code>: 亲和力度量，有 <code>euclidean</code>（欧式距离）, <code>l1</code>（L1 范数）, <code>l2</code>（L2 范数）, <code>manhattan</code>（曼哈顿距离）等可选。</li>
<li><code>linkage</code>: 连接方法：<code>ward</code>（单连接）, <code>complete</code>（全连接）, <code>average</code>（平均连接）可选。</li>
</ul>
<p>实验同样使用上面的数据集完成模型构建并聚类：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import AgglomerativeClustering  </span><br><span class="line">  </span><br><span class="line">model &#x3D; AgglomerativeClustering(n\_clusters&#x3D;2, affinity&#x3D;&#39;euclidean&#39;, linkage&#x3D;&#39;average&#39;)  </span><br><span class="line">model.fit\_predict(data\[0\])  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 0, 0, 0, 1, 0, 1, 1, 1, 0]) </span><br></pre></td></tr></table></figure>

<p>可以看到，最终的聚类结构和我们上面是一致的。（表示类别的 <code>1, 0</code> 相反没有影响）</p>
<h2 id="自顶向下层次聚类法"><a href="#自顶向下层次聚类法" class="headerlink" title="自顶向下层次聚类法"></a><a href="#%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E6%B3%95" title="自顶向下层次聚类法"></a>自顶向下层次聚类法</h2><p>除了上面讲到的自底向上的层次聚类法，还有一类是自顶向下层次聚类法，这种方法的计算流程与前一种正好相反，但过程要复杂很多。</p>
<p>大致说来，我们首先将全部数据归为一类，然后逐步分割成小类，而这里的分割方法又有常见的 2 种形式：</p>
<h3 id="利用-K-Means-算法进行分割"><a href="#利用-K-Means-算法进行分割" class="headerlink" title="利用 K-Means 算法进行分割"></a><a href="#%E5%88%A9%E7%94%A8-K-Means-%E7%AE%97%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%88%86%E5%89%B2" title="利用 K-Means 算法进行分割"></a>利用 K-Means 算法进行分割</h3><p>首先，我们说一说利用 K-Means 算法分割方法：</p>
<ol>
<li>把数据集 D$D$ 归为单个类别 C$C$ 作为顶层。</li>
<li>使用 K-Means 算法把 C$C$ 划分成 2 个子类别，构成子层；</li>
<li>可递归使用 K-Means 算法继续划分子层到终止条件。</li>
</ol>
<p>同样，我们可以通过示意图来演示该聚类算法的流程。</p>
<p>首先，全部数据在一个类别中：</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/13.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/13.png"></a></p>
<p>然后，通过 K-Means 算法把其聚成 <code>2</code> 类。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/14.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/14.png"></a></p>
<p>紧接着，将子类再分别使用 K-Means 算法聚成 <code>2</code> 类。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/15.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/15.png"></a></p>
<p>最终，直到所以的数据都各自为 1 个类别，即分割完成。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/16.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/16.png"></a></p>
<p>利用 K-Means 算法进行自顶向下层次聚类过程同样属于看起来简单，但计算量庞大的过程。</p>
<h3 id="利用平均距离进行分割"><a href="#利用平均距离进行分割" class="headerlink" title="利用平均距离进行分割"></a><a href="#%E5%88%A9%E7%94%A8%E5%B9%B3%E5%9D%87%E8%B7%9D%E7%A6%BB%E8%BF%9B%E8%A1%8C%E5%88%86%E5%89%B2" title="利用平均距离进行分割"></a>利用平均距离进行分割</h3><ol>
<li>把数据集 D$D$ 归为单个类别 C$C$ 作为顶层。</li>
<li>从类别 C$C$ 中取出点 d$d$，使得 d$d$ 满足到 C$C$ 中其他点的平均距离最远，构成类别 N$N$。</li>
<li>继续从类别 C$C$ 中取出点 d’$d’$, 使得 d’$d’$ 满足到 C$C$ 中其他点的平均距离与到 N$N$ 中点的平均距离之间的差值最大，并将点放入 N$N$。</li>
<li>重复步骤 <code>3</code>，直到差值为负数。</li>
<li>再从子类中重复步骤 <code>2</code>,<code>3</code>,<code>4</code> 直到全部点单独成类，即完成分割。</li>
</ol>
<p>同样，我们可以通过示意图来演示该聚类算法的流程。</p>
<p>首先，全部数据在一个类别中：</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/17.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/17.png"></a></p>
<p>然后，我们依次抽取 1 个数据点，并计算它与其他点的平均距离，且最终取平均距离最大的点单独成类。例如这里计算出结果为 5。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/18.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/18.png"></a></p>
<p>同样，从剩下的 4 个点再取出一个点，使该点到剩下点的平均距离与该点到点 5 的距离差值最大且不为负数。这里没有点满足条件，终止。</p>
<p>接下来，从剩下的 4 个点中再取出一个点，并计算它与其他 3 点的距离，取最大单独成类。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/19.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/19.png"></a></p>
<p>同样，从剩下的 3 个点中再取出一个点，使该点到剩下点的平均距离与该点到点 4 的距离差值最大且不为负数，合并为 1 类。点 3 明显满足：</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/20.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/20.png"></a></p>
<p>重复步骤，继续计算形成子层。然后对子层中包含有对应元素的类重复上面的步骤聚类，直到全部点单独成类，即完成分割。</p>
<p>自顶向下层次聚类法在实施过程中常常遇到一个问题，那就是如果两个样本在上一步聚类中被划分成不同的类别，那么即使这两个点距离非常近，后面也不会被放到一类中。</p>
<p>所以在实际应用中，自顶向下层次聚类法没有自底而上的层次聚类法常用，这里也就不再进行实现了，了解其运行原理即可。</p>
<h2 id="BIRCH-聚类算法"><a href="#BIRCH-聚类算法" class="headerlink" title="BIRCH 聚类算法"></a><a href="#BIRCH-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95" title="BIRCH 聚类算法"></a>BIRCH 聚类算法</h2><p>除了上文提到的两种层次聚类方法，还有一种非常常用且高效的层次聚类法，叫做 BIRCH。</p>
<p>BIRCH 的全称为 Balanced Iterative Reducing and Clustering using Hierarchies，直译过来就是「使用层次方法的平衡迭代规约和聚类」。该算法由时任 IBM 工程师 Tian Zhang 于 1996 年发明，详见 <a target="_blank" rel="noopener" href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">论文</a>。</p>
<p>BIRCH 最大的特点就是高效，可用于大型数据集的快速聚类。</p>
<h3 id="CF-CF-聚类特征"><a href="#CF-CF-聚类特征" class="headerlink" title="CF$CF$ 聚类特征"></a><a href="#CF-%E8%81%9A%E7%B1%BB%E7%89%B9%E5%BE%81" title="`CF`  聚类特征"></a>CF$CF$ 聚类特征</h3><p>BIRCH 的聚类过程主要是涉及到 CF$CF$ 聚类特征和 CF$CF$ Tree 聚类特征树的概念。所以，我们需要先了解什么是聚类特征。</p>
<p>一组样本的 CF$CF$ 聚类特征定义为如下所示的三元组：</p>
<p>CF=⟨(N,LS,SS)⟩</p>
<p>$$CF=⟨(N,LS,SS)⟩$$</p>
<p>其中，N$N$ 表示该 CF$CF$ 中拥有的样本点的数量； LS$LS$ 表示该 CF$CF$ 中拥有的样本点各特征维度的和向量；SS$SS$ 表示该 CF$CF$ 中拥有的样本点各特征维度的平方和。</p>
<p>例如，我们有 5 个样本，分别为：(1,3),(2,5),(1,3),(7,9),(8,8)$(1,3),(2,5),(1,3),(7,9),(8,8)$，那么：</p>
<ul>
<li>N=5$N=5$</li>
<li>LS=(1+2+1+7+8,3+5+3+9+8)=(19,28)$LS=(1+2+1+7+8,3+5+3+9+8)=(19,28)$</li>
<li>SS=(12+22+12+72+82+32+52+32+92+82)=(307)$SS=(12+22+12+72+82+32+52+32+92+82)=(307)$</li>
</ul>
<p>于是，对应的 CF$CF$ 值就为：</p>
<p>CF=⟨5,(19,28),(307)⟩</p>
<p>$$CF=⟨5,(19,28),(307)⟩$$</p>
<p>CF$CF$ 拥有可加性，例如当 CF′=⟨3,(35,36),857⟩$CF′=⟨3,(35,36),857⟩$ 时：</p>
<p>CF′+CF=⟨5,(19,28),(307)⟩+⟨3,(12,26),87⟩=⟨8,(31,54),(394)⟩</p>
<p>$$CF′+CF=⟨5,(19,28),(307)⟩+⟨3,(12,26),87⟩=⟨8,(31,54),(394)⟩$$</p>
<p>CF$CF$ 聚类特征本质上是定义类别（簇）的信息，并有效地对数据进行压缩。</p>
<h3 id="CF-CF-聚类特征树"><a href="#CF-CF-聚类特征树" class="headerlink" title="CF$CF$ 聚类特征树"></a><a href="#CF-%E8%81%9A%E7%B1%BB%E7%89%B9%E5%BE%81%E6%A0%91" title="`CF`  聚类特征树"></a>CF$CF$ 聚类特征树</h3><p>接下来，我们介绍第二个概念 CF$CF$ 聚类特征树。</p>
<p>CF$CF$ 树由根节点（root node）、枝节点（branch node）和叶节点（leaf node）构成。另包含有三个参数，分别为：枝平衡因子 β$β$、叶平衡因子 λ$λ$ 和空间阈值 τ$τ$。而非叶节点（nonleaf node）中包含不多于 β$β$ 个 [CF,childi]$[CF,childi]$ 的元项。</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/21.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/21.png"></a></p>
<p>BIRCH 算法的核心就是基于训练样本建立了 CF$CF$ 聚类特征树。CF$CF$ 聚类特征树对应的输出就是若干个 CF$CF$ 节点，每个节点里的样本点就是一个聚类的类别。</p>
<p>其实，关于 CF$CF$ 聚类特征树的特点以及树的生成过程还有很多内容可以深入学习，不过这里面涉及到大量的数学理论和推导过程，不太好理解，这里就不再展开了。有兴趣的同学可以阅读 <a target="_blank" rel="noopener" href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">原论文</a>。</p>
<p>最后，我们简单说一下 BIRCH 算法相比 Agglomerative 算法的优势，也就是总结学习 BIRCH 算法的必要性：</p>
<ol>
<li>BIRCH 算法在建立 CF 特征树时只存储原始数据的特征信息，并不需要存储原始数据信息，内存开销上更优，计算高效。</li>
<li>BIRCH 算法只需要遍历一遍原始数据，而 Agglomerative 算法在每次迭代都需要遍历一遍数据，再次突出 BIRCH 的高效性。</li>
<li>BIRCH 属于在线学习算法，并支持对流数据的聚类，开始聚类时并不需要知道所有的数据。</li>
</ol>
<h3 id="BIRCH-聚类实现"><a href="#BIRCH-聚类实现" class="headerlink" title="BIRCH 聚类实现"></a><a href="#BIRCH-%E8%81%9A%E7%B1%BB%E5%AE%9E%E7%8E%B0" title="BIRCH 聚类实现"></a>BIRCH 聚类实现</h3><p>上面说了这么多，总结就是 BIRCH 属于层次聚类算法中非常高效的那一种方法。下面，就来看一看如何调用 scikit-learn 提供的 BIRCH 类完成聚类任务。</p>
<p>我们先导入 DIGITS 数据集，并查看前 5 个手写字符。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line">from sklearn import datasets  </span><br><span class="line">  </span><br><span class="line">digits &#x3D; datasets.load\_digits()  </span><br><span class="line">  </span><br><span class="line">\# 查看前 5 个字符  </span><br><span class="line">fig, axes &#x3D; plt.subplots(1, 5, figsize&#x3D;(12,4))  </span><br><span class="line">for i, image in enumerate(digits.images\[:5\]):  </span><br><span class="line">    axes\[i\].imshow(image, cmap&#x3D;plt.cm.gray\_r)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a href="output_117_0.png"><img src="output_117_0.png" alt="png">png</a></p>
<p>我们都知道，一个手写字符的数据是由 8x8 的矩阵表示。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">digits.images\[0\]  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],</span><br><span class="line">       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],</span><br><span class="line">       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],</span><br><span class="line">       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]]) </span><br></pre></td></tr></table></figure>

<p>如果我们针对该矩阵进行扁平化处理，就能变为 1x64 的向量。对于这样一个高维向量，虽然可以在聚类时直接计算距离，但却无法很好地在二维平面中表示相应的数据点。因为，二维平面中的点只由横坐标和纵坐标组成。</p>
<p>所以，为了尽可能还原聚类的过程，我们需要将 1x64 的行向量（64 维），处理成 1x2 的行向量（2 维），也就是降维的过程。</p>
<p>既然是降低维度，那么应该怎样做呢？是直接取前面两位数，或者随机取出两位？当然不是。这里学习一种新方法，叫 PCA 主成分分析。</p>
<h3 id="PCA-主成分分析（降维）"><a href="#PCA-主成分分析（降维）" class="headerlink" title="PCA 主成分分析（降维）"></a><a href="#PCA-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88%E9%99%8D%E7%BB%B4%EF%BC%89" title="PCA 主成分分析（降维）"></a>PCA 主成分分析（降维）</h3><p>主成分分析是多元线性统计里面的概念，它的英文是：Principal Components Analysis，简称 PCA。主成分分析旨在降低数据的维数，通过保留数据集中的主要成分来简化数据集。</p>
<p>主成分分析的数学原理非常简单，通过对协方差矩阵进行特征分解，从而得出主成分（特征向量）与对应的权值（特征值）。然后剔除那些较小特征值（较小权值）对应的特征，从而达到降低数据维数的目的。</p>
<p>主成分分析通常有两个作用：</p>
<ol>
<li><p>参考本文的目的，方便将数据用于低维空间可视化。聚类过程中的可视化是很有必要的。</p>
</li>
<li><p>高维度数据集往往就意味着计算资源的大量消耗。通过对数据进行降维，我们就能在不较大影响结果的同时，减少模型学习时间。</p>
</li>
</ol>
<p>现在，假定我们需要将特征维度从 n$n$ 维降到 m$m$ 维，PCA 的计算流程如下：</p>
<p><strong>1.对各维度特征进行标准化处理：</strong></p>
<p>x(i)j=x(i)j−μjsj</p>
<p>$$xj(i)=xj(i)−μjsj$$</p>
<p>其中，μj$μj$ 为特征 j$j$ 的均值，sj$sj$ 为特征 j$j$ 的标准差。</p>
<p><strong>2.计算对应的协方差矩阵：</strong><br>Σ=1mm∑i=1(x(i))(x(i))T=1m⋅XTX</p>
<p>$$Σ=1m∑i=1m(x(i))(x(i))T=1m⋅XTX$$</p>
<p><strong>3.对协方差矩阵进行奇异值分解（SVD），得到特征向量：</strong><br>(U,S,VT)=SVD(Σ)</p>
<p>$$(U,S,VT)=SVD(Σ)$$</p>
<p><strong>4.从 U$U$ 中取出前 m$m$ 个左奇异向量，构成一个约减矩阵 Ureduce$Ureduce$ ：</strong><br>Ureduce=(u(1),u(2),⋯,u(k))</p>
<p>$$Ureduce=(u(1),u(2),⋯,u(k))$$</p>
<p><strong>5.计算新的特征向量 z(i)$z(i)$：</strong><br>z(i)=UTreduce⋅x(i)</p>
<p>$$z(i)=UreduceT⋅x(i)$$</p>
<p><strong>6.最后根据新的特征向量执行特征还原：</strong><br>xnew=Ureducez(i)</p>
<p>$$xnew=Ureducez(i)$$</p>
<p>PCA 的过程听起来简单，执行起来还是比较麻烦的。所以，我们这里直接使用 scikit-learn 中 <code>PCA</code> 方法完成：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.decomposition.PCA(n\_components&#x3D;None, copy&#x3D;True, whiten&#x3D;False, svd\_solver&#x3D;&#39;auto&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>其中：</p>
<ul>
<li><code>n_components=</code> 表示需要保留主成分（特征）的数量。</li>
<li><code>copy=</code> 表示针对原始数据降维还是针对原始数据副本降维。当参数为 False 时，降维后的原始数据会发生改变，这里默认为 True。</li>
<li><code>whiten=</code> 白化表示将特征之间的相关性降低，并使得每个特征具有相同的方差。</li>
<li><code>svd_solver=</code> 表示奇异值分解 SVD 的方法。有 4 参数，分别是：<code>auto</code>, <code>full</code>, <code>arpack</code>, <code>randomized</code>。</li>
</ul>
<p>在使用 PCA 降维时，我们也会使用到 <code>PCA.fit()</code> 方法。<code>.fit()</code> 是 scikit-learn 训练模型的通用方法，但是该方法本身返回的是模型的参数。所以，通常我们会使用 <code>PCA.fit_transform()</code> 方法直接返回降维后的数据结果。</p>
<p>下面，我们就针对 DIGITS 数据集进行特征降维。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA  </span><br><span class="line">  </span><br><span class="line">\# PCA 将数据降为 2 维  </span><br><span class="line">pca &#x3D; PCA(n\_components&#x3D;2)  </span><br><span class="line">pca\_data &#x3D; pca.fit\_transform(digits.data)  </span><br><span class="line">pca\_data  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[ -1.25946556,  21.27488973],</span><br><span class="line">       [  7.95761389, -20.7686883 ],</span><br><span class="line">       [  6.99192015,  -9.95599952],</span><br><span class="line">       ...,</span><br><span class="line">       [ 10.80128328,  -6.96025693],</span><br><span class="line">       [ -4.87209994,  12.42397329],</span><br><span class="line">       [ -0.34439245,   6.36553344]]) </span><br></pre></td></tr></table></figure>

<p>可以看到，每一行的特征已经由先前的 64 个缩减为 2 个了。</p>
<p>接下来将降维后的数据绘制到二维平面中。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.scatter(pca\_data\[:,0\], pca\_data\[:,1\])  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_147_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_147_1.png"></a></p>
<p>上图就是 DIGITS 数据集中 1797 个样本通过 PCA 降维后对应在二维平面的数据点。</p>
<p>现在，我们可以直接使用 BIRCH 对降维后的数据进行聚类。由于我们提前知道这是手写数字字符，所以选择聚为 <code>10</code> 类。当然，在聚类时，我们只是知道大致要聚集的类别数量，而并不知道数据对应的标签值。</p>
<p>BIRCH 在 scikit-learn 对应的主要类及参数如下：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cluster.Birch(threshold&#x3D;0.5, branching\_factor&#x3D;50, n\_clusters&#x3D;3, compute\_labels&#x3D;True, copy&#x3D;True)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p>其中：</p>
<ul>
<li><code>threshold</code>: 每个 CF 的空间阈值 τ$τ$。参数值越小，则 CF 特征树的规模会越大，学习时花费的时间和内存会越多。默认值是 0.5，但如果样本的方差较大，则一般需要增大这个默认值。</li>
<li><code>branching_factor</code>: CF 树中所有节点的最大 CF 数。该参数默认为 50，如果样本量非常大，一般需要增大这个默认值。</li>
<li><code>n_clusters</code>: 虽然层次聚类无需预先设定类别数量，但可以设定期望查询的类别数。</li>
</ul>
<p>接下来，使用 BIRCH 算法得到 PCA 降维后数据的聚类结果：</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import Birch  </span><br><span class="line">  </span><br><span class="line">birch &#x3D; Birch(n\_clusters&#x3D;10)  </span><br><span class="line">cluster\_pca &#x3D; birch.fit\_predict(pca\_data)  </span><br><span class="line">cluster\_pca  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([3, 0, 0, ..., 0, 5, 9]) </span><br></pre></td></tr></table></figure>

<p>利用得到的聚类结果对散点图进行着色。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.scatter(pca\_data\[:,0\], pca\_data\[:,1\],c&#x3D;cluster\_pca)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_156_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_156_1.png"></a></p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">\# 计算聚类过程中的决策边界  </span><br><span class="line">x\_min, x\_max &#x3D; pca\_data\[:, 0\].min() - 1, pca\_data\[:, 0\].max() + 1  </span><br><span class="line">y\_min, y\_max &#x3D; pca\_data\[:, 1\].min() - 1, pca\_data\[:, 1\].max() + 1  </span><br><span class="line">  </span><br><span class="line">xx, yy &#x3D; np.meshgrid(np.arange(x\_min, x\_max, .4), np.arange(y\_min, y\_max, .4))  </span><br><span class="line">temp\_cluster &#x3D; birch.predict(np.c\_\[xx.ravel(), yy.ravel()\])  </span><br><span class="line">  </span><br><span class="line">\# 将决策边界绘制出来  </span><br><span class="line">temp\_cluster &#x3D; temp\_cluster.reshape(xx.shape)  </span><br><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.contourf(xx, yy, temp\_cluster, cmap&#x3D;plt.cm.bwr, alpha&#x3D;.3)  </span><br><span class="line">plt.scatter(pca\_data\[:, 0\], pca\_data\[:, 1\], c&#x3D;cluster\_pca, s&#x3D;15)  </span><br><span class="line">  </span><br><span class="line">\# 图像参数设置  </span><br><span class="line">plt.xlim(x\_min, x\_max)  </span><br><span class="line">plt.ylim(y\_min, y\_max)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_157_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_157_1.png"></a></p>
<p>其实，我们可以利用预先知道的各字符对应的标签对散点图进行着色，对比上面的聚类结果。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.scatter(pca\_data\[:,0\], pca\_data\[:,1\],c&#x3D;digits.target)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_159_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_159_1.png"></a></p>
<p>对照两幅图片，你会发现对 PCA 降维数据的聚类结果大致符合原数据的分布趋势。这里色块的颜色不对应没有关系，因为原标签和聚类标签的顺序不对应，只需要关注数据块的分布规律即可。</p>
<p>不过，使用真实标签绘制出来的散点图明显凌乱很多，这其实是由于 PCA 降维造成的。</p>
<p>一般情况下，我们输入到聚类模型中的数据不一定要是降维后的数据。下面输入原数据重新聚类试一试。</p>
<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cluster\_ori &#x3D; birch.fit\_predict(digits.data)  </span><br><span class="line">cluster\_ori  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([7, 9, 4, ..., 4, 1, 4]) </span><br></pre></td></tr></table></figure>

<p>| </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> | </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.scatter(pca\_data\[:,0\], pca\_data\[:,1\],c&#x3D;cluster\_ori)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> |</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_163_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_163_1.png"></a></p>
<p>现在你会发现，实验得到的聚类结果更加符合原数据集的分布规律了。再次强调，这里颜色不分离其实是由于 PCA 降维后在二维平面可视化的效果，不代表真实的聚类效果。</p>
<p>不过，最后我们再强调一下 PCA 的使用情形。一般情况下，我们不会拿到数据就进行 PCA 处理，只有当算法不尽如人意、训练时间太长、需要可视化等情形才考虑使用 PCA。其主要原因是，PCA 被看作是对数据的有损压缩，会造成数据集原始特征丢失。</p>
<p>本次我们了解了层次聚类方法，特别地学习了向上、向下以及 BIRCH 算法。其中，比较常用的是自底向上或 BIRCH 方法，且 BIRCH 拥有计算高效的特点。不过，BIRCH 也有一些弊端，例如对高维数据的聚类效果往往不太好，有时候我们也会使用 Mini Batch K-Means 进行替代。最后，通过表格对比本次实验的 3 种层次聚类法的优缺点：</p>
<p><a target="_blank" rel="noopener" href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/22.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/22.png"></a></p>
<p><strong>拓展阅读：</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hierarchical_clustering">Hierarchical clustering - Wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/BIRCH">BIRCH - Wikipedia</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>机器学习|层次聚类方法</p><p><a href="http://www.laugh12321.cn/blog/2019/02/11/hierarchical_clustering_method/">http://www.laugh12321.cn/blog/2019/02/11/hierarchical_clustering_method/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Laugh</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-02-11</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-10-23</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Cluster/">Cluster</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Hierarchical-Clustering/">Hierarchical Clustering</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5f902f1aa809f500123872bf&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/alipay-reward-image.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh&#039;s%20blog/images/reward/wechat-reward-image.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/03/09/gradient_descent/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">译|Gradient Descent in Python</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/02/09/image_compression_using_mini_batch_k-means/"><span class="level-item">使用 Mini Batch K-Means 进行图像压缩</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.css"><script src="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.2/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "69514f1fbfcec17b5513d6d69f71411f",
            repo: "blog",
            owner: "laugh12321",
            clientID: "f831b7db637971edc8c9",
            clientSecret: "9767a6cb27b89b67e2875e8915671499b75a6436",
            admin: ["laugh12321"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars0.githubusercontent.com/u/38065710?s=400&amp;u=80ee4d360562451484188e6f1677fa442720891b&amp;v=4" alt="Laugh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Laugh</p><p class="is-size-6 is-block">The life I want, there is no shortcut.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Northern Hemisphere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/blog/archives"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/blog/categories"><p class="title">29</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/blog/tags"><p class="title">50</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/laugh12321" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/laugh12321"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="BiliBili" href="https://space.bilibili.com/86034462"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/laugh12321"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Steam" href="https://steamcommunity.com/id/laugh12321/"><i class="fab fa-steam"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#自底向上层次聚类法"><span class="level-left"><span class="level-item">1</span><span class="level-item">自底向上层次聚类法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#自底向上层次聚类流程"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">自底向上层次聚类流程</span></span></a></li><li><a class="level is-mobile" href="#距离计算方法"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">距离计算方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#单连接（Single-linkage）"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">单连接（Single-linkage）</span></span></a></li><li><a class="level is-mobile" href="#全连接（Complete-linkage）"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">全连接（Complete-linkage）</span></span></a></li><li><a class="level is-mobile" href="#平均连接（Average-linkage）"><span class="level-left"><span class="level-item">1.2.3</span><span class="level-item">平均连接（Average-linkage）</span></span></a></li><li><a class="level is-mobile" href="#中心连接（Center-linkage）"><span class="level-left"><span class="level-item">1.2.4</span><span class="level-item">中心连接（Center-linkage）</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Agglomerative-聚类-Python-实现"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Agglomerative 聚类 Python 实现</span></span></a></li><li><a class="level is-mobile" href="#使用-scikit-learn-完成-Agglomerative-聚类"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">使用 scikit-learn 完成 Agglomerative 聚类</span></span></a></li></ul></li><li><a class="level is-mobile" href="#自顶向下层次聚类法"><span class="level-left"><span class="level-item">2</span><span class="level-item">自顶向下层次聚类法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#利用-K-Means-算法进行分割"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">利用 K-Means 算法进行分割</span></span></a></li><li><a class="level is-mobile" href="#利用平均距离进行分割"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">利用平均距离进行分割</span></span></a></li></ul></li><li><a class="level is-mobile" href="#BIRCH-聚类算法"><span class="level-left"><span class="level-item">3</span><span class="level-item">BIRCH 聚类算法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#CF-CF-聚类特征"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">CF$CF$ 聚类特征</span></span></a></li><li><a class="level is-mobile" href="#CF-CF-聚类特征树"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">CF$CF$ 聚类特征树</span></span></a></li><li><a class="level is-mobile" href="#BIRCH-聚类实现"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">BIRCH 聚类实现</span></span></a></li><li><a class="level is-mobile" href="#PCA-主成分分析（降维）"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">PCA 主成分分析（降维）</span></span></a></li></ul></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Ubuntu/"><span class="level-start"><span class="level-item">Ubuntu</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/Ubuntu/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><span class="level-start"><span class="level-item">环境配置</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E5%8D%9A%E5%AE%A2/"><span class="level-start"><span class="level-item">博客</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E5%8D%9A%E5%AE%A2/%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96/"><span class="level-start"><span class="level-item">主题美化</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">15</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">K-近邻算法</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-start"><span class="level-item">人工神经网络</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/"><span class="level-start"><span class="level-item">决策树</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB/"><span class="level-start"><span class="level-item">划分聚类</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB/K-Means/"><span class="level-start"><span class="level-item">K-Means</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92/"><span class="level-start"><span class="level-item">多项式回归</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/"><span class="level-start"><span class="level-item">层次聚类</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA/"><span class="level-start"><span class="level-item">感知机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8F%90%E5%8D%87/"><span class="level-start"><span class="level-item">提升</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"><span class="level-start"><span class="level-item">支持向量机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"><span class="level-start"><span class="level-item">朴素贝叶斯</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><span class="level-start"><span class="level-item">梯度下降</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><span class="level-start"><span class="level-item">线性回归</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A3%85%E8%A2%8B/"><span class="level-start"><span class="level-item">装袋</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"><span class="level-start"><span class="level-item">推荐系统</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"><span class="level-start"><span class="level-item">矩阵分解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"><span class="level-start"><span class="level-item">论文翻译</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"><span class="level-start"><span class="level-item">计算机视觉</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-start"><span class="level-item">卷积神经网络</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">迁移学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"><span class="level-start"><span class="level-item">风格迁移</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/blog/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/blog/categories/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">排序算法</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Artificial-Neural-Network/"><span class="tag">Artificial Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/BGD/"><span class="tag">BGD</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/BPR/"><span class="tag">BPR</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Bagging/"><span class="tag">Bagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Blog/"><span class="tag">Blog</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/BoosTing/"><span class="tag">BoosTing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/CV/"><span class="tag">CV</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Cluster/"><span class="tag">Cluster</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ECS/"><span class="tag">ECS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GD/"><span class="tag">GD</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GitHub-Pages/"><span class="tag">GitHub Pages</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Hierarchical-Clustering/"><span class="tag">Hierarchical Clustering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Jekyll/"><span class="tag">Jekyll</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Means/"><span class="tag">K-Means</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/K-Nearest-Neighbors/"><span class="tag">K-Nearest Neighbors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/KNN/"><span class="tag">KNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Kreas/"><span class="tag">Kreas</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAE/"><span class="tag">MAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MAPE/"><span class="tag">MAPE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MBGD/"><span class="tag">MBGD</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MF/"><span class="tag">MF</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MSE/"><span class="tag">MSE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/NN/"><span class="tag">NN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Neural-Style-Transfer/"><span class="tag">Neural Style Transfer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Next/"><span class="tag">Next</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/RS/"><span class="tag">RS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Recommender-System/"><span class="tag">Recommender System</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SGD/"><span class="tag">SGD</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SVM/"><span class="tag">SVM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Seafile/"><span class="tag">Seafile</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Transfer-Learning/"><span class="tag">Transfer Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/VGG19/"><span class="tag">VGG19</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/"><span class="tag">冒泡排序</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9/"><span class="tag">图像压缩</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"><span class="tag">快速排序</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"><span class="tag">排序算法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/"><span class="tag">插入排序</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><span class="tag">环境配置</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/"><span class="tag">选择排序</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Laugh</a><p class="is-size-7"><span>&copy; 2020 Laugh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>