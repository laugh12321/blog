<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Laugh&#39;s blog</title>
  
  
  <link href="http://www.laugh12321.cn/blog/atom.xml" rel="self"/>
  
  <link href="http://www.laugh12321.cn/blog/"/>
  <updated>2020-10-22T08:07:02.907Z</updated>
  <id>http://www.laugh12321.cn/blog/</id>
  
  <author>
    <name>Laugh</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用 Mini Batch K-Means 进行图像压缩</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/09/image_compression_using_mini_batch_k-means/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/09/image_compression_using_mini_batch_k-means/</id>
    <published>2019-02-09T00:00:00.000Z</published>
    <updated>2020-10-22T08:07:02.907Z</updated>
    
    <content type="html"><![CDATA[<p>针对一张成都著名景点：锦里的图片，通过 Mini Batch K-Means 的方法将相近的像素点聚合后用同一像素点代替，以达到图像压缩的效果。  </p><a id="more"></a><h3 id="图像导入"><a href="#图像导入" class="headerlink" title="图像导入"></a>图像导入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\# 使用 Matplotlib 可视化示例图片  </span><br><span class="line">%matplotlib inline  </span><br><span class="line">import matplotlib.pyplot as plt   </span><br><span class="line">import matplotlib.image as mpimg   </span><br><span class="line">  </span><br><span class="line">chengdu &#x3D; mpimg.imread(&#39;chengdu.png&#39;) # 将图片加载为 ndarray 数组  </span><br><span class="line">plt.imshow(chengdu) # 将数组还原成图像  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_2_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_2_1.png" alt="&lt;matplotlib.image.AxesImage at 0x7f347f1b0320&gt;"></a></p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chengdu.shape  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(516, 819, 3) </span><br></pre></td></tr></table></figure><p>在使用 <code>mpimg.imread</code> 函数读取图片后，实际上返回的是一个 <code>numpy.array</code> 类型的数组，该数组表示的是一个像素点的矩阵，包含长，宽，高三个要素。如成都锦里这张图片，总共包含了 516$516$ 行，819$819$ 列共 516⋅819=422604$516⋅819=422604$ 个像素点，每一个像素点的高度对应着计算机颜色中的三原色 R,G,B$R,G,B$（红，绿，蓝），共 3 个要素构成。</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86" title="数据预处理"></a>数据预处理</h3><p>为方便后期的数据处理，需要对数据进行降维。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\# 将形状为 (516, 819, 3) 的数据转换为 (422604, 3) 形状的数据。  </span><br><span class="line">data &#x3D; chengdu.reshape(516\*819, 3)  </span><br><span class="line">data.shape, data\[10\]  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((422604, 3), array([0.12941177, 0.13333334, 0.14901961], dtype&#x3D;float32)) </span><br></pre></td></tr></table></figure><h3 id="像素点种类个数计算"><a href="#像素点种类个数计算" class="headerlink" title="像素点种类个数计算"></a><a href="#%E5%83%8F%E7%B4%A0%E7%82%B9%E7%A7%8D%E7%B1%BB%E4%B8%AA%E6%95%B0%E8%AE%A1%E7%AE%97" title="像素点种类个数计算"></a>像素点种类个数计算</h3><p>尽管有 <code>422604</code> 个像素点，但其中仍然有许多相同的像素点。在此我们定义：R,G,B$R,G,B$ 值相同的点为一个种类，其中任意值不同的点为不同种类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算像素点种类个数  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def get\_variety(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    预处理后像素点集合  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    num\_variety -- 像素点种类个数  </span><br><span class="line">      </span><br><span class="line">    思路：将数据转化为 list 类型，然后将每一个元素转换为 tuple 类型，最后利用 set() 和 len() 函数进行计算。  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">    temp &#x3D; data.tolist()  </span><br><span class="line">    num\_variety&#x3D;len(set(\[tuple(t) for t in temp\]))  </span><br><span class="line">      </span><br><span class="line">    return num\_variety  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_variety(data), data\[20\]  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(100109, array([0.24705882, 0.23529412, 0.2627451 ], dtype&#x3D;float32)) </span><br></pre></td></tr></table></figure><h3 id="Mini-Batch-K-Means-聚类"><a href="#Mini-Batch-K-Means-聚类" class="headerlink" title="Mini Batch K-Means 聚类"></a><a href="#Mini-Batch-K-Means-%E8%81%9A%E7%B1%BB" title="Mini Batch K-Means 聚类"></a>Mini Batch K-Means 聚类</h3><p>像素点种类的数量是决定图片大小的主要因素之一，在此使用 Mini Batch K-Means 的方式将图片的像素点进行聚类，将相似的像素点用同一像素点值来代替，从而降低像素点种类的数量，以达到压缩图片的效果。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import MiniBatchKMeans  </span><br><span class="line">  </span><br><span class="line">model &#x3D; MiniBatchKMeans(10) # 聚类簇数量设置为 10 类  </span><br><span class="line">  </span><br><span class="line">model.fit(data)  </span><br><span class="line">predict&#x3D;model.predict(data)  </span><br><span class="line">  </span><br><span class="line">new\_colors &#x3D; model.cluster\_centers\_\[predict\]  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\# 调用前面实现计算像素点种类的函数，计算像素点更新后种类的个数  </span><br><span class="line">get\_variety(new\_colors)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10 </span><br></pre></td></tr></table></figure><h3 id="图像压缩前后展示"><a href="#图像压缩前后展示" class="headerlink" title="图像压缩前后展示"></a><a href="#%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9%E5%89%8D%E5%90%8E%E5%B1%95%E7%A4%BA" title="图像压缩前后展示"></a>图像压缩前后展示</h3><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\# 将聚类后并替换为类别中心点值的像素点，变换为数据处理前的格式，并绘制出图片进行对比展示  </span><br><span class="line">fig, ax &#x3D; plt.subplots(1, 2, figsize&#x3D;(16, 6))  </span><br><span class="line">  </span><br><span class="line">new\_chengdu &#x3D; new\_colors.reshape(chengdu.shape)  </span><br><span class="line">ax\[0\].imshow(chengdu)  </span><br><span class="line">ax\[1\].imshow(new\_chengdu)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_17_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_17_1.png" alt="&lt;matplotlib.image.AxesImage at 0x7f347651cac8&gt;"></a></p><p>通过图片对比，可以十分容易发现画质被压缩了。其实，因为使用了聚类，压缩后的图片颜色就变为了 10 种。</p><p>接下来，使用 <code>mpimg.imsave()</code> 函数将压缩好的文件进行存储，并对比压缩前后图像的体积变化。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\# 运行对比  </span><br><span class="line">mpimg.imsave(&quot;new\_chengdu.png&quot;,new\_chengdu)  </span><br><span class="line">!du -h new\_chengdu.png  </span><br><span class="line">!du -h chengdu.png  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">220K    new_chengdu.png</span><br><span class="line">1.1M    chengdu.png </span><br></pre></td></tr></table></figure><p>可以看到，使用 Mini Batch K-Means 聚类方法对图像压缩之后，体积明显缩小。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;针对一张成都著名景点：锦里的图片，通过 Mini Batch K-Means 的方法将相近的像素点聚合后用同一像素点代替，以达到图像压缩的效果。  &lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="K-Means" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/K-Means/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="K-Means" scheme="http://www.laugh12321.cn/blog/tags/K-Means/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|划分聚类之 K-Means 详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/09/cluster_of_k-means/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/09/cluster_of_k-means/</id>
    <published>2019-02-09T00:00:00.000Z</published>
    <updated>2020-10-22T08:04:02.957Z</updated>
    
    <content type="html"><![CDATA[<p>划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。</p><p>在划分的过程中，首先由用户确定划分子集的个数 k$k$，然后随机选定 k$k$ 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 k$k$ 个子集，即将数据划分为 k$k$ 类。</p><p>而评估划分的好坏标准就是：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。</p><a id="more"></a><h2 id="K-Means-聚类方法"><a href="#K-Means-聚类方法" class="headerlink" title="K-Means 聚类方法"></a>K-Means 聚类方法</h2><p>在划分聚类中，K-Means 是最具有代表性的算法，下面用图片的方式演示 K-Means 的基本算法流程。希望大家能通过简单的图文演示，对 K-Means 方法的原理过程产生大致的印象。</p><p><strong>[1] 对于未聚类数据集，首先随机初始化 K 个（代表拟聚类簇个数）中心点，如图红色五角星所示。</strong></p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/1.png"></a></p><p><strong>[2] 每一个样本按照距离自身最近的中心点进行聚类，等效于通过两中心点连线的中垂线划分区域。</strong></p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/2.png"></a></p><p><strong>[3] 依据上次聚类结果，移动中心点到个簇的质心位置，并将此质心作为新的中心点</strong></p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/3.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/3.png"></a></p><p><strong>[4] 反复迭代，直至中心点的变化满足收敛条件（变化很小或几乎不变化），最终得到聚类结果。</strong></p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/4.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/4.png"></a></p><p>在对 K-Means 有了一个直观了解后，下面我们用 Python 来进行实现。</p><h3 id="生成示例数据"><a href="#生成示例数据" class="headerlink" title="生成示例数据"></a><a href="#%E7%94%9F%E6%88%90%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE" title="生成示例数据"></a>生成示例数据</h3><p>首先通过 <code>scikit-learn</code> 模块的 <code>make_blobs()</code> 函数生成本次实验所需的示例数据。该方法可以按照我们的要求，生成特定的团状数据。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data,label &#x3D; sklearn.datasets.make\_blobs(n\_samples&#x3D;100,n\_features&#x3D;2,centers&#x3D;3,center\_box&#x3D;(-10.0,10.0),random\_state&#x3D;None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中参数为：</p><ul><li><code>n_samples</code>：表示生成数据总个数,默认为 100 个。</li><li><code>n_features</code>：表示每一个样本的特征个数，默认为 2 个。</li><li><code>centers</code>：表示中心点的个数，默认为 3 个。</li><li><code>center_box</code>：表示每一个中心的边界,默认为 -10.0到10.0。</li><li><code>random_state</code>：表示生成数据的随机数种子。</li></ul><p>返回值为：</p><ul><li><code>data</code>：表示数据信息。</li><li><code>label</code>：表示数据类别。</li></ul><p>根据上面函数，在 0.0 到 10.0 上生成 200 条数据，大致包含 3 个中心。由于是用于演示聚类效果，数据标签就不是必须的了，在生成数据时赋值给 <code>_</code>，后面也不会使用到。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;构造数据  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">from sklearn.datasets import make\_blobs  </span><br><span class="line">  </span><br><span class="line">blobs, \_ &#x3D; make\_blobs(n\_samples&#x3D;200, centers&#x3D;3, random\_state&#x3D;18)  </span><br><span class="line">blobs\[:10\] # 打印出前 10 条数据的信息  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[ 8.28390539,  4.98011149],</span><br><span class="line">       [ 7.05638504,  7.00948082],</span><br><span class="line">       [ 7.43101466, -6.56941148],</span><br><span class="line">       [ 8.20192526, -6.4442691 ],</span><br><span class="line">       [ 3.15614247,  0.46193832],</span><br><span class="line">       [ 7.7037692 ,  6.14317389],</span><br><span class="line">       [ 5.62705611, -0.35067953],</span><br><span class="line">       [ 7.53828533, -4.86595492],</span><br><span class="line">       [ 8.649291  ,  3.98488194],</span><br><span class="line">       [ 7.91651636,  4.54935348]]) </span><br></pre></td></tr></table></figure><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a><a href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96" title="数据可视化"></a>数据可视化</h3><p>为了更加直观的查看数据分布情况，使用 <code>matplotlib</code> 将生成数据绘画出来。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;数据展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">%matplotlib inline  </span><br><span class="line">import matplotlib.pyplot as plt  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20);  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_21_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_21_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd80877b8&gt;"></a></p><h3 id="随机初始化中心点"><a href="#随机初始化中心点" class="headerlink" title="随机初始化中心点"></a><a href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%AD%E5%BF%83%E7%82%B9" title="随机初始化中心点"></a>随机初始化中心点</h3><p>当我们得到数据时，依照划分聚类方法的思想，首先需要随机选取 k$k$ 个点作为每一个子集的中心点。从图像中，通过肉眼很容易的发现该数据集有 <code>3</code> 个子集。接下来，用 <code>numpy</code> 模块随机生成 <code>3</code> 个中心点，为了更方便展示，这里我们加入了随机数种子以便每一次运行结果相同。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;初始化中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import numpy as np  </span><br><span class="line">  </span><br><span class="line">def random\_k(k, data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    init\_centers -- 初始化中心点  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    prng &#x3D; np.random.RandomState(27) # 定义随机种子  </span><br><span class="line">    num\_feature&#x3D;np.shape(data)\[1\]  </span><br><span class="line">    init\_centers &#x3D; prng.randn(k, num\_feature)\*5 # 由于初始化的随机数是从-1到1，为了更加贴近数据集这里乘了一个 5  </span><br><span class="line">    return init\_centers  </span><br><span class="line">  </span><br><span class="line">init\_centers&#x3D;random\_k(3, blobs)  </span><br><span class="line">init\_centers  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 6.42802708, -1.51776689],</span><br><span class="line">       [ 3.09537831,  1.97999275],</span><br><span class="line">       [ 1.11702824, -0.27169709]]) </span><br></pre></td></tr></table></figure><p>在随机生成好中心点之后，将其在图像中表示出来，这里同样使用红色五角星表示。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;初始中心点展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20);  </span><br><span class="line">plt.scatter(init\_centers\[:,0\], init\_centers\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_26_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_26_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd849b128&gt;"></a></p><h3 id="计算样本与中心点的距离"><a href="#计算样本与中心点的距离" class="headerlink" title="计算样本与中心点的距离"></a><a href="#%E8%AE%A1%E7%AE%97%E6%A0%B7%E6%9C%AC%E4%B8%8E%E4%B8%AD%E5%BF%83%E7%82%B9%E7%9A%84%E8%B7%9D%E7%A6%BB" title="计算样本与中心点的距离"></a>计算样本与中心点的距离</h3><p>为了找到最合适的中心点位置，需要计算每一个样本和中心点的距离，从而根据距离更新中心点位置。常见的距离计算方法有欧几里得距离和余弦相似度，本实验采用更常见且更易于理解的欧几里得距离（欧式距离）。</p><p>欧式距离源自 N$N$ 维欧氏空间中两点之间的距离公式。表达式如下:</p><p>deuc= ⎷N∑i=1(Xi−Yi)2(1)</p><p>$$(1)deuc=∑i=1N(Xi−Yi)2$$</p><p>其中：</p><ul><li>X$X$, Y$Y$ ：两个数据点</li><li>N$N$：每个数据中有 N$N$ 个特征值，</li><li>Xi$Xi$ ：数据 X$X$ 的第 i$i$ 个特征值</li></ul><p>将两个数据 X$X$ 和 Y$Y$ 中的每一个对应的特征值之间差值的平方，再求和，最后开平方，便是欧式距离。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算欧氏距离  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def d\_euc(x, y):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    x -- 数据 a  </span><br><span class="line">    y -- 数据 b  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    d -- 数据 a 和 b 的欧氏距离  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    d &#x3D; np.sqrt(np.sum(np.square(x - y)))  </span><br><span class="line">    return d  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="最小化-SSE，更新聚类中心"><a href="#最小化-SSE，更新聚类中心" class="headerlink" title="最小化 SSE，更新聚类中心"></a><a href="#%E6%9C%80%E5%B0%8F%E5%8C%96-SSE%EF%BC%8C%E6%9B%B4%E6%96%B0%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83" title="最小化 SSE，更新聚类中心"></a>最小化 SSE，更新聚类中心</h3><p>和第一章的回归算法通过减小目标函数（如：损失函数）的值拟合数据集一样，聚类算法通常也是优化一个目标函数，从而提高聚类的质量。在聚类算法中，常常使用误差的平方和 SSE（Sum of squared errors）作为度量聚类效果的标准，当 SSE 越小表示聚类效果越好。其中 SSE 表示为：</p><p>SSE(C)=K∑k=1∑xi∈Ck|xi−ck|2(2)</p><p>$$(2)SSE(C)=∑k=1K∑xi∈Ck|xi−ck|2$$</p><p>其中数据集 D={x1,x2,…,xn}$D={x1,x2,…,xn}$，xi$xi$表示每一个样本值，C$C$ 表示通过 K-Means 聚类分析后的产生类别集合 C={C1,C2,…,CK}$C={C1,C2,…,CK}$ ，ck$ck$ 是类别 Ck$Ck$ 的中心点，其中 ck$ck$ 计算方式为：</p><p>ck=∑xi∈CkxiI(Ck)(3)</p><p>$$(3)ck=∑xi∈CkxiI(Ck)$$</p><p>I(Ck)$I(Ck)$ 表示在第 k$k$ 个集合 Ck$Ck$ 中数据的个数。</p><p>当然，我们希望同最小化损失函数一样，最小化 SSE 函数，从而找出最优化的聚类模型，但是求其最小值并不容易，是一个 NP 难（非确定性多项式）的问题，其中 NP 难问题是一个经典图论问题，至今也没有找到一个完美且有效的算法。</p><p>下面我们对中心点的更新用代码的方式进行实现：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;中心点的更新  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def update\_center(clusters, data, centers):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    clusters -- 每一点分好的类别  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    centers -- 中心点集合  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    new\_centers.reshape(num\_centers,num\_features) -- 新中心点集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">    num\_centers &#x3D; np.shape(centers)\[0\]  # 中心点的个数  </span><br><span class="line">    num\_features &#x3D; np.shape(centers)\[1\]  # 每一个中心点的特征数  </span><br><span class="line">    container &#x3D; \[\]  </span><br><span class="line">    for x in range(num\_centers):  </span><br><span class="line">        each\_container &#x3D; \[\]  </span><br><span class="line">        container.append(each\_container)  # 首先创建一个容器,将相同类别数据存放到一起  </span><br><span class="line">  </span><br><span class="line">    for i, cluster in enumerate(clusters):  </span><br><span class="line">        container\[cluster\].append(data\[i\])  </span><br><span class="line">  </span><br><span class="line">    # 为方便计算，将 list 类型转换为 np.array 类型  </span><br><span class="line">    container &#x3D; np.array(list(map(lambda x: np.array(x), container)))  </span><br><span class="line">  </span><br><span class="line">    new\_centers &#x3D; np.array(\[\])  # 创建一个容器，存放中心点的坐标  </span><br><span class="line">    for i in range(len(container)):  </span><br><span class="line">        each\_center &#x3D; np.mean(container\[i\], axis&#x3D;0)  # 计算每一子集中数据均值作为中心点  </span><br><span class="line">        new\_centers &#x3D; np.append(new\_centers, each\_center)  </span><br><span class="line">  </span><br><span class="line">    return new\_centers.reshape(num\_centers, num\_features)  # 以矩阵的方式返回中心点坐标  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="K-Means-聚类算法实现"><a href="#K-Means-聚类算法实现" class="headerlink" title="K-Means 聚类算法实现"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="K-Means 聚类算法实现"></a>K-Means 聚类算法实现</h3><p>K-Means 算法则采用的是迭代算法，避开优化 SSE 函数，通过不断移动中心点的距离，最终达到聚类的效果。</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><a href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B" title="算法流程"></a>算法流程</h4><ol><li>初始化中心点：判断数据集可能被分为 k$k$ 个子集，随机生成 k$k$ 个随机点作为每一个子集的中心点。</li><li>距离计算，类别标记：样本和每一个中心点进行距离计算，将距离最近的中心点所代表的类别标记为该样本的类别。</li><li>中心点位置更新：计算每一个类别中的所有样本的均值，作为新的中心点位置。</li><li>重复 2，3 步骤，直到中心点位置不再变化。</li></ol><h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a><a href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="算法实现"></a>算法实现</h4><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line">33  </span><br><span class="line">34  </span><br><span class="line">35  </span><br><span class="line">36  </span><br><span class="line">37  </span><br><span class="line">38  </span><br><span class="line">39  </span><br><span class="line">40  </span><br><span class="line">41  </span><br><span class="line">42  </span><br><span class="line">43  </span><br><span class="line">44  </span><br><span class="line">45  </span><br><span class="line">46  </span><br><span class="line">47  </span><br><span class="line">48  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;K-Means 聚类  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def kmeans\_cluster(data, init\_centers, k):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    init\_centers -- 初始化中心点集合  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    centers\_container -- 每一次更新中心点的集合  </span><br><span class="line">    cluster\_container -- 每一次更新类别的集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    max\_step &#x3D; 50  # 定义最大迭代次数，中心点最多移动的次数。  </span><br><span class="line">    epsilon &#x3D; 0.001  # 定义一个足够小的数，通过中心点变化的距离是否小于该数，判断中心点是否变化。  </span><br><span class="line">  </span><br><span class="line">    old\_centers &#x3D; init\_centers  </span><br><span class="line">  </span><br><span class="line">    centers\_container &#x3D; \[\]  # 建立一个中心点容器，存放每一次变化后的中心点，以便后面的绘图。  </span><br><span class="line">    cluster\_container &#x3D; \[\]  # 建立一个分类容器，存放每一次中心点变化后数据的类别  </span><br><span class="line">    centers\_container.append(old\_centers)  </span><br><span class="line">  </span><br><span class="line">    for step in range(max\_step):  </span><br><span class="line">        cluster &#x3D; np.array(\[\], dtype&#x3D;int)  </span><br><span class="line">        for each\_data in data:  </span><br><span class="line">            distances &#x3D; np.array(\[\])  </span><br><span class="line">            for each\_center in old\_centers:  </span><br><span class="line">                temp\_distance &#x3D; d\_euc(each\_data, each\_center)  # 计算样本和中心点的欧式距离  </span><br><span class="line">                distances &#x3D; np.append(distances, temp\_distance)  </span><br><span class="line">            lab &#x3D; np.argmin(distances)  # 返回距离最近中心点的索引，即按照最近中心点分类  </span><br><span class="line">            cluster &#x3D; np.append(cluster, lab)  </span><br><span class="line">        cluster\_container.append(cluster)  </span><br><span class="line">  </span><br><span class="line">        new\_centers &#x3D; update\_center(cluster, data, old\_centers)  # 根据子集分类更新中心点  </span><br><span class="line">  </span><br><span class="line">        # 计算每个中心点更新前后之间的欧式距离  </span><br><span class="line">        difference &#x3D; \[\]  </span><br><span class="line">        for each\_old\_center, each\_new\_center in zip(old\_centers, new\_centers):  </span><br><span class="line">            difference.append(d\_euc(each\_old\_center, each\_new\_center))  </span><br><span class="line">          </span><br><span class="line">        if (np.array(difference) &lt; epsilon).all():  # 判断每个中心点移动是否均小于 epsilon  </span><br><span class="line">            return centers\_container, cluster\_container  </span><br><span class="line">  </span><br><span class="line">        centers\_container.append(new\_centers)  </span><br><span class="line">        old\_centers &#x3D; new\_centers  </span><br><span class="line">  </span><br><span class="line">    return centers\_container, cluster\_container  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>完成 K-Means 聚类函数后，接下来用函数得到最终中心点的位置。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算最终中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">centers\_container, cluster\_container &#x3D; kmeans\_cluster(blobs, init\_centers, 3)  </span><br><span class="line">final\_center &#x3D; centers\_container\[-1\]  </span><br><span class="line">final\_cluster &#x3D; cluster\_container\[-1\]  </span><br><span class="line">final\_center  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 7.67007252, -6.44697348],</span><br><span class="line">       [ 6.83832746,  4.98604668],</span><br><span class="line">       [ 3.28477676,  0.15456871]]) </span><br></pre></td></tr></table></figure><p>最后，我们把聚类得到的中心绘制到原图中看一看聚类效果。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;可视化展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;final\_cluster);  </span><br><span class="line">plt.scatter(final\_center\[:,0\], final\_center\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_50_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_50_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd89e8438&gt;"></a></p><h3 id="中心点移动过程可视化"><a href="#中心点移动过程可视化" class="headerlink" title="中心点移动过程可视化"></a><a href="#%E4%B8%AD%E5%BF%83%E7%82%B9%E7%A7%BB%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96" title="中心点移动过程可视化"></a>中心点移动过程可视化</h3><p>截止上小节，已经完成了 K-Means 聚类的流程。为了帮助大家理解，我们尝试将 K-Means 聚类过程中，中心点移动变化的过程绘制出来。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num\_axes &#x3D; len(centers\_container)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(1, num\_axes, figsize&#x3D;(20, 4))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[0\])  </span><br><span class="line">axes\[0\].scatter(init\_centers\[:, 0\], init\_centers\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[0\].set\_title(&quot;initial center&quot;)  </span><br><span class="line">  </span><br><span class="line">for i in range(1, num\_axes-1):  </span><br><span class="line">    axes\[i\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[i\])  </span><br><span class="line">    axes\[i\].scatter(centers\_container\[i\]\[:, 0\],  </span><br><span class="line">                    centers\_container\[i\]\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">    axes\[i\].set\_title(&quot;step &#123;&#125;&quot;.format(i))  </span><br><span class="line">  </span><br><span class="line">axes\[-1\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[-1\])  </span><br><span class="line">axes\[-1\].scatter(final\_center\[:, 0\], final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[-1\].set\_title(&quot;final center&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_53_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_53_1.png" alt="Text(0.5, 1.0, &#39;final center&#39;)">Text(0.5, 1.0, ‘final center’)</a></p><p>你会惊讶的发现，对于示例数据集，虽然我们先前将最大迭代次数 <code>max_step</code> 设为了 <code>50</code>，但实际上 K-Means 迭代 3 次即收敛。原因主要有 2 点：</p><ul><li>初始化中心点的位置很好，比较均匀分布在了数据范围中。如果初始化中心点集中分布在某一角落，迭代次数肯定会增加。</li><li>示例数据分布规整和简单，使得无需迭代多次就能收敛。</li></ul><h3 id="K-Means-算法聚类中的-K-值选择"><a href="#K-Means-算法聚类中的-K-值选择" class="headerlink" title="K-Means 算法聚类中的 K 值选择"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E8%81%9A%E7%B1%BB%E4%B8%AD%E7%9A%84-K-%E5%80%BC%E9%80%89%E6%8B%A9" title="K-Means 算法聚类中的 K 值选择"></a>K-Means 算法聚类中的 K 值选择</h3><p>不知道你是否还记得，前面在学习分类算法 K-近邻的时候，我们讲到了 K 值的选择。而在使用 K-Means 算法聚类时，由于要提前确定随机初始化中心点的数量，同样面临着 K 值选择问题。</p><p>在前面寻找 K 值时，我们通过肉眼观察认为应该聚为 3 类。那么，如果我们设定聚类为 5 类呢？</p><p>这一次，我们尝试通过 <code>scikit-learn</code> 模块中的 K-Means 算法完成聚类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import k\_means  </span><br><span class="line">  </span><br><span class="line">k\_means(X, n\_clusters)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中参数为：</p><ul><li><code>X</code>：表示需要聚类的数据。</li><li><code>n_clusters</code>：表示聚类的个数，也就是 K 值。</li></ul><p>返回值包含：</p><ul><li><code>centroid</code>：表示中心点坐标。</li><li><code>label</code>：表示聚类后每一个样本的类别。</li><li><code>inertia</code>：每一个样本与最近中心点距离的平方和，即 SSE。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;用 scikit-learn 聚类并绘图  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">from sklearn.cluster import k\_means  </span><br><span class="line">model &#x3D; k\_means(blobs, n\_clusters&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">centers &#x3D; model\[0\]  </span><br><span class="line">clusters\_info &#x3D; model\[1\]  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;clusters\_info)  </span><br><span class="line">plt.scatter(centers\[:, 0\], centers\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_60_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_60_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdab66e10&gt;"></a></p><p>从图片上来看，聚为 5 类效果明显不如聚为 3 类的好。当然，我们提前用肉眼就能看出数据大致为 3 团。</p><p>实际的应用过程中，如果通过肉眼无法判断数据应该聚为几类？或者是高维数据无法可视化展示。面对这样的情况，我们就要从数值计算的角度去判断 K 值的大小。</p><p><strong>接下来，将介绍一种启发式学习算法，被称之为 肘部法则，可以帮助我们选取 K 值。</strong></p><p>使用 K-Means 算法聚类时，我们可以计算出按不同 K 值聚类后，每一个样本距离最近中心点距离的平方和 SSE。</p><p>随着 K 值增加时，也就是类别增加时，每个类别中的类内相似性也随之增加，由此造成的 SSE 的变化是单调减小的。可以想象一下，聚类类别的数量和样本的总数相同时，也就是说一个样本就代表一个类别时，这个数值会变成 0。</p><p>下面我们通过代码将不同的数量的聚类下，样本和最近中心点的距离和绘制出来。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">index &#x3D; \[\] # 横坐标数组  </span><br><span class="line">inertia &#x3D; \[\] # 纵坐标数组  </span><br><span class="line">  </span><br><span class="line">\# K 从 1~ 6 聚类  </span><br><span class="line">for i in range(6):  </span><br><span class="line">    model &#x3D; k\_means(blobs, n\_clusters&#x3D;i + 1)  </span><br><span class="line">    index.append(i + 1)  </span><br><span class="line">    inertia.append(model\[2\])  </span><br><span class="line">  </span><br><span class="line">\# 绘制折线图  </span><br><span class="line">plt.plot(index, inertia, &quot;-o&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_63_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_63_1.png" alt="&lt;matplotlib.lines.Line2D at 0x2dbdac2fdd8&gt;"></a></p><p>通过上图可以看到，和预想的一样，样本距离最近中心点距离的总和会随着 K 值的增大而降低。</p><p>现在，回想本实验划分聚类中所讲评估划分的好坏标准：「保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大」。</p><p>当 K 值越大时，越满足「同一划分的样本之间的差异尽可能的小」。而当 K 值越小时，越满足「不同划分中的样本差异尽可能的大畸变程度最大」。那么如何做到两端的平衡呢？</p><p>于是，<strong>我们通过 SSE 所绘制出来的图，将畸变程度最大的点称之为「肘部」</strong>。从图中可以看到，这里的「肘部」是 K = 3（内角最小，弯曲度最大）。这也说明，将样本聚为 3 类是最佳选择（K = 2 比较接近）。这就是所谓的「肘部法则」，你明白了吗？</p><h2 id="K-Means-聚类算法"><a href="#K-Means-聚类算法" class="headerlink" title="K-Means++ 聚类算法"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95" title="K-Means++ 聚类算法"></a>K-Means++ 聚类算法</h2><h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a><a href="#%E9%97%AE%E9%A2%98%E5%BC%95%E5%85%A5" title="问题引入"></a>问题引入</h3><p>随着数据量的增长，分类数目增多时，由于 K-Means 中初始化中心点是随机的，常常会出现：一个较大子集有多个中心点，而其他多个较小子集公用一个中心点的问题。即算法陷入局部最优解而不是达到全局最优解的问题。</p><p>造成这种问题主要原因就是：一部分中心点在初始化时离的太近。下面我们通过例子来进一步了解。</p><h3 id="生成示例数据-1"><a href="#生成示例数据-1" class="headerlink" title="生成示例数据"></a><a href="#%E7%94%9F%E6%88%90%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE-1" title="生成示例数据"></a>生成示例数据</h3><p>同样，我们先使用 <code>scikit-learn</code> 模块的 <code>make_blobs</code> 函数生成本次实验所需数据，本次生成 <code>800</code> 条数据，共 5 堆。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;生成数据并展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">blobs\_plus, \_ &#x3D; make\_blobs(n\_samples&#x3D;800, centers&#x3D;5, random\_state&#x3D;18)  # 生成数据  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20)  # 将数据可视化展示  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_71_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_71_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdae88278&gt;"></a></p><h3 id="随机初始化中心点-1"><a href="#随机初始化中心点-1" class="headerlink" title="随机初始化中心点"></a><a href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%AD%E5%BF%83%E7%82%B9-1" title="随机初始化中心点"></a>随机初始化中心点</h3><p>从数据点分布中可以很容易的观测出聚类数量应该为 5 类，我们先用 K-Means 中随机初始中心点的方法完成聚类：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">km\_init\_center&#x3D;random\_k(5, blobs\_plus)  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20);  </span><br><span class="line">plt.scatter(km\_init\_center\[:,0\], km\_init\_center\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_74_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_74_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdabfea90&gt;"></a></p><h3 id="K-Means-聚类"><a href="#K-Means-聚类" class="headerlink" title="K-Means 聚类"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB" title="K-Means 聚类"></a>K-Means 聚类</h3><p>用传统的 K-Means 算法，将数据集进行聚类，聚类数量为 5。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">km\_centers, km\_clusters &#x3D; kmeans\_cluster(blobs\_plus, km\_init\_center, 5)  </span><br><span class="line">km\_final\_center &#x3D; km\_centers\[-1\]  </span><br><span class="line">km\_final\_cluster &#x3D; km\_clusters\[-1\]  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;km\_final\_cluster)  </span><br><span class="line">plt.scatter(km\_final\_center\[:, 0\], km\_final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_77_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_77_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb2a92b0&gt;"></a></p><p>通过传统 K-Means 算法聚类后，你会发现聚类效果和我们预想不同，我们预想的结果应该是下面这样的：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;\_)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_79_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_79_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb2e6978&gt;"></a></p><p>对比 K-Means 聚类和预想聚类的两张图，可以直观的看出 K-Means 算法显然没有达到最优的聚类效果，出现了本章开头所提到的局部最优解的问题。</p><p>对于局部最优问题是可以通过 SSE 来解决的，即在同一数据集上运行多次 K-Means 算法聚类，之后选取 SSE 最小的那次作为最终的聚类结果。虽然通过 SSE 找到最优解十分困难，但通过 SSE 判断最优解是十分容易的。</p><p>但当遇到更大的数据集，每一次 K-Means 算法会花费大量时间时，如果使用多次运行通过 SSE 来判断最优解，显然不是好的选择。是否有一种方法在初始化中心点时，就能有效避免局部最优问题的出现呢？</p><p>在 K-Means 的基础上，D.Arthur 等人在 2007 年提出了 K-Means++ 算法。其中 K-Means++ 算法主要针对初始化中心点问题进行改进，这样就可以从源头上解决局部最优解的问题。</p><h3 id="K-Means-算法流程"><a href="#K-Means-算法流程" class="headerlink" title="K-Means++ 算法流程"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B" title="K-Means++ 算法流程"></a>K-Means++ 算法流程</h3><p>K-Means++ 相较于 K-Means 在初始化中心点上做了改进，在其他方面和 K-Means 相同。</p><ol><li>在数据集中随机选择一个样本点作为第一个初始化的聚类中心。</li><li>计算样本中的非中心点与最近中心点之间的距离 D(x)$D(x)$ 并保存于一个数组里，将数组中的这些距离加起来得到 Sum(D(x))$Sum(D(x))$。</li><li>取一个落在 Sum(D(x))$Sum(D(x))$范围中的随机值 R$R$ ，重复计算 R=R−D(x)$R=R−D(x)$ 直至得到 R≤0$R≤0$ ，选取此时的点作为下一个中心点。</li><li>重复 2,3 步骤，直到 K$K$ 个聚类中心都被确定。</li><li>对 K$K$ 个初始化的聚类中心，利用 K-Means 算法计算最终的聚类中心。</li></ol><p>看完整个算法流程，可能会出现一个疑问：为避免初始点距离太近，直接选取距离最远的点不就好了，为什么要引入一个随机值 R$R$ 呢？</p><p>其实当采用直接选取距离最远的点作为初始点的方法，会容易受到数据集中离群点的干扰。采用引入随机值 R$R$ 的方法避免数据集中所包含的离群点对算法思想中要选择相距最远的中心点的目标干扰。</p><p>相对于正常的数据点，离群点所计算得出的 D(x)$D(x)$ 距离一定比较大，这样在选取的过程中，它被选中的概率也就相对较大，但是离群点在整个数据集中只占一小部分，大部分依然是正常的点，这样离群点由于距离大而造成的概率大，就被正常点的数量大给平衡掉。从而保证了整个算法的平衡性。</p><h3 id="K-Means-算法实现"><a href="#K-Means-算法实现" class="headerlink" title="K-Means++ 算法实现"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="K-Means++ 算法实现"></a>K-Means++ 算法实现</h3><p>K-Means++ 在初始化样本点之后，计算其他样本与其最近的中心点距离之和，以备下一个中心点的选择，下面用 Python 来进行实现：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def get\_sum\_dis(centers, data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    centers -- 中心点集合  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    np.sum(dis\_container) -- 样本距离最近中心点的距离之和  </span><br><span class="line">    dis\_container -- 样本距离最近中心点的距离集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    dis\_container &#x3D; np.array(\[\])  </span><br><span class="line">    for each\_data in data:  </span><br><span class="line">        distances &#x3D; np.array(\[\])  </span><br><span class="line">        for each\_center in centers:  </span><br><span class="line">            temp\_distance &#x3D; d\_euc(each\_data, each\_center)  # 计算样本和中心点的欧式距离  </span><br><span class="line">            distances &#x3D; np.append(distances, temp\_distance)  </span><br><span class="line">        lab &#x3D; np.min(distances)  </span><br><span class="line">        dis\_container &#x3D; np.append(dis\_container, lab)  </span><br><span class="line">    return np.sum(dis\_container), dis\_container  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>接下来，我们初始化中心点：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;K-Means++ 初始化中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def get\_init\_center(data, k):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    np.array(center\_container) -- 初始化中心点集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    seed &#x3D; np.random.RandomState(20)  </span><br><span class="line">    p &#x3D; seed.randint(0, len(data))  </span><br><span class="line">    first\_center &#x3D; data\[p\]  </span><br><span class="line">  </span><br><span class="line">    center\_container &#x3D; \[\]  </span><br><span class="line">    center\_container.append(first\_center)  </span><br><span class="line">  </span><br><span class="line">    for i in range(k-1):  </span><br><span class="line">        sum\_dis, dis\_con &#x3D; get\_sum\_dis(center\_container, data)  </span><br><span class="line">        r &#x3D; np.random.randint(0, sum\_dis)  </span><br><span class="line">        for j in range(len(dis\_con)):  </span><br><span class="line">            r &#x3D; r - dis\_con\[j\]  </span><br><span class="line">            if r &lt;&#x3D; 0:  </span><br><span class="line">                center\_container.append(data\[j\])  </span><br><span class="line">                break  </span><br><span class="line">            else:  </span><br><span class="line">                pass  </span><br><span class="line">  </span><br><span class="line">    return np.array(center\_container)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>实现 K-Means++ 初始化中心点函数之后，根据生成数据，得到初始化的中心点坐标。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plus\_init\_center &#x3D; get\_init\_center(blobs\_plus, 5)  </span><br><span class="line">plus\_init\_center  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[ 4.1661903 ,  0.81807492],</span><br><span class="line">       [ 8.9161603 ,  5.58757202],</span><br><span class="line">       [ 7.62699601,  2.3492678 ],</span><br><span class="line">       [-3.42049424, -9.57117787],</span><br><span class="line">       [ 3.35681598, -0.54000802]]) </span><br></pre></td></tr></table></figure><p>为了让你更清晰的看到 K-Means++ 初始化中心点的过程，我们用 <code>matplotlib</code> 进行展示。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num &#x3D; len(plus\_init\_center)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(1, num, figsize&#x3D;(25, 4))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">axes\[0\].scatter(plus\_init\_center\[0, 0\], plus\_init\_center\[0, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[0\].set\_title(&quot;first center&quot;)  </span><br><span class="line">  </span><br><span class="line">for i in range(1, num):  </span><br><span class="line">    axes\[i\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">    axes\[i\].scatter(plus\_init\_center\[:i+1, 0\],  </span><br><span class="line">                    plus\_init\_center\[:i+1, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">    axes\[i\].set\_title(&quot;step&#123;&#125;&quot;.format(i))  </span><br><span class="line">  </span><br><span class="line">axes\[-1\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">axes\[-1\].scatter(plus\_init\_center\[:, 0\], plus\_init\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[-1\].set\_title(&quot;final center&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_96_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_96_1.png" alt="Text(0.5, 1.0, &#39;final center&#39;)">Text(0.5, 1.0, ‘final center’)</a></p><p>通过上图可以看到点的变化，即除了最初随机选择点之外，之后的每一个点都是尽可能选择远一些的点。这样就很好的保证初始中心点的分散。</p><p>通过多次执行代码可以看到，使用 K-Means++ 同样可能出现两个中心点较近的情况，因此，在极端情况也可能出现局部最优的问题。但相比于 K-Means 算法的随机选取，K-Means++ 的初始化中心点会在很大程度上降低局部最优问题出现的概率。</p><p>在通过 K-Means++ 算法初始化中心点后，下面我们通过 K-Means 算法对数据进行聚类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plus\_centers, plus\_clusters &#x3D; kmeans\_cluster(blobs\_plus, plus\_init\_center, 5)  </span><br><span class="line">plus\_final\_center &#x3D; plus\_centers\[-1\]  </span><br><span class="line">plus\_final\_cluster &#x3D; plus\_clusters\[-1\]  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;plus\_final\_cluster)  </span><br><span class="line">plt.scatter(plus\_final\_center\[:, 0\], plus\_final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_100_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_100_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb3e3630&gt;"></a></p><p>在 K-Means++ 算法中，我们依旧无法完全避免随机选择中心点带来的不稳定性，所以偶尔也会得到不太好的结果。当然，K-Means++ 算法得到不太好的聚类的概率远小于 K-Means 算法。所以，如果你并没有得到一个较好的聚类效果，可以再次初始化中心点尝试。</p><h2 id="Mini-Batch-K-Means-聚类算法"><a href="#Mini-Batch-K-Means-聚类算法" class="headerlink" title="Mini-Batch K-Means 聚类算法"></a><a href="#Mini-Batch-K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95" title="Mini-Batch K-Means 聚类算法"></a>Mini-Batch K-Means 聚类算法</h2><p>在「大数据」如此火的时代，K-Means 算法是否还能一如既往优秀的处理大数据呢？现在我们重新回顾下 K-Means 的算法原理：首先，计算每一个样本同所有中心点的距离，通过比较找到最近的中心点，将距离最近中心点的距离进行存储并归类。然后通过相同类别样本的特征值，更新中心点的位置。至此完成一次迭代，经过多次迭代后最终进行聚类。</p><p>通过上面的表述，你是否感觉到不断计算距离的过程，涉及到的计算量有多大呢？那么，设想一下数据量达到十万，百万，千万级别，且如果每一条数据有上百个特征，这将会消耗大量的计算资源。</p><p>为了解决大规模数据的聚类问题，我们就可以使用 K-Means 的另外一个变种 Mini Batch K-Means 来完成。</p><p>其算法原理也十分简单：在每一次迭代过程中，从数据集中随机抽取一部分数据形成小批量数据集，用该部分数据集进行距离计算和中心点的更新。由于每一次都是随机抽取，所以每一次抽取的数据能很好的表现原本数据集的特性。</p><p>下面，我们生成一组测试数据，并测试 K-Means 算法和 Mini Batch K-Means 在同一组数据上聚类时间和 SSE 上的差异。由于 scikit-learn 中 <code>MiniBatchKMeans()</code> 和 <code>KMeans()</code> 方法的参数几乎一致，这里就不再赘述了。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import time  </span><br><span class="line">from sklearn.cluster import MiniBatchKMeans, KMeans  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">test\_data, \_ &#x3D; make\_blobs(2000, n\_features&#x3D;2, cluster\_std&#x3D;2, centers&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">km &#x3D; KMeans(n\_clusters&#x3D;5)  </span><br><span class="line">mini\_km &#x3D; MiniBatchKMeans(n\_clusters&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(nrows&#x3D;1, ncols&#x3D;2, figsize&#x3D;(10, 5))  </span><br><span class="line">  </span><br><span class="line">for i, model in enumerate(\[km, mini\_km\]):  </span><br><span class="line">    t0 &#x3D; time.time()  </span><br><span class="line">    model.fit(test\_data)  </span><br><span class="line">    t1 &#x3D; time.time()  </span><br><span class="line">    t &#x3D; t1 - t0  </span><br><span class="line">    sse &#x3D; model.inertia\_  </span><br><span class="line">    axes\[i\].scatter(test\_data\[:, 0\], test\_data\[:, 1\], c&#x3D;model.labels\_)  </span><br><span class="line">    axes\[i\].set\_xlabel(&quot;time: &#123;:.4f&#125; s&quot;.format(t))  </span><br><span class="line">    axes\[i\].set\_ylabel(&quot;SSE: &#123;:.4f&#125;&quot;.format(sse))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].set\_title(&quot;K-Means&quot;)  </span><br><span class="line">axes\[1\].set\_title(&quot;Mini Batch K-Means&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_106_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_106_1.png" alt="Text(0.5, 1.0, &#39;Mini Batch K-Means&#39;)">Text(0.5, 1.0, ‘Mini Batch K-Means’)</a></p><p>以上是对 2000 条数据分别用 K-Means 和 Mini Batch K-Means进行聚类，从图像中可以看出，Mini Batch K-Means 在训练时间上明显比 K-Means 快（大于 2 倍不等），且聚类得到的 SSE 值比较接近。</p><hr><p><strong>拓展阅读：</strong></p><ul><li><a href="https://zh.wikipedia.org/zh-hans/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95">K-平均算法- 维基百科</a></li><li><a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">The 5 Clustering Algorithms Data Scientists Need to Know</a></li><li><a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">Visualizing K-Means Clustering</a></li></ul><p># <a href="/tags/cluster/">cluster</a>, <a href="/tags/k-means/">k-means</a>, <a href="/tags/machine-learning/">machine learning</a>, <a href="/tags/python/">python</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。&lt;/p&gt;
&lt;p&gt;在划分的过程中，首先由用户确定划分子集的个数 k$k$，然后随机选定 k$k$ 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 k$k$ 个子集，即将数据划分为 k$k$ 类。&lt;/p&gt;
&lt;p&gt;而评估划分的好坏标准就是：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Cluster" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Cluster/"/>
    
    <category term="K-Means" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Cluster/K-Means/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="K-Means" scheme="http://www.laugh12321.cn/blog/tags/K-Means/"/>
    
    <category term="Cluster" scheme="http://www.laugh12321.cn/blog/tags/Cluster/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|装袋和提升方法详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/07/bagging_and_boosting/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/07/bagging_and_boosting/</id>
    <published>2019-02-07T00:00:00.000Z</published>
    <updated>2020-10-22T07:59:27.842Z</updated>
    
    <content type="html"><![CDATA[<p>前面的文章都是独立的讲解每一个分类器的分类过程，每一个分类器都有其独有的特点并非常适合某些数据。但在实际中，可能没有那样合适的数据，在应用前面的分类器时，可能会出现分类准确率低的问题。为解决这样的问题，集成学习便被提出，利用多个弱分类器结合的方式使得分类准确率提高。本次详细讲解了集成学习中十分经典的几个算法：装袋（Bagging）中的 Bagging tree 和随机森林（Ramdom Forest）以及 提升（Boosting）中的 Adaboost 和梯度提升树（GBDT）。</p><a id="more"></a><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="集成学习概念"><a href="#集成学习概念" class="headerlink" title="集成学习概念"></a>集成学习概念</h3><p>在学习装袋和提升算法之前，先引入一个概念：集成学习。集成学习，顾名思义就是通过构建多个分类器并结合使用来完成学习任务，同时也被称为多分类器系统。其最大的特点就是结合各个弱分类器的长处，从而达到“三个臭皮匠顶个诸葛亮”的效果。</p><p>每一个弱分类器我们将其称作「个体学习器」，集成学习的基本结构就是生成一组个体学习器，再用某种策略将他们结合起来。</p><p>从个体学习器类别来看，集成学习通常分为两种类型：</p><ul><li>「同质」集成，在一个集成学习中，「个体学习器」是同一类型，如 「决策树集成」 所有个体学习器都为决策树，「神经网络集成」所有的个体学习器都为神经网络。</li><li>「异质」集成，在一个集成学习中，「个体学习器」为不同类型，如一个集成学习中可以包含决策树模型也可以包含神经网络模型。</li></ul><p>同样从集成方式来看，集成学习也可以分为两类：</p><ul><li>并行式，当个体学习器之间不存在强依赖关系时，可同时生成并行化方法，其中代表算法为装袋（Bagging）算法。</li><li>串行式，当个体学习器之间存在强依赖关系时，必须串行生成序列化方法，其中代表算法为提升（Boosting）算法。</li></ul><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/1.png"></center><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p>集成学习中，当数据被多个个体学习器学习后，如何最终决定学习结果呢？假定集成包含 T$T$ 个个体学习器 {h1,h2,…,hT}${h1,h2,…,hT}$ 。常用的有三种方法：平均法，投票法，学习法。</p><h4 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a><a href="#%E5%B9%B3%E5%9D%87%E6%B3%95" title="平均法"></a>平均法</h4><p>在数值型输出中，最常用的结合策略为平均法（Averaging），在平均法中有两种方式：</p><p><strong>简单平均法：</strong></p><p>H(x)=1TT∑i=1hi(x)(1)</p><p>$$(1)H(x)=1T∑i=1Thi(x)$$</p><p>取每一个「个体学习器」学习后的平均值。</p><p><strong>加权平均法：</strong></p><p>H(x)=T∑i=1wihi(x)(2)</p><p>$$(2)H(x)=∑i=1Twihi(x)$$</p><p>其中 wi$wi$ 是每一个「个体学习器」 hi$hi$ 的权重，通常为 wi≥0,∑Ti=1wi=1$wi≥0,∑i=1Twi=1$。</p><h4 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a><a href="#%E6%8A%95%E7%A5%A8%E6%B3%95" title="投票法"></a>投票法</h4><p>对于分类输出而言，平均法显然效果不太好，最常用的结合策略为投票法(Voting)，在投票法中主要有三种方式：</p><p><strong>绝对多数投票法：</strong></p><p>H(X)={cj,if∑Ti=1hji&gt;0.5∑Nk=1∑Ti=1hki(x);None,if∑Ti=1hji&lt;=0.5∑Nk=1∑Ti=1hki(x);(3)</p><p>$$(3)H(X)={cj,if∑i=1Thij&gt;0.5∑k=1N∑i=1Thik(x);None,if∑i=1Thij&lt;=0.5∑k=1N∑i=1Thik(x);$$</p><p>简单而言，当某一个输出的分类超过了半数则输出该分类，若未超过半数则不输出分类。</p><p><strong>相对多数投票法：</strong></p><p>H(X)=cargmaxj∑Ti=1hji(x)(4)</p><p>$$(4)H(X)=cargmaxj∑i=1Thij(x)$$</p><p>即在「个体学习器」分类完成后，通过投票选出分类最多的标签作为此次分类的结果。</p><p><strong>加权投票法：</strong></p><p>H(X)=cargmaxj∑Ti=1wihji(x)(5)</p><p>$$(5)H(X)=cargmaxj∑i=1Twihij(x)$$</p><p>同加权平均法类似，wi$wi$ 是每一个「个体学习器」 hi$hi$ 的权重，通常为 wi≥0,∑Ti=1wi=1$wi≥0,∑i=1Twi=1$。</p><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a><a href="#%E5%AD%A6%E4%B9%A0%E6%B3%95" title="学习法"></a>学习法</h4><p>以上两种方法（平均法和投票法）相对比较简单，但是可能学习误差较大，为了解决这种情况，还有一种方法为学习法，其代表方法是 <code>stacking</code> ，当使用<code>stacking</code> 的结合策略时， 不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，即把训练集弱学习器的学习结果作为输入，把训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p><p>在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。</p><h2 id="装袋算法-Bagging"><a href="#装袋算法-Bagging" class="headerlink" title="装袋算法 Bagging"></a><a href="#%E8%A3%85%E8%A2%8B%E7%AE%97%E6%B3%95-Bagging" title="装袋算法 Bagging"></a>装袋算法 Bagging</h2><p>在大致了解集成学习相关概念之后，接下来就是对集成学习中常用算法思想之一的装袋算法进行详细的讲解。</p><h3 id="装袋算法原理"><a href="#装袋算法原理" class="headerlink" title="装袋算法原理"></a><a href="#%E8%A3%85%E8%A2%8B%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" title="装袋算法原理"></a>装袋算法原理</h3><p>装袋算法是并行式集成学习的代表，其原理也比较简单。算法步骤如下：</p><ol><li>数据处理：将数据根据实际情况进行清洗整理。</li><li>随机采样：重复 T 次，每一次从样本中随机选出 T 个子样本。</li><li>个体训练：将每一个子样本放入个体学习器训练。</li><li>分类决策：用投票法集成进行分类决策。</li></ol><h3 id="Bagging-tree"><a href="#Bagging-tree" class="headerlink" title="Bagging tree"></a><a href="#Bagging-tree" title="Bagging tree"></a>Bagging tree</h3><p>在前一节的决策树讲解中提到，决策树是一个十分「完美」的训练器，但特别容易出现过拟合的情况，最终导致预测准确率低的问题。事实上在装袋算法中，决策树常常被用作弱分类器。下面我们通过具体实验来看看决策树和以决策树作为装袋算法的预测效果。</p><h4 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a><a href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD" title="数据加载"></a>数据加载</h4><p>本实验我们使用在上一章讲决策树时所用的学生成绩预测数据集。其中数据处理已在上一章详细说明，本次实验我们使用处理过后的数据集。数据集名称为 <code>course-14-student.csv</code>.</p><p>数据集下载 👉 <a href="http://labfile.oss.aliyuncs.com/courses/1081/course-14-student.csv">传送门</a></p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd  </span><br><span class="line">  </span><br><span class="line">stu\_data &#x3D; pd.read\_csv(&quot;course-14-student.csv&quot;)  </span><br><span class="line">stu\_data.head(10)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/output_00.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/output_00.png"></a></p><h4 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a><a href="#%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86" title="数据划分"></a>数据划分</h4><p>加载好预处理的数据集之后，为了应用装袋算法，我们需要将数据集分为 <strong>训练集</strong>和<strong>测试集</strong>，依照经验：<strong>训练集</strong>占比为 70%，<strong>测试集</strong>占 30%。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model\_selection import train\_test\_split  </span><br><span class="line">  </span><br><span class="line">x\_train, x\_test, y\_train, y\_test &#x3D; train\_test\_split(stu\_data.iloc\[:,:-1\], stu\_data\[&quot;G3&quot;\],  </span><br><span class="line">                                                    test\_size&#x3D;0.3, random\_state&#x3D;35)  </span><br><span class="line">x\_test.head(10)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/output_01.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/output_01.png"></a></p><h4 id="决策树预测"><a href="#决策树预测" class="headerlink" title="决策树预测"></a><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E9%A2%84%E6%B5%8B" title="决策树预测"></a>决策树预测</h4><p>作为比较，首先我们将该数据集用决策树的方式进行预测,使用 <code>scikit-learn</code> 实现决策树预测的用法在前一章节已详细介绍，本实验直接使用。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier  </span><br><span class="line">  </span><br><span class="line">dt\_model &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;entropy&#39;, random\_state&#x3D;34)  </span><br><span class="line">dt\_model.fit(x\_train, y\_train)  # 使用训练集训练模型  </span><br><span class="line">dt\_y\_predict &#x3D; dt\_model.predict(x\_test)  </span><br><span class="line">dt\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([2, 0, 3, 2, 1, 2, 2, 2, 3, 3, 0, 2, 1, 3, 3, 2, 3, 0, 1, 2, 1, 2,</span><br><span class="line">       1, 2, 3, 3, 3, 0, 3, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 2,</span><br><span class="line">       1, 3, 2, 3, 3, 3, 3, 1, 1, 3, 2, 0, 1, 3, 2, 3, 3, 0, 0, 2, 2, 3,</span><br><span class="line">       3, 3, 2, 1, 0, 3, 2, 2, 3, 2, 1, 3, 0, 2, 3, 1, 3, 3, 0, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,</span><br><span class="line">       2, 1, 1, 1, 3, 0, 3, 3, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><p>计算使用决策树预测的准确率。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_accuracy(test\_labels, pred\_labels):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    test\_labels -- 测试集的真实值  </span><br><span class="line">    pred\_labels -- 测试集的预测值  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    accur -- 准确率  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    correct &#x3D; np.sum(test\_labels &#x3D;&#x3D; pred\_labels)  # 计算预测正确的数据个数  </span><br><span class="line">    n &#x3D; len(test\_labels)  # 总测试集数据个数  </span><br><span class="line">    accur &#x3D; correct&#x2F;n  </span><br><span class="line">    return accur  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">get\_accuracy(y\_test, dt\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7899159663865546 </span><br></pre></td></tr></table></figure><p>由于本次实验所采用的数据集特征值比前一章节多，所以决策树泛化能力更差。</p><h4 id="Bagging-Tree-数据模型构建"><a href="#Bagging-Tree-数据模型构建" class="headerlink" title="Bagging Tree 数据模型构建"></a><a href="#Bagging-Tree-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA" title="Bagging Tree 数据模型构建"></a>Bagging Tree 数据模型构建</h4><p>单棵决策树的预测结果并不能使我们满意，下面我们使用 <strong>装袋（Bagging）</strong> 的思想来提高预测准确率。我们通过 <code>scikit-learn</code> 来对 Bagging Tree 算法进行实现。</p><p>在 <code>scikit-learn</code> 中 Bagging tree 常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BaggingClassifier(base\_estimator&#x3D;None, n\_estimators&#x3D;10, max\_samples&#x3D;1.0, max\_features&#x3D;1.0)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>base_estimator</code>：表示基础分类器（弱分类器）种类，默认为决策树 。</li><li><code>n_estimators</code>：表示建立树的个数，默认值为 10 。</li><li><code>max_samples</code>：表示从抽取数据中选取训练样本的数量，int（整型）表示数量，float（浮点型）表示比例，默认为所有样本。</li><li><code>max_features</code>：表示抽取特征的数量，int（整型）表示数量，float（浮点型）表示比例，默认为所有特征。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>：训练。</li><li><code>predict(X)</code>：对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import BaggingClassifier  </span><br><span class="line">  </span><br><span class="line">tree &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;entropy&#39;, random\_state&#x3D;28)  </span><br><span class="line">bag &#x3D; BaggingClassifier(tree, n\_estimators&#x3D;100,  </span><br><span class="line">                        max\_samples&#x3D;1.0, random\_state&#x3D;3)  # 使用决策树  </span><br><span class="line">  </span><br><span class="line">bag.fit(x\_train, y\_train)  </span><br><span class="line">bt\_y\_predict &#x3D; bag.predict(x\_test)  </span><br><span class="line">bt\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 0, 3, 2, 1, 2, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2,</span><br><span class="line">       1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 0, 2, 1, 3, 3,</span><br><span class="line">       1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 0, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3,</span><br><span class="line">       2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 3, 1, 3, 3, 0, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,</span><br><span class="line">       3, 1, 1, 1, 3, 0, 3, 3, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h4 id="准确率计算"><a href="#准确率计算" class="headerlink" title="准确率计算"></a><a href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97" title="准确率计算"></a>准确率计算</h4><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_accuracy(y\_test,bt\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8991596638655462 </span><br></pre></td></tr></table></figure><p>根据准确率可以看到在决策树通过装袋（Bagging）算法后预测准确率有明显提升。</p><h3 id="随机森林-Random-Forest"><a href="#随机森林-Random-Forest" class="headerlink" title="随机森林 Random Forest"></a><a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-Random-Forest" title="随机森林 Random Forest"></a>随机森林 Random Forest</h3><p>其实，Bagging tree 算法，是应用子数据集中的所有特征构建一棵完整的树，最终通过投票的方式进行预测。而随机森林就是在 Bagging tree 算法的基础上进行进一步的改进。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/2.png"></a></p><p>随机森林的思想就是将一个大的数据集使用<strong>自助采样法</strong>进行处理，即从原样本数据集中随机抽取多个子样本集，并基于每一个子样本集生成相应的决策树。这样，就可以构建出由许多小决策树组形成的决策树「森林」。最后，实验通过<strong>投票法</strong>选择决策树最多的预测结果作为最终的输出。</p><p>所以，随机森林的名称来源就是「随机抽样 + 决策树森林」。</p><h4 id="随机森林算法原理"><a href="#随机森林算法原理" class="headerlink" title="随机森林算法原理"></a><a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" title="随机森林算法原理"></a>随机森林算法原理</h4><p>随机森林作为装袋（Bagging）的代表算法，算法原理和装袋十分相似，但在此基础上做了一些改进：</p><ol><li><p>对于普通的决策树，会在 N 个样本的所有特征中选择一个最优划分特征，但是随机森林首先会从所有特征中随机选择部分特征，再从该部分特征中选择一个最优划分特征。这样进一步增强了模型的泛化能力。</p></li><li><p>在决定部分特征个数时，通过交叉验证的方式来获取一个合适的值。</p></li></ol><p>随机森林算法流程：</p><ol><li>从样本集中有放回随机采样选出 <code>n</code> 个样本；</li><li>从所有特征中随机选择 <code>k</code> 个特征，对选出的样本利用这些特征建立决策树；</li><li>重复以上两步 <code>m</code> 次，即生成 <code>m</code> 棵决策树，形成随机森林；</li><li>对于新数据，经过每棵树决策，最后投票确认分到哪一类。</li></ol><h4 id="模型构建和数据预测"><a href="#模型构建和数据预测" class="headerlink" title="模型构建和数据预测"></a><a href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E5%92%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E6%B5%8B" title="模型构建和数据预测"></a>模型构建和数据预测</h4><p>在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 <code>scikit-learn</code> 来对其进行实现。</p><p>在 <code>scikit-learn</code> 随机森林常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RandomForestClassifier(n\_estimators,criterion,max\_features,random\_state&#x3D;None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>n_estimators</code>：表示建立树的个数，默认值为 10 。</li><li><code>criterion</code>：表示特征划分方法选择，默认为 <code>gini</code>，可选择为 <code>entropy</code> (信息增益)。</li><li><code>max_features</code>：表示随机选择特征个数，默认为特征数的根号。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>：训练随机森林。</li><li><code>predict(X)</code>：对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier  </span><br><span class="line">  </span><br><span class="line">\# 这里构建 100 棵决策树，采用信息熵来寻找最优划分特征。  </span><br><span class="line">rf &#x3D; RandomForestClassifier(  </span><br><span class="line">    n\_estimators&#x3D;100, max\_features&#x3D;None, criterion&#x3D;&#39;entropy&#39;)  </span><br><span class="line">rf.fit(x\_train, y\_train)  # 进行模型的训练  </span><br><span class="line">rf\_y\_predict &#x3D; rf.predict(x\_test)  </span><br><span class="line">rf\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 0, 3, 2, 1, 2, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2,</span><br><span class="line">       1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 3,</span><br><span class="line">       1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 2, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3,</span><br><span class="line">       2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 0, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,</span><br><span class="line">       3, 1, 1, 1, 3, 0, 3, 3, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h4 id="准确率计算-1"><a href="#准确率计算-1" class="headerlink" title="准确率计算"></a><a href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97-1" title="准确率计算"></a>准确率计算</h4><p>当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_accuracy(y\_test, rf\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8907563025210085 </span><br></pre></td></tr></table></figure><p>可以通过结果看到，本次实验的数据集用随机森林预测的准确率和用 Bagging tree 预测的准确率差别不大，但随着数据集的增大和特征数的增多，随机森林的优势就会慢慢显现出来。</p><h2 id="提升算法-Boosting"><a href="#提升算法-Boosting" class="headerlink" title="提升算法 Boosting"></a><a href="#%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95-Boosting" title="提升算法 Boosting"></a>提升算法 Boosting</h2><p>当「个体学习器」之间存在较强的依赖时，采用装袋的算法便有些不合适，此时最好的方法就是使用串行集成方式：提升（Boosting）。</p><h3 id="提升算法原理"><a href="#提升算法原理" class="headerlink" title="提升算法原理"></a><a href="#%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" title="提升算法原理"></a>提升算法原理</h3><p>提升算法是可以将弱学习器提升为强学习器的算法，其具体思想是从初始训练集训练出一个「个体学习器」，再根据个体学习器的表现对训练样本分布进行调整，使得在个体学习器中判断错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个「个体学习器」。如此重复进行，直至个体学习器数目达到事先指定的值 <code>T</code>，最终将这 <code>T</code> 个「个体学习器」输出的值进行加权结合得到最终的输出值。</p><h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a><a href="#Adaboost" title="Adaboost"></a>Adaboost</h3><p>提升（Boosting）算法中最具代表性的算法为 Adaboost。</p><p>AdaBoost（Adaptive Boosting）名为自适应增强，其主要自适应增强表现在：上一个「个体学习器」中被错误分类的样本的权值会增大，正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/3.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/3.png"></a></p><h4 id="AdaBoost-原理"><a href="#AdaBoost-原理" class="headerlink" title="AdaBoost 原理"></a><a href="#AdaBoost-%E5%8E%9F%E7%90%86" title="AdaBoost 原理"></a>AdaBoost 原理</h4><p>AdaBoost 算法与 Boosting 算法不同的是，其不需要预先知道弱分类器的误差，并且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度。</p><p>Adaboost 算法流程：</p><ol><li>数据准备：通过数据清理和数据整理的方式得到符合规范的数据。</li><li>初始化权重：如果有 <code>N</code> 个训练样本数据，在最开始时每一个数据被赋予相同的权值：<code>1/N</code>。</li><li>弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。</li><li>更改权重：如果某个样本点被准确地分类，降低其权值；若被分类错误，那么提高其权值。然后，权值更新过的样本集被用于训练下一个分类器。</li><li>强分类器组合：重复 <code>3，4</code> 步骤，直至训练结束，加大分类误差率小的弱分类器的权重（这里的权重和样本权重不一样），使其在最终的分类函数中起着较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用，最终输出结果。</li></ol><h4 id="模型构建和数据预测-1"><a href="#模型构建和数据预测-1" class="headerlink" title="模型构建和数据预测"></a><a href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E5%92%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E6%B5%8B-1" title="模型构建和数据预测"></a>模型构建和数据预测</h4><p>在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 <code>scikit-learn</code> 来对其进行实现。</p><p>在 <code>scikit-learn</code> Adaboost 常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AdaBoostClassifier(base\_estimators,n\_estimators)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>base_estimators</code>：表示弱分类器种类，默认为 CART 分类树。</li><li><code>n_estimators</code>：表示弱学习器的最大个数，默认值为 <code>50</code>。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>：训练弱分类器。</li><li><code>predict(X)</code>：对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import AdaBoostClassifier  </span><br><span class="line">  </span><br><span class="line">ad &#x3D; AdaBoostClassifier(n\_estimators&#x3D;100)  </span><br><span class="line">ad.fit(x\_train, y\_train)  </span><br><span class="line">ad\_y\_predict &#x3D; ad.predict(x\_test)  </span><br><span class="line">ad\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 3, 3, 2, 0, 3, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 3,</span><br><span class="line">       1, 3, 3, 3, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 3, 1, 3, 3,</span><br><span class="line">       1, 3, 2, 3, 2, 3, 3, 3, 1, 2, 3, 3, 1, 3, 2, 2, 3, 2, 0, 2, 2, 3,</span><br><span class="line">       2, 3, 3, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 3, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 1, 2, 0, 2, 3, 0, 3, 1, 3, 1, 0, 3, 3, 3, 3, 2, 3,</span><br><span class="line">       3, 1, 3, 1, 3, 3, 3, 1, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h4 id="准确率计算-2"><a href="#准确率计算-2" class="headerlink" title="准确率计算"></a><a href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97-2" title="准确率计算"></a>准确率计算</h4><p>当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_accuracy(y\_test, ad\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7983193277310925 </span><br></pre></td></tr></table></figure><p>通过结果可以看到，应用 Adaboost 算法得到的准确率和决策树相差不大，说明在使用 Adaboost 算法时预测效果不好。</p><h3 id="梯度提升树-GBDT"><a href="#梯度提升树-GBDT" class="headerlink" title="梯度提升树 GBDT"></a><a href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-GBDT" title="梯度提升树 GBDT"></a>梯度提升树 GBDT</h3><p>梯度提升树（Gradient Boosting Decison Tree，GBDT）同样是 Boosting 算法家族中的一员， Adaboost 是利用前一轮迭代弱学习器的误差率来更新训练集的权重，而梯度提升树所采用的是前向分布算法，且弱学习器限定了只能使用CART树模型。</p><h4 id="梯度提升树算法原理"><a href="#梯度提升树算法原理" class="headerlink" title="梯度提升树算法原理"></a><a href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" title="梯度提升树算法原理"></a>梯度提升树算法原理</h4><p>在 GBDT 的迭代中，假设我们前一轮迭代得到的强学习器是 ft−1(x)$ft−1(x)$, 损失函数是 L(y,ft−1(x))$L(y,ft−1(x))$, 我们本轮迭代的目标是找到一个 CART 回归树模型的弱学习器 ht(x)$ht(x)$，让本轮的损失 L(y,ft(x)=L(y,ft−1(x)+ht(x))$L(y,ft(x)=L(y,ft−1(x)+ht(x))$ 最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p><p>算法流程：</p><ol><li>数据准备：通过数据清理和数据整理的方式得到符合规范的数据。</li><li>初始化权重：如果有 <code>N</code> 个训练样本数据，在最开始时每一个数据被赋予相同的权值：<code>1/N</code>。</li><li>弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。</li><li>CART 树拟合：计算每一个子样本的梯度值，通过梯度值和子样本拟合一棵 CART 树</li><li>更新强学习器：在拟合好的 CART树中通过损失函数计算出最佳的拟合值，更新先前组成的强学习器。</li><li>强分类器组合：重复 <code>3，4，5</code> 步骤，直至训练结束，得到一个强分类器，最终输出结果。</li></ol><h4 id="梯度提升树模型构建及预测"><a href="#梯度提升树模型构建及预测" class="headerlink" title="梯度提升树模型构建及预测"></a><a href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E5%8F%8A%E9%A2%84%E6%B5%8B" title="梯度提升树模型构建及预测"></a>梯度提升树模型构建及预测</h4><p>在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 <code>scikit-learn</code> 来对其进行实现。</p><p>在 <code>scikit-learn</code> GBDT 常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GradientBoostingClassifier(max\_depth &#x3D; 3，learning\_rate &#x3D; 0.1, n\_estimators &#x3D; 100，random\_state &#x3D; None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>max_depth</code>:表示生成 CART 树的最大深度，默认为 3</li><li><code>learning_rate</code>:表示学习效率，默认为 0.1。</li><li><code>n_estimators</code>：表示弱学习器的最大个数，默认值为 100。</li><li><code>random_state</code>:表示随机数种子。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>：训练弱分类器。</li><li><code>predict(X)</code>：对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import GradientBoostingClassifier  </span><br><span class="line">  </span><br><span class="line">clf &#x3D; GradientBoostingClassifier(  </span><br><span class="line">    n\_estimators&#x3D;100, learning\_rate&#x3D;1.0, random\_state&#x3D;33)  </span><br><span class="line">clf.fit(x\_train, y\_train)  </span><br><span class="line">gt\_y\_predict &#x3D; clf.predict(x\_test)  </span><br><span class="line">gt\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 0, 3, 2, 1, 3, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2,</span><br><span class="line">       1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 3,</span><br><span class="line">       1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3,</span><br><span class="line">       2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 0, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 3, 2, 0, 0, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,</span><br><span class="line">       3, 1, 1, 1, 3, 2, 3, 3, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h4 id="准确率计算-3"><a href="#准确率计算-3" class="headerlink" title="准确率计算"></a><a href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97-3" title="准确率计算"></a>准确率计算</h4><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_accuracy(y\_test, gt\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8739495798319328 </span><br></pre></td></tr></table></figure><p>可以看到，在使用装袋和提升算法时，在大部分情况下，会产生更好的预测结果，但有时也可能出现没有优化的情况。事实上，机器学习分类器的选择就是如此，没有最好的分类器只有最适合的分类器，不同的数据集，由于其数据特点的不同，在不同的分类器中表现也不同。</p><p><strong>拓展阅读：</strong></p><ul><li><a href="https://zh.wikipedia.org/zh-hans/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97">随机森林 - 维基百科</a></li><li><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">Bootstrap aggregating - 维基百科</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;前面的文章都是独立的讲解每一个分类器的分类过程，每一个分类器都有其独有的特点并非常适合某些数据。但在实际中，可能没有那样合适的数据，在应用前面的分类器时，可能会出现分类准确率低的问题。为解决这样的问题，集成学习便被提出，利用多个弱分类器结合的方式使得分类准确率提高。本次详细讲解了集成学习中十分经典的几个算法：装袋（Bagging）中的 Bagging tree 和随机森林（Ramdom Forest）以及 提升（Boosting）中的 Adaboost 和梯度提升树（GBDT）。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Bagging and BoosTing" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Bagging-and-BoosTing/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Bagging" scheme="http://www.laugh12321.cn/blog/tags/Bagging/"/>
    
    <category term="BoosTing" scheme="http://www.laugh12321.cn/blog/tags/BoosTing/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|决策树详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/01/decision_tree/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/01/decision_tree/</id>
    <published>2019-02-01T00:00:00.000Z</published>
    <updated>2020-10-21T10:18:01.341Z</updated>
    
    <content type="html"><![CDATA[<p>决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。  </p><a id="more"></a><p>举一个通俗的例子，假设在B站工作多年仍然单身的小B和他母亲在给他介绍对象时的一段对话：</p><blockquote><p>母亲：小B，你都 28 了还是单身，明天亲戚家要来个姑娘要不要去见见。<br>小B：多大年纪？<br>母亲：26。<br>小B：有多高？<br>母亲：165厘米。<br>小B：长的好看不。<br>母亲：还行，比较朴素。<br>小B：温柔不？<br>母亲：看起来挺温柔的，很有礼貌。<br>小B：好，去见见。</p></blockquote><p>作为程序员的小B的思考逻辑就是典型的决策树分类逻辑，将年龄，身高，长相，是否温柔作为特征，并最后对见或者不见进行决策。其决策逻辑如图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/1.png"></center><h2 id="决策树算法实现"><a href="#决策树算法实现" class="headerlink" title="决策树算法实现"></a>决策树算法实现</h2><p>其实决策树算法如同上面场景一样，其思想非常容易理解，具体的算法流程为：</p><ul><li><p><strong>第 1 步</strong>: 数据准备：通过数据清洗和数据处理，将数据整理为没有缺省值的向量。</p></li><li><p><strong>第 2 步</strong>: 寻找最佳特征：遍历每个特征的每一种划分方式，找到最好的划分特征。</p></li><li><p><strong>第 3 步</strong>: 生成分支：划分成两个或多个节点。</p></li><li><p><strong>第 4 步</strong>: 生成决策树：对分裂后的节点分别继续执行2-3步，直到每个节点只有一种类别。</p></li><li><p><strong>第 5 步</strong>: 决策分类：根据训练决策树模型，将预测数据进行分类。</p></li></ul><h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><p>下面我们依照决策树的算法流程，用 <code>python</code> 来实现决策树构建和分类。首先生成一组数据，数据包含两个类别 <code>man</code> 和 <code>woman</code>,特征分别为:</p><ul><li><code>hair</code>:头发长短(<code>long</code>:长,<code>short</code>:短)</li><li><code>voice</code>:声音粗细(<code>thick</code>:粗,<code>thin</code>:细)</li><li><code>height</code>:身高</li><li><code>ear_stud</code>:是否带有耳钉(<code>yes</code>:是,<code>no</code>:没有)</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;生成示例数据  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import numpy as np  </span><br><span class="line">import pandas as pd  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def create\_data():  </span><br><span class="line">    data\_value &#x3D; np.array(  </span><br><span class="line">        \[\[&#39;long&#39;, &#39;thick&#39;, 175, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 168, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 178, &#39;yes&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thick&#39;, 172, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;medium&#39;, 163, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thick&#39;, 180, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 173, &#39;yes&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 174, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 164, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;medium&#39;, 158, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 161, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 166, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 158, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 163, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 161, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 164, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 172, &#39;yes&#39;, &#39;woman&#39;\]\])  </span><br><span class="line">    columns &#x3D; np.array(\[&#39;hair&#39;, &#39;voice&#39;, &#39;height&#39;, &#39;ear\_stud&#39;, &#39;labels&#39;\])  </span><br><span class="line">    data &#x3D; pd.DataFrame(data\_value.reshape(17, 5), columns&#x3D;columns)  </span><br><span class="line">    return data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>在创建好数据之后，加载并打印出这些数据</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; create\_data()  </span><br><span class="line">data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a><a href="#%E5%88%92%E5%88%86%E9%80%89%E6%8B%A9" title="划分选择"></a>划分选择</h3><p>在得到数据后，根据算法流程，接下来需要寻找最优的划分特征，随着划分的不断进行，我们尽可能的将划分的分支所包含的样本归于同一类别，即结点的“纯度”越来越高。而常用的特征划分方式为信息增益和增益率。</p><h4 id="信息增益（ID3）"><a href="#信息增益（ID3）" class="headerlink" title="信息增益（ID3）"></a><a href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%EF%BC%88ID3%EF%BC%89" title="信息增益（ID3）"></a>信息增益（ID3）</h4><p>在介绍信息增益之前，先引入“信息熵”的概念。“信息熵”是度量样本纯度最常用的一种指标，其公式为：</p><p>Ent(D)=−|y|∑k=1pklog2pk(1)</p><p>$$(1)Ent(D)=−∑k=1|y|pklog2pk$$</p><p>其中 D$D$ 表示样本集合，pk$pk$ 表示第 k$k$ 类样本所占的比例。其中 Ent(D)$Ent(D)$ 的值越小，则 D$D$ 的纯度越高。根据以上数据，在计算数据集的“信息熵”时，|y|$|y|$ 显然只有 <code>man</code>,<code>woman</code> 共 2 种，其中为 <code>man</code> 的概率为 817$817$, <code>woman</code> 的概率为 917$917$,则根据公式(1)得到数据集的纯度为：</p><p>Ent(data)=−2∑k=1pklog2pk=−(817log2817+917log2917)=0.9975</p><p>$$Ent(data)=−∑k=12pklog2pk=−(817log2817+917log2917)=0.9975$$</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算信息熵  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import math  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_Ent(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    Ent -- 信息熵  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    num\_sample &#x3D; len(data)  # 样本个数  </span><br><span class="line">    label\_counts &#x3D; &#123;&#125;  # 初始化标签统计字典  </span><br><span class="line">    for i in range(num\_sample):  </span><br><span class="line">        each\_data &#x3D; data.iloc\[i, :\]  </span><br><span class="line">        current\_label &#x3D; each\_data\[&quot;labels&quot;\]  # 得到当前元素的标签（label）  </span><br><span class="line">  </span><br><span class="line">        # 如果标签不在当前字典中，添加该类标签并初始化 value&#x3D;0,否则该类标签 value+1  </span><br><span class="line">        if current\_label not in label\_counts.keys():  </span><br><span class="line">            label\_counts\[current\_label\] &#x3D; 0  </span><br><span class="line">        label\_counts\[current\_label\] +&#x3D; 1  </span><br><span class="line">      </span><br><span class="line">    Ent &#x3D; 0.0  # 初始化信息熵  </span><br><span class="line">    for key in label\_counts:  </span><br><span class="line">        prob &#x3D; float(label\_counts\[key\])&#x2F;num\_sample  </span><br><span class="line">        Ent -&#x3D; prob \* math.log(prob, 2)  # 应用信息熵公式计算信息熵  </span><br><span class="line">    return Ent  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>通过计算信息熵函数，计算根节点的信息熵：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">base\_ent &#x3D; get\_Ent(data)  </span><br><span class="line">base\_ent  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9975025463691153 </span><br></pre></td></tr></table></figure><p><strong>信息增益</strong> 就是建立在信息熵的基础上，在离散特征 x$x$ 有 M$M$ 个取值，如果用 x$x$ 对样本 D$D$ 来进行划分，就会产生 M$M$ 个分支，其中第 m$m$ 个分支包含了集合 D$D$ 的所有在特征 x$x$ 上取值为 m$m$ 的样本，记为 Dm$Dm$（例如：根据以上生成数据，如果我们用 <code>hair</code> 进行划分，则会产生<code>long</code>，<code>short</code>两个分支，每一个分支中分别包含了整个集合中属于 <code>long</code> 或者 <code>short</code> 的数据）。</p><p>考虑到不同分支节点包含样本数不同，给分支赋予权重 |Dm||D|$|Dm||D|$ ,使得样本越多的分支节点影响越大，则 <strong>信息增益</strong> 的公式就可以得到：</p><p>Gain(D,x)=Ent(D)−M∑m=1|Dm||D|Ent(Dm)(2)</p><p>$$(2)Gain(D,x)=Ent(D)−∑m=1M|Dm||D|Ent(Dm)$$</p><p>一般情况下，信息增益越大，则说明用 x$x$ 来划分样本集合 D$D$ 的纯度越高。以 <code>hair</code> 为例，其中它有 <code>short</code> 和 <code>long</code> 两个可能取值，则分别用 D1$D1$ (hair = long) 和 `D^{2} (hair = short)来表示。</p><p>其中为 D1$D1$ 的数据编号为 {0，4，6，8，9，10，12，14，15}${0，4，6，8，9，10，12，14，15}$ 共 9 个，在这之中为 <code>man</code> 的有 {0，4，6} 共3 个占比为39$39$，为 <code>woman</code> 的有{8, 9，10，12，14，15}共 6 个占比为69$69$; 同样 D2$D2$ 编号为{1，2，3，5，7，11，13, 16}共 8 个，其中为 <code>man</code> 的有{1，2，3，5，7}共 5 个占比58$58$,为 <code>woman</code> 的有{11，13, 16}共 3 个占比 38$38$,若按照 <code>hair</code> 进行划分，则两个分支点的信息熵为：</p><p>Ent(D1)=−(39log239+69log269)=0.449</p><p>$$Ent(D1)=−(39log239+69log269)=0.449$$</p><p>Ent(D2)=−(58log258+38log238)=0.486</p><p>$$Ent(D2)=−(58log258+38log238)=0.486$$</p><p>根据信息增益的公式可以计算出 <code>hair</code> 的信息增益为：</p><p>Gain(D,hair)=Ent(D)−2∑m=1|Dm||D|Ent(Dm)=0.9975−(917∗0.449+817∗0.486)=0.062</p><p>$$Gain(D,hair)=Ent(D)−∑m=12|Dm||D|Ent(Dm)=0.9975−(917∗0.449+817∗0.486)=0.062$$</p><p>下面我们用 <code>python</code> 来实现<strong>信息增益（ID3）</strong>算法：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算信息增益  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_gain(data, base\_ent, feature):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    base\_ent -- 根节点的信息熵  </span><br><span class="line">    feature -- 计算信息增益的特征  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    Ent -- 信息熵  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">    feature\_list &#x3D; data\[feature\]  # 得到一个特征的全部取值  </span><br><span class="line">    unique\_value &#x3D; set(feature\_list)  # 特征取值的类别  </span><br><span class="line">    feature\_ent &#x3D; 0.0  </span><br><span class="line">  </span><br><span class="line">    for each\_feature in unique\_value:  </span><br><span class="line">        temp\_data &#x3D; data\[data\[feature\] &#x3D;&#x3D; each\_feature\]  </span><br><span class="line">        weight &#x3D; len(temp\_data)&#x2F;len(feature\_list)  # 计算该特征的权重值  </span><br><span class="line">        temp\_ent &#x3D; weight\*get\_Ent(temp\_data)  </span><br><span class="line">        feature\_ent &#x3D; feature\_ent+temp\_ent  </span><br><span class="line">  </span><br><span class="line">    gain &#x3D; base\_ent - feature\_ent  # 信息增益  </span><br><span class="line">    return gain  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>完成 <strong>信息增益</strong> 函数后，尝试计算特征 <code>hair</code> 的信息增益值。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_gain(data,base\_ent,&#39;hair&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.062200515199107964 </span><br></pre></td></tr></table></figure><h4 id="信息增益率（C4-5）"><a href="#信息增益率（C4-5）" class="headerlink" title="信息增益率（C4.5）"></a><a href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87%EF%BC%88C4-5%EF%BC%89" title="信息增益率（C4.5）"></a>信息增益率（C4.5）</h4><p>信息增益也存在许多不足之处，经过大量的实验发现，当信息增益作为标准时，易偏向于取值较多的特征，为了避免这种偏好给预测结果带来的不好影响，可以使用<strong>增益率</strong>来选择最优划分。<strong>增益率</strong>的公式定义为:</p><p>GainRatio(D,a)=Gain(D,a)IV(a)(3)</p><p>$$(3)GainRatio(D,a)=Gain(D,a)IV(a)$$</p><p>其中：</p><p>IV(a)=−M∑m=1|Dm||D|log2|Dm||D|(4)</p><p>$$(4)IV(a)=−∑m=1M|Dm||D|log2|Dm||D|$$</p><p>IV(a)$IV(a)$ 称为特征 a$a$ 的固有值，当 a$a$ 的取值数目越多，则 IV(a)$IV(a)$ 的值通常会比较大。例如：</p><p>IV(hair)=−917log2917−817log2817=0.998</p><p>$$IV(hair)=−917log2917−817log2817=0.998$$</p><p>IV(voice)=−717log2717−517log2517−517log2517=1.566</p><p>$$IV(voice)=−717log2717−517log2517−517log2517=1.566$$</p><h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a><a href="#%E8%BF%9E%E7%BB%AD%E5%80%BC%E5%A4%84%E7%90%86" title="连续值处理"></a>连续值处理</h3><p>在前面介绍的特征选择中，都是对离散型数据进行处理，但在实际的生活中数据常常会出现连续值的情况，如生成数据中的身高，当数据较少时，可以将每一个值作为一个类别，但当数据量大时，这样是不可取的，在 <strong>C4.5</strong> 算法中采用二分法对连续值进行处理。</p><p>对于连续的属性 X$X$ 假设共出现了 n 个不同的取值，将这些取值从小到大排序{x1,x2,x3,…,xn}${x1,x2,x3,…,xn}$ ，其中找一点作为划分点 t ，则将数据划分为两类，大于 t 的为一类，小于 t 的为另一类。而 t 的取值通常为相邻两点的平均数 t=xi+xi+12$t=xi+xi+12$。</p><p>则在 n 个连续值之中，可以作为划分点的 t 有 n-1 个。通过遍历可以像离散型一样来考察这些划分点。</p><p>Gain(D,X)=Ent(D)−|D&lt;t||D|Ent(D&lt;t)−|D&gt;t||D|Ent(D&gt;t)(5)</p><p>$$(5)Gain(D,X)=Ent(D)−|D&lt;t||D|Ent(D&lt;t)−|D&gt;t||D|Ent(D&gt;t)$$</p><p>其中得到样本 D$D$ 基于划分点 t 二分后的信息增益，于是我们可以选择使得 Ga∈(D,X)$Ga∈(D,X)$ 值最大的划分点。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line">33  </span><br><span class="line">34  </span><br><span class="line">35  </span><br><span class="line">36  </span><br><span class="line">37  </span><br><span class="line">38  </span><br><span class="line">39  </span><br><span class="line">40  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算连续值的划分点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_splitpoint(data, base\_ent, feature):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    base\_ent -- 根节点的信息熵  </span><br><span class="line">    feature -- 需要划分的连续特征  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    final\_t -- 连续值最优划分点  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    # 将连续值进行排序并转化为浮点类型  </span><br><span class="line">    continues\_value &#x3D; data\[feature\].sort\_values().astype(np.float64)  </span><br><span class="line">    continues\_value &#x3D; \[i for i in continues\_value\]  # 不保留原来的索引  </span><br><span class="line">    t\_set &#x3D; \[\]  </span><br><span class="line">    t\_ent &#x3D; &#123;&#125;  </span><br><span class="line">  </span><br><span class="line">    # 得到划分点 t 的集合  </span><br><span class="line">    for i in range(len(continues\_value)-1):  </span><br><span class="line">        temp\_t &#x3D; (continues\_value\[i\]+continues\_value\[i+1\])&#x2F;2  </span><br><span class="line">        t\_set.append(temp\_t)  </span><br><span class="line">  </span><br><span class="line">    # 计算最优划分点  </span><br><span class="line">    for each\_t in t\_set:  </span><br><span class="line">        # 将大于划分点的分为一类  </span><br><span class="line">        temp1\_data &#x3D; data\[data\[feature\].astype(np.float64) &gt; each\_t\]  </span><br><span class="line">        # 将小于划分点的分为一类  </span><br><span class="line">        temp2\_data &#x3D; data\[data\[feature\].astype(np.float64) &lt; each\_t\]  </span><br><span class="line">        weight1 &#x3D; len(temp1\_data)&#x2F;len(data)  </span><br><span class="line">        weight2 &#x3D; len(temp2\_data)&#x2F;len(data)  </span><br><span class="line">        # 计算每个划分点的信息增益  </span><br><span class="line">        temp\_ent &#x3D; base\_ent-weight1 \* \\  </span><br><span class="line">            get\_Ent(temp1\_data)-weight2\*get\_Ent(temp2\_data)  </span><br><span class="line">        t\_ent\[each\_t\] &#x3D; temp\_ent  </span><br><span class="line">    print(&quot;t\_ent:&quot;, t\_ent)  </span><br><span class="line">    final\_t &#x3D; max(t\_ent, key&#x3D;t\_ent.get)  </span><br><span class="line">    return final\_t  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>实现连续值最优划分点的函数后，寻找 <code>height</code> 连续特征值的划分点。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">final\_t &#x3D; get\_splitpoint(data, base\_ent, &#39;height&#39;)  </span><br><span class="line">final\_t  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t_ent: &#123;158.0: 0.1179805181500242, 159.5: 0.1179805181500242, 161.0: 0.2624392604045631, 162.0: 0.2624392604045631, 163.0: 0.3856047022157598, 163.5: 0.15618502398692893, 164.0: 0.3635040117533678, 165.0: 0.33712865788827096, 167.0: 0.4752766311586692, 170.0: 0.32920899348970845, 172.0: 0.5728389611412551, 172.5: 0.4248356349861979, 173.5: 0.3165383509071513, 174.5: 0.22314940393447813, 176.5: 0.14078143361499595, 179.0: 0.06696192680347068&#125;</span><br><span class="line"></span><br><span class="line">172.0 </span><br></pre></td></tr></table></figure><h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a><a href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="算法实现"></a>算法实现</h3><p>在对决策树中最佳特征选择和连续值处理之后，接下来就是对决策树的构建。</p><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86" title="数据预处理"></a>数据预处理</h4><p>首先我们将连续值进行处理，在找到最佳划分点之后，将 &lt;t$&lt;t$ 的值设为 0，将 &gt;=t$&gt;=t$ 的值设为 1。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def choice\_1(x, t):  </span><br><span class="line">    if x &gt; t:  </span><br><span class="line">        return &quot;&gt;&#123;&#125;&quot;.format(t)  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;&lt;&#123;&#125;&quot;.format(t)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">deal\_data &#x3D; data.copy()  </span><br><span class="line">\# 使用lambda和map函数将 height 按照final\_t划分为两个类别  </span><br><span class="line">deal\_data\[&quot;height&quot;\] &#x3D; pd.Series(  </span><br><span class="line">    map(lambda x: choice\_1(int(x), final\_t), deal\_data\[&quot;height&quot;\]))  </span><br><span class="line">deal\_data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h4 id="选择最优划分特征"><a href="#选择最优划分特征" class="headerlink" title="选择最优划分特征"></a><a href="#%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%88%92%E5%88%86%E7%89%B9%E5%BE%81" title="选择最优划分特征"></a>选择最优划分特征</h4><p>将数据进行预处理之后，接下来就是选择最优的划分特征。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;选择最优划分特征  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def choose\_feature(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    best\_feature -- 最优的划分特征  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    num\_features &#x3D; len(data.columns) - 1  # 特征数量  </span><br><span class="line">    base\_ent &#x3D; get\_Ent(data)  </span><br><span class="line">    best\_gain &#x3D; 0.0  # 初始化信息增益  </span><br><span class="line">    best\_feature &#x3D; data.columns\[0\]  </span><br><span class="line">    for i in range(num\_features):  # 遍历所有特征  </span><br><span class="line">        temp\_gain &#x3D; get\_gain(data, base\_ent, data.columns\[i\])    # 计算信息增益  </span><br><span class="line">        if (temp\_gain &gt; best\_gain):  # 选择最大的信息增益  </span><br><span class="line">            best\_gain &#x3D; temp\_gain  </span><br><span class="line">            best\_feature &#x3D; data.columns\[i\]  </span><br><span class="line">    return best\_feature  # 返回最优特征  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>完成函数之后，我们首先看看数据集中信息增益值最大的特征是什么？</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">choose\_feature(deal\_data)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;height&#39; </span><br></pre></td></tr></table></figure><h4 id="构建决策树"><a href="#构建决策树" class="headerlink" title="构建决策树"></a><a href="#%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91" title="构建决策树"></a>构建决策树</h4><p>在将所有的子模块构建好之后，最后就是对核心决策树的构建，本次实验采用<strong>信息增益（ID3）</strong>的方式构建决策树。在构建的过程中，根据算法流程，我们反复遍历数据集，计算每一个特征的信息增益，通过比较将最好的特征作为父节点，根据特征的值确定分支子节点，然后重复以上操作，直到某一个分支全部属于同一类别，或者遍历完所有的数据特征，当遍历到最后一个特征时，若分支数据依然“不纯”，就将其中数量较多的类别作为子节点。</p><p>因此最好采用递归的方式来构建决策树。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;构建决策树  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def create\_tree(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    tree -- 以字典的形式返回决策树  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    feature\_list &#x3D; data.columns\[:-1\].tolist()  </span><br><span class="line">    label\_list &#x3D; data.iloc\[:, -1\]  </span><br><span class="line">    if len(data\[&quot;labels&quot;\].value\_counts()) &#x3D;&#x3D; 1:  </span><br><span class="line">        leaf\_node &#x3D; data\[&quot;labels&quot;\].mode().values  </span><br><span class="line">        return leaf\_node            # 第一个递归结束条件：所有的类标签完全相同  </span><br><span class="line">    if len(feature\_list) &#x3D;&#x3D; 1:  </span><br><span class="line">        leaf\_node &#x3D; data\[&quot;labels&quot;\].mode().values  </span><br><span class="line">        return leaf\_node   # 第二个递归结束条件：用完了所有特征  </span><br><span class="line">    best\_feature &#x3D; choose\_feature(data)   # 最优划分特征  </span><br><span class="line">    tree &#x3D; &#123;best\_feature: &#123;&#125;&#125;  </span><br><span class="line">    feat\_values &#x3D; data\[best\_feature\]  </span><br><span class="line">    unique\_value &#x3D; set(feat\_values)  </span><br><span class="line">    for value in unique\_value:  </span><br><span class="line">        temp\_data &#x3D; data\[data\[best\_feature\] &#x3D;&#x3D; value\]  </span><br><span class="line">        temp\_data &#x3D; temp\_data.drop(\[best\_feature\], axis&#x3D;1)  </span><br><span class="line">        tree\[best\_feature\]\[value\] &#x3D; create\_tree(temp\_data)  </span><br><span class="line">    return tree  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>完成创建决策树函数后，接下来对我们第一棵树进行创建。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tree &#x3D; create\_tree(deal\_data)  </span><br><span class="line">tree  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;height&#39;: &#123;&#39;&lt;172.0&#39;: &#123;&#39;ear_stud&#39;: &#123;&#39;no&#39;: &#123;&#39;voice&#39;: &#123;&#39;thick&#39;: array([&#39;man&#39;], dtype&#x3D;object),</span><br><span class="line">      &#39;medium&#39;: array([&#39;man&#39;], dtype&#x3D;object),</span><br><span class="line">      &#39;thin&#39;: array([&#39;woman&#39;], dtype&#x3D;object)&#125;&#125;,</span><br><span class="line">    &#39;yes&#39;: array([&#39;woman&#39;], dtype&#x3D;object)&#125;&#125;,</span><br><span class="line">  &#39;&gt;172.0&#39;: array([&#39;man&#39;], dtype&#x3D;object)&#125;&#125; </span><br></pre></td></tr></table></figure><p>通过字典的方式表示构建好的树，可以通过图像的方式更加直观的了解。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/2.png"></a></p><p>通过图形可以看出，在构建决策树时不一定每一个特征都会成为树的节点（如同 <code>hair</code>）。</p><h4 id="决策分类"><a href="#决策分类" class="headerlink" title="决策分类"></a><a href="#%E5%86%B3%E7%AD%96%E5%88%86%E7%B1%BB" title="决策分类"></a>决策分类</h4><p>在构建好决策树之后，最终就可以使用未知样本进行预测分类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;决策分类  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def classify(tree, test):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    test -- 需要测试的数据  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    class\_label -- 分类结果  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    first\_feature &#x3D; list(tree.keys())\[0\]  # 获取根节点  </span><br><span class="line">    feature\_dict &#x3D; tree\[first\_feature\]  # 根节点下的树  </span><br><span class="line">    labels &#x3D; test.columns.tolist()  </span><br><span class="line">    value &#x3D; test\[first\_feature\]\[0\]  </span><br><span class="line">    for key in feature\_dict.keys():  </span><br><span class="line">        if value &#x3D;&#x3D; key:  </span><br><span class="line">            if type(feature\_dict\[key\]).\_\_name\_\_ &#x3D;&#x3D; &#39;dict&#39;:  # 判断该节点是否为叶节点  </span><br><span class="line">                class\_label &#x3D; classify(feature\_dict\[key\], test)  # 采用递归直到遍历到叶节点  </span><br><span class="line">            else:  </span><br><span class="line">                class\_label &#x3D; feature\_dict\[key\]  </span><br><span class="line">    return class\_label  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>在分类函数完成之后，接下来我们尝试对未知数据进行分类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test &#x3D; pd.DataFrame(&#123;&quot;hair&quot;: \[&quot;long&quot;\], &quot;voice&quot;: \[&quot;thin&quot;\], &quot;height&quot;: \[163\], &quot;ear\_stud&quot;: \[&quot;yes&quot;\]&#125;)  </span><br><span class="line">test  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>对连续值进行预处理。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test\[&quot;height&quot;\] &#x3D; pd.Series(map(lambda x: choice\_1(int(x), final\_t), test\[&quot;height&quot;\]))  </span><br><span class="line">test  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>分类预测。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classify(tree,test)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([&#39;woman&#39;], dtype&#x3D;object) </span><br></pre></td></tr></table></figure><p>一个身高 163 厘米，长发，带着耳钉且声音纤细的人，在我们构建的决策树判断后预测为一名女性。</p><p>上面的实验中，我们没有考虑 <code>=划分点</code> 的情况，你可以自行尝试将 <code>&gt;=划分点</code> 或 <code>&lt;=划分点</code> 归为一类，看看结果又有哪些不同？</p><h3 id="预剪枝和后剪枝"><a href="#预剪枝和后剪枝" class="headerlink" title="预剪枝和后剪枝"></a><a href="#%E9%A2%84%E5%89%AA%E6%9E%9D%E5%92%8C%E5%90%8E%E5%89%AA%E6%9E%9D" title="预剪枝和后剪枝"></a>预剪枝和后剪枝</h3><p>在决策树的构建过程中，特别在数据特征非常多时，为了尽可能正确的划分每一个训练样本，结点的划分就会不停的重复，则一棵决策树的分支就非常多。对于训练集而言，拟合出来的模型是非常完美的。但是，这种完美就使得整体模型的复杂度变高，同时对其他数据集的预测能力下降，也就是我们常说的过拟合使得模型的泛化能力变弱。为了避免过拟合问题的出现，在决策树中最常见的两种方法就是预剪枝和后剪枝。</p><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a><a href="#%E9%A2%84%E5%89%AA%E6%9E%9D" title="预剪枝"></a>预剪枝</h4><p>预剪枝，顾名思义预先减去枝叶，在构建决策树模型的时候，每一次对数据划分之前进行估计，如果当前节点的划分不能带来决策树泛化的提升，则停止划分并将当前节点标记为叶节点。例如前面构造的决策树，按照决策树的构建原则，通过 <code>height</code> 特征进行划分后 <code>&lt;172</code> 分支中又按照 <code>ear_stud</code> 特征值进行继续划分。如果应用预剪枝，则当通过 <code>height</code> 进行特征划分之后，对 <code>&lt;172</code> 分支是否进行 <code>ear_stud</code> 特征进行划分时计算划分前后的准确度，如果划分后的更高则按照 <code>ear_stud</code> 继续划分，如果更低则停止划分。</p><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a><a href="#%E5%90%8E%E5%89%AA%E6%9E%9D" title="后剪枝"></a>后剪枝</h4><p>跟预剪枝在构建决策树的过程中判断是否继续特征划分所不同的是，后剪枝在决策树构建好之后对树进行修剪。如果说预剪枝是自顶向下的修剪，那么后剪枝就是自底向上进行修剪。后剪枝将最后的分支节点替换为叶节点，判断是否带来决策树泛化的提升，是则进行修剪，并将该分支节点替换为叶节点，否则不进行修剪。例如在前面构建好决策树之后，<code>&gt;172</code>分支的 <code>voice</code> 特征，将其替换为叶节点如（<code>man</code>），计算替换前后划分准确度，如果替换后准确度更高则进行修剪（用叶节点替换分支节点），否则不修剪。</p><h2 id="预测分类"><a href="#预测分类" class="headerlink" title="预测分类"></a><a href="#%E9%A2%84%E6%B5%8B%E5%88%86%E7%B1%BB" title="预测分类"></a>预测分类</h2><p>在前面我们使用 <code>python</code> 将决策树的特征选择，连续值处理和预测分类做了详细的讲解。接下来我们应用决策树模型对真实的数据进行分类预测。</p><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a><a href="#%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE" title="导入数据"></a>导入数据</h3><p>本次应用到的数据为学生成绩数据集 <code>course-13-student.csv</code>，一共有 395 条数据，26 个特征。</p><blockquote><p>数据集下载 👉 <a href="http://labfile.oss.aliyuncs.com/courses/1081/course-13-student.csv">传送门</a></p></blockquote><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;导入数据集并预览  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import pandas as pd  </span><br><span class="line">  </span><br><span class="line">stu\_grade &#x3D; pd.read\_csv(&#39;course-13-student.csv&#39;)  </span><br><span class="line">stu\_grade.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>由于特征过多，我们选择部分特征作为决策树模型的分类特征,分别为：</p><ul><li><code>school</code>：学生所读学校(<code>GP</code>，<code>MS</code>)</li><li><code>sex</code>: 性别(<code>F</code>：女，<code>M</code>：男)</li><li><code>address</code>: 家庭住址(<code>U</code>：城市，<code>R</code>：郊区)</li><li><code>Pstatus</code>: 父母状态(<code>A</code>：同居，<code>T</code>：分居)</li><li><code>Pedu</code>: 父母学历由低到高</li><li><code>reason</code>: 选择这所学校的原因(<code>home</code>：家庭,<code>course</code>：课程设计，<code>reputation</code>：学校地位，<code>other</code>：其他)</li><li><code>guardian</code>: 监护人(<code>mother</code>：母亲，<code>father</code>：父亲，<code>other</code>：其他)</li><li><code>studytime</code>: 周末学习时长</li><li><code>schoolsup</code>: 额外教育支持(<code>yes</code>：有，<code>no</code>：没有)</li><li><code>famsup</code>: 家庭教育支持(<code>yes</code>：有，<code>no</code>：没有)</li><li><code>paid</code>: 是否上补习班(<code>yes</code>：是，<code>no</code>：否)</li><li><code>higher</code>: 是否想受更好的教育(<code>yes</code>：是，<code>no</code>：否)</li><li><code>internet</code>: 是否家里联网(<code>yes</code>：是，<code>no</code>：否)</li><li><code>G1</code>: 一阶段测试成绩</li><li><code>G2</code>: 二阶段测试成绩</li><li><code>G3</code>: 最终成绩</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new\_data &#x3D; stu\_grade.iloc\[:, \[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 15, 24, 25, 26\]\]  </span><br><span class="line">new\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1" title="数据预处理"></a>数据预处理</h3><p>首先我们将成绩 <code>G1</code>，<code>G2</code>，<code>G3</code> 根据分数进行等级划分，将 <code>0-4</code> 划分为 <code>bad</code>，<code>5-9</code> 划分为 <code>medium</code> ，</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def choice\_2(x):  </span><br><span class="line">    x &#x3D; int(x)  </span><br><span class="line">    if x &lt; 5:  </span><br><span class="line">        return &quot;bad&quot;  </span><br><span class="line">    elif x &gt;&#x3D; 5 and x &lt; 10:  </span><br><span class="line">        return &quot;medium&quot;  </span><br><span class="line">    elif x &gt;&#x3D; 10 and x &lt; 15:  </span><br><span class="line">        return &quot;good&quot;  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;excellent&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">stu\_data &#x3D; new\_data.copy()  </span><br><span class="line">stu\_data\[&quot;G1&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G1&quot;\]))  </span><br><span class="line">stu\_data\[&quot;G2&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G2&quot;\]))  </span><br><span class="line">stu\_data\[&quot;G3&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G3&quot;\]))  </span><br><span class="line">stu\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>同样我们对 <code>Pedu</code> （父母教育程度）也进行划分</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def choice\_3(x):  </span><br><span class="line">    x &#x3D; int(x)  </span><br><span class="line">    if x &gt; 3:  </span><br><span class="line">        return &quot;high&quot;  </span><br><span class="line">    elif x &gt; 1.5:  </span><br><span class="line">        return &quot;medium&quot;  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;low&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">stu\_data\[&quot;Pedu&quot;\] &#x3D; pd.Series(map(lambda x: choice\_3(x), stu\_data\[&quot;Pedu&quot;\]))  </span><br><span class="line">stu\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>在等级划分之后，为遵循 <code>scikit-learn</code> 函数的输入规范，需要将数据特征进行替换。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;特征值替换  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def replace\_feature(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    data -- 将特征值替换后的数据集  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    for each in data.columns:  # 遍历每一个特征名称  </span><br><span class="line">        feature\_list &#x3D; data\[each\]  </span><br><span class="line">        unique\_value &#x3D; set(feature\_list)  </span><br><span class="line">        i &#x3D; 0  </span><br><span class="line">        for fea\_value in unique\_value:  </span><br><span class="line">            data\[each\] &#x3D; data\[each\].replace(fea\_value, i)  </span><br><span class="line">            i +&#x3D; 1  </span><br><span class="line">    return data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>将特征值进行替换后展示。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stu\_data &#x3D; replace\_feature(stu\_data)  </span><br><span class="line">stu\_data.head(10)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a><a href="#%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86" title="数据划分"></a>数据划分</h3><p>加载好预处理的数据集之后，为了实现决策树算法，同样我们需要将数据集分为 <strong>训练集</strong>和<strong>测试集</strong>，依照经验：<strong>训练集</strong>占比为 70%，<strong>测试集</strong>占 30%。</p><p>同样在此我们使用 <code>scikit-learn</code> 模块的 <code>train_test_split</code> 函数完成数据集切分。  </p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model\_selection import train\_test\_split  </span><br><span class="line">  </span><br><span class="line">x\_train,x\_test, y\_train, y\_test &#x3D;train\_test\_split(train\_data,train\_target,test\_size&#x3D;0.4, random\_state&#x3D;0)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>x_train</code>,<code>x_test</code>, <code>y_train</code>, <code>y_test</code> 分别表示，切分后的 特征的训练集，特征的测试集，标签的训练集，标签的测试集；其中特征和标签的值是一一对应的。</li><li><code>train_data</code>,<code>train_target</code>分别表示为待划分的特征集和待划分的标签集。</li><li><code>test_size</code>：测试样本所占比例。</li><li><code>random_state</code>：随机数种子,在需要重复实验时，保证在随机数种子一样时能得到一组一样的随机数。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model\_selection import train\_test\_split  </span><br><span class="line">  </span><br><span class="line">x\_train, x\_test, y\_train, y\_test &#x3D; train\_test\_split(stu\_data.iloc\[:, :-1\], stu\_data\[&quot;G3&quot;\],   </span><br><span class="line">                                                    test\_size&#x3D;0.3, random\_state&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">x\_test  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="决策树构建"><a href="#决策树构建" class="headerlink" title="决策树构建"></a><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA" title="决策树构建"></a>决策树构建</h3><p>在划分好数据集之后，接下来就是进行预测。在前面的实验中我们采用 <code>python</code> 对决策树算法进行实现，下面我们通过 <code>scikit-learn</code> 来对其进行实现。 <code>scikit-learn</code> 决策树类及常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(criterion&#x3D;’gini’，random\_state&#x3D;None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>criterion</code> 表示特征划分方法选择，默认为 <code>gini</code> (在后面会讲到)，可选择为 <code>entropy</code> (信息增益)。</li><li><code>ramdom_state</code> 表示随机数种子，当特征特别多时 <code>scikit-learn</code> 为了提高效率，随机选取部分特征来进行特征选择，即找到所有特征中较优的特征。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>训练决策树。</li><li><code>predict(X)</code> 对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier  </span><br><span class="line">  </span><br><span class="line">dt\_model &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;entropy&#39;, random\_state&#x3D;34)  </span><br><span class="line">dt\_model.fit(x\_train,y\_train) # 使用训练集训练模型  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(class_weight&#x3D;None, criterion&#x3D;&#39;entropy&#39;, max_depth&#x3D;None,</span><br><span class="line">            max_features&#x3D;None, max_leaf_nodes&#x3D;None,</span><br><span class="line">            min_impurity_decrease&#x3D;0.0, min_impurity_split&#x3D;None,</span><br><span class="line">            min_samples_leaf&#x3D;1, min_samples_split&#x3D;2,</span><br><span class="line">            min_weight_fraction_leaf&#x3D;0.0, presort&#x3D;False, random_state&#x3D;34,</span><br><span class="line">            splitter&#x3D;&#39;best&#39;) </span><br></pre></td></tr></table></figure><h3 id="决策树可视化"><a href="#决策树可视化" class="headerlink" title="决策树可视化"></a><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96" title="决策树可视化"></a>决策树可视化</h3><p>在构建好决策树之后，我们需要对创建好的决策树进行可视化展示，引入 <code>export_graphviz</code> 进行画图。由于环境中没有函数需要进行安装。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\# Linux  </span><br><span class="line">!apt-get install --yes graphviz # 安装所需模块  </span><br><span class="line">!pip install graphviz  </span><br><span class="line">  </span><br><span class="line">\# windows Anaconda  </span><br><span class="line">conda install graphviz  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>下面开始生成决策树图像，其中生成决策树较大需要拖动滑动条进行查看。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import export\_graphviz  </span><br><span class="line">import graphviz  </span><br><span class="line">  </span><br><span class="line">img &#x3D; export\_graphviz(  </span><br><span class="line">    dt\_model, out\_file&#x3D;None,  </span><br><span class="line">    feature\_names&#x3D;stu\_data.columns\[:-1\].values.tolist(),  # 传入特征名称  </span><br><span class="line">    class\_names&#x3D;np.array(\[&quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;, &quot;excellent&quot;\]),  # 传入类别值  </span><br><span class="line">    filled&#x3D;True, node\_ids&#x3D;True,  </span><br><span class="line">    rounded&#x3D;True)  </span><br><span class="line">  </span><br><span class="line">graphviz.Source(img)  # 展示决策树  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/output_157_0.svg"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/output_157_0.svg" alt="svg">svg</a></p><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a><a href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B" title="模型预测"></a>模型预测</h3><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y\_predict &#x3D; dt\_model.predict(x\_test) # 使用模型对测试集进行预测  </span><br><span class="line">y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 1, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 3, 0, 3, 2, 0, 3, 2, 2, 0,</span><br><span class="line">       3, 2, 2, 0, 3, 3, 2, 2, 0, 0, 0, 2, 1, 0, 1, 2, 3, 0, 3, 3, 2, 2,</span><br><span class="line">       0, 2, 2, 2, 2, 2, 3, 2, 2, 0, 3, 0, 2, 2, 2, 1, 3, 0, 2, 2, 2, 2,</span><br><span class="line">       3, 3, 2, 0, 0, 0, 1, 2, 2, 0, 0, 3, 0, 3, 2, 3, 2, 2, 3, 1, 0, 0,</span><br><span class="line">       0, 2, 1, 2, 2, 2, 2, 3, 0, 0, 3, 0, 0, 2, 3, 2, 1, 2, 2, 0, 0, 2,</span><br><span class="line">       0, 2, 0, 3, 2, 2, 2, 3, 2], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h3 id="分类准确率计算"><a href="#分类准确率计算" class="headerlink" title="分类准确率计算"></a><a href="#%E5%88%86%E7%B1%BB%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97" title="分类准确率计算"></a>分类准确率计算</h3><p>当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。</p><p>accur=∑Ni=1I(¯yi=yi)N(6)</p><p>$$(6)accur=∑i=1NI(yi¯=yi)N$$</p><p>公式(6)中 N$N$ 表示数据总条数，¯yi$yi¯$ 表示第 i$i$ 条数据的种类预测值，yi$yi$ 表示第 i$i$ 条数据的种类真实值，I$I$ 同样是指示函数，表示 ¯yi$yi¯$ 和 yi$yi$ 相同的个数。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;准确率计算  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def get\_accuracy(test\_labels, pred\_labels):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    test\_labels -- 测试集的真实值  </span><br><span class="line">    pred\_labels -- 测试集的预测值  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    accur -- 准确率  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    correct &#x3D; np.sum(test\_labels &#x3D;&#x3D; pred\_labels)  # 计算预测正确的数据个数  </span><br><span class="line">    n &#x3D; len(test\_labels)  # 总测试集数据个数  </span><br><span class="line">    accur &#x3D; correct&#x2F;n  </span><br><span class="line">    return accur  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">get\_accuracy(y\_test, y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.6974789915966386 </span><br></pre></td></tr></table></figure><h2 id="CART-决策树"><a href="#CART-决策树" class="headerlink" title="CART 决策树"></a><a href="#CART-%E5%86%B3%E7%AD%96%E6%A0%91" title="CART 决策树"></a>CART 决策树</h2><p>分类与回归树（classification and regression tree, CART）同样也是应用广泛的决策树学习算法，CART 算法是按照特征划分，由树的生成和树的剪枝构成，既可以进行分类又可以用于回归，按照作用将其分为决策树和回归树，由于本实验设计为决策树的概念，所以回归树的部分有兴趣的同学可以自己查找相关资料进一步学习。</p><p>CART决策树的构建和常见的 <strong>ID3</strong> 和 <strong>C4.5</strong> 算法的流程相似，但在特征划分选择上CART选择了 <strong>基尼指数</strong> 作为划分标准。数据集 D$D$ 的纯度可用基尼值来度量：</p><p>Gini(D)=|y|∑y=1∑k′≠kpkp′k(7)</p><p>$$(7)Gini(D)=∑y=1|y|∑k′≠kpkpk′$$</p><p><strong>基尼指数</strong>表示随机抽取两个样本，两个样本类别不一致的概率，<strong>基尼指数</strong>越小则数据集的纯度越高。同样对于每一个特征值的基尼指数计算，其和 <strong>ID3</strong> 、 <strong>C4.5</strong> 相似，定义为：</p><p>GiniValue(D,a)=M∑m=1|Dm||D|Gini(Dm)(8)</p><p>$$(8)GiniValue(D,a)=∑m=1M|Dm||D|Gini(Dm)$$</p><p>在进行特征划分的时候，选择特征中基尼值最小的作为最优特征划分点。</p><p>实际上，在应用过程中，更多的会使用 <strong>基尼指数</strong> 对特征划分点进行决策，最重要的原因是计算复杂度相较于 <strong>ID3</strong> 和 <strong>C4.5</strong> 小很多（没有对数运算）。</p><p><strong>拓展阅读：</strong></p><ul><li><a href="https://zh.wikipedia.org/zh-hans/%E5%86%B3%E7%AD%96%E6%A0%91">决策树- 维基百科</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。  &lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Decision Tree" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Decision-Tree/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Decision Tree" scheme="http://www.laugh12321.cn/blog/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|感知机和人工神经网络详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/31/neural_network/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/31/neural_network/</id>
    <published>2019-01-31T00:00:00.000Z</published>
    <updated>2020-10-21T08:57:06.985Z</updated>
    
    <content type="html"><![CDATA[<p>人工神经网络是一种发展时间较早且十分常用的机器学习算法。因其模仿人类神经元工作的特点，在监督学习和非监督学习领域都给予了人工神经网络较高的期望。目前，由传统人工神经网络发展而来的卷积神经网络、循环神经网络已经成为了深度学习的基石。本篇文章中，我们将从人工神经网络的原型感知机出发，介绍机器学习中人工神经网络的特点及应用。  </p><a id="more"></a><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>在介绍人工神经网络之前，我们先介绍它的原型：感知机。关于感知机，我们先引用一段来自维基百科的背景介绍：</p><blockquote><p>感知器（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。</p></blockquote><p>如果你之前从未接触过人工神经网络，那么上面这句话或许还需要等到阅读完这篇文章才能完整理解。不过，你可以初步发现，感知机其实就是人工神经网络，只不过是其初级形态。</p><h3 id="感知机的推导过程"><a href="#感知机的推导过程" class="headerlink" title="感知机的推导过程"></a>感知机的推导过程</h3><p>那么，<strong>感知机到底是什么？它是怎样被发明出来的呢？</strong></p><p>要搞清楚上面的问题，我们就需要提到前面学习过的一个非常熟悉的知识点：线性回归。回忆关于逻辑回归的内容，你应该还能记得当初我们说过逻辑回归起源于线性回归。而感知机作为一种最简单的二分类模型，它其实就是使用了线性回归的方法完成平面数据点的分类。而逻辑回归后面引入了逻辑斯蒂估计来计算分类概率的方法甚至可以被当作是感知机的进步。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/1.png"></center><p>你还记得上面这张图片吗？当数据点处于线性可分时，我们可以使用一条直线将其分开，而分割线的函数为：</p><p>$$<br>f(x) = w_1x_1+w_2x_2+ \cdots +w_nx_n + b = WX+b \tag{1}<br>$$</p><p>对于公式（1）而言，我们可以认为分割直线方程其实就是针对数据集的每一个特征 $x_1,x_2,⋯,x_n$ 依次乘上权重 $w_1,w_2,⋯,w_n$ 所得。</p><p>当我们确定好公式（1）的参数后，每次输入一个数据点对应的特征 $x_1,x_2,⋯,x_n$ 就能得到对应的函数值 $f(x)$。那么，<strong>怎样判定这个数据点属于哪一个类别呢？</strong></p><p>在二分类问题中，我们最终的类别共有两个，通常被称之为正类别和负类别。而当我们使用线性回归中对应的公式（1）完成分类时，不同于逻辑回归中将 $f(x)$ 传入 <code>sigmoid</code> 函数，这里我们将 $f(x)$ 传入如下所示的 <code>sign</code> 函数。</p><p>$$<br>sign(x) =<br>\begin{cases}<br>+1, &amp; \text{if } x  \geq  0 \\<br>-1, &amp; \text{if } x  &lt;  0<br>\end{cases}<br>\tag{2}<br>$$</p><p><code>sign()</code> 函数又被称之为符号函数，它的函数值只有 <code>2</code> 个。即当自变量 $x≥0$ 时，因变量为 <code>1</code>。同理，当 $x&lt;0$ 时，因变量为 <code>-1</code>。函数图像如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/2.PNG"></center><p>于是，当我们将公式（1）中的 $f(x)$ 传入公式（2），就能得到 $sign(f(x))$ 的值。其中，当 $sign(f(x))=1$ 时，就为正分类点，而 $sign(f(x))=−1$ 时，则为负分类点。</p><p><strong>综上所示</strong>，我们就假设输入空间(特征向量)为 $X \subseteq R^n$，输出空间为 $Y=−1,+1$。输入 $x⊆X$ 表示实例的特征向量，对应于输入空间的点；输出 $y⊆Y$ 表示示例的类别。由输入空间到输出空间的函数如下：</p><p>$$<br>f(x) = sign(w*x +b) \tag{3}<br>$$</p><p><strong>公式（3）就被称之为感知机</strong>。注意，公式（3）中的 $f(x)$ 和公式（1）中的 $f(x)$ 不是同一个 $f(x)$。</p><h3 id="感知机计算流程图"><a href="#感知机计算流程图" class="headerlink" title="感知机计算流程图"></a>感知机计算流程图</h3><p>上面，我们针对感知机进行了数学推导。为了更加清晰地展示出感知机的计算过程，我们将其绘制成如下所示的流程图。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/3.png"></center><h3 id="感知机的损失函数"><a href="#感知机的损失函数" class="headerlink" title="感知机的损失函数"></a>感知机的损失函数</h3><p>前面的文章中，我们已经介绍过损失函数的定义。在感知机的学习过程中，我们同样需要确定每一个特征变量对应的参数，而损失函数的极小值往往就意味着参数最佳。那么，感知机学习的策略，也就是其通常采用哪种形式的损失函数呢？</p><p>如下图所示，当我们使用一条直线去分隔一个线性可分的数据集时，有可能会出现「误分类」的状况。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/4.png"></center><p>而在感知机的学习过程中，我们通常会使用误分类点到分割线（面）的距离去定义损失函数。</p><h4 id="点到直线的距离"><a href="#点到直线的距离" class="headerlink" title="点到直线的距离"></a>点到直线的距离</h4><p>中学阶段，我们学过点到直线的距离公式推导。对于 $n$ 维实数向量空间中任意一点 $x_0$ 到直线 $W⋅x+b=0$ 的距离为：</p><p>$$<br>d= \dfrac{1}{\parallel W\parallel}|W*x_{0}+b| \tag{4}<br>$$</p><p>其中 $||W||$ 表示 $L_2$ 范数，即向量各元素的平方和然后开方。</p><p>然后，对于误分类点 $(xi,yi)$ 来讲，公式（5）成立。</p><p>$$<br>y_i(W * x_{i}+b)&gt;0 \tag{5}<br>$$</p><p>那么，误分类点 $(xi,yi)$ 到分割线（面）的距离就为：</p><p>$$<br>d=-\dfrac{1}{\parallel W\parallel}y_i(W*x_{i}+b) \tag{6}<br>$$</p><p>于是，假设所有误分类点的集合为 $M$，全部误分类点到分割线（面）的距离就为：</p><p>$$<br>-\dfrac{1}{\parallel W\parallel}\sum_{x_i\epsilon M} y_i(W*x_{i}+b) \tag{7}<br>$$</p><p><strong>最后得到感知机的损失函数</strong>为：</p><p>$$<br>J(W,b) = - \sum_{x_i\epsilon M} y_i(W*x_{i}+b) \tag{8}<br>$$</p><p>从公式（8）可以看出，损失函数 $J(W,b)$ 是非负的。也就是说，当没有误分类点时，损失函数的值为 <code>0</code>。同时，误分类点越少，误分类点距离分割线（面）就越近，损失函数值就越小。同时，损失函数 $J(W,b)$ 是连续可导函数。</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>当我们在实现分类时，最终想要的结果肯定是没有误分类的点，也就是损失函数取极小值时的结果。在逻辑回归的中，为了找到损失函数的极小值，我们使用到了一种叫做梯度下降法（Gradient descent）。而在这篇中，我们尝试一种梯度下降法的改进方法，也称之为随机梯度下降法（Stochastic gradient descent，简称：SGD)。</p><p>实验 SGD 计算公式（8）的极小值时，首先任选一个分割面 $W_0$ 和 $b_0$，然后使用梯度下降法不断地极小化损失函数：</p><p>$$<br>min_{W,b} J(W,b) = - \sum_{x_i\epsilon M} y_i(W*x_{i}+b) \tag{9}<br>$$</p><p>随机梯度下降的特点在于，极小化过程中不是一次针对 $M$ 中的所有误分类点执行梯度下降，而是每次<strong>随机</strong>选取一个误分类点执行梯度下降。等到更新完 $W$ 和 $b$ 之后，下一次再另随机选择一个误分类点执行梯度下降直到收敛。</p><p>计算损失函数的偏导数：</p><p>$$<br>\frac{\partial J(W,b)}{\partial W} = - \sum_{x_i\epsilon M}y_ix_i \ \frac{\partial J(W,b)}{\partial b} = - \sum_{x_i\epsilon M}y_i \tag{10}<br>$$</p><p>如果 $yi(W∗xi+b)≤0$ 更新 $W$ 和 $b$：</p><p>$$<br>W \leftarrow   W + \lambda y_ix_i \ b \leftarrow  b + \lambda y_i \tag{11}<br>$$</p><p>同前面的梯度下降一致，$λ$ 为学习率，也就是每次梯度下降的步长。</p><p>下面，我们使用 Python 将上面的随机梯度下降算法进行实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">感知机随机梯度下降算法实现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceptron_sgd</span>(<span class="params">X, Y, alpha, epochs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 自变量数据矩阵</span></span><br><span class="line"><span class="string">    Y -- 因变量数据矩阵</span></span><br><span class="line"><span class="string">    alpha -- lamda 参数</span></span><br><span class="line"><span class="string">    epochs -- 迭代次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    w -- 权重系数</span></span><br><span class="line"><span class="string">    b -- 截距项</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w = np.zeros(<span class="built_in">len</span>(X[<span class="number">0</span>])) <span class="comment"># 初始化参数为 0</span></span><br><span class="line">    b = np.zeros(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs): <span class="comment"># 迭代</span></span><br><span class="line">        <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(X):</span><br><span class="line">            <span class="keyword">if</span> ((np.dot(X[i], w)+b)*Y[i]) &lt;= <span class="number">0</span>: <span class="comment"># 判断条件</span></span><br><span class="line">                w = w + alpha*X[i]*Y[i] <span class="comment"># 更新参数</span></span><br><span class="line">                b = b + alpha*Y[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h3 id="感知机分类实例"><a href="#感知机分类实例" class="headerlink" title="感知机分类实例"></a>感知机分类实例</h3><p>前面的内容中，我们讨论了感知机的计算流程，感知机的损失函数，以及如何使用随机梯度下降求解感知机的参数。理论说了这么多，下面就举一个实际的例子看一看。</p><h4 id="示例数据集"><a href="#示例数据集" class="headerlink" title="示例数据集"></a>示例数据集</h4><p>为了方便绘图到二维平面，这里只使用包含两个特征变量的数据，数据集名称为 <code>course-12-data.csv</code>。</p><blockquote><p>数据集下载 👉 <a href="http://labfile.oss.aliyuncs.com/courses/1081/course-12-data.csv">传送门</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;加载数据集</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;course-12-data.csv&quot;</span>, header=<span class="number">0</span>) <span class="comment"># 加载数据集</span></span><br><span class="line">df.head() <span class="comment"># 预览前 5 行数据</span></span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_00.png"></center><p>可以看到，该数据集共有两个特征变量 <code>X0</code> 和 <code>X1</code>, 以及一个目标值 <code>Y</code>。其中，目标值 <code>Y</code> 只包含 <code>-1</code> 和 <code>1</code>。我们尝试将该数据集绘制成图，看一看数据的分布情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制数据集</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.scatter(df[<span class="string">&#x27;X0&#x27;</span>],df[<span class="string">&#x27;X1&#x27;</span>], c=df[<span class="string">&#x27;Y&#x27;</span>])</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_71_1.png"></center><h4 id="感知机训练"><a href="#感知机训练" class="headerlink" title="感知机训练"></a>感知机训练</h4><p>接下来，我们就使用感知机求解最佳分割线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = df[[<span class="string">&#x27;X0&#x27;</span>,<span class="string">&#x27;X1&#x27;</span>]].values</span><br><span class="line">Y = df[<span class="string">&#x27;Y&#x27;</span>].values</span><br><span class="line"></span><br><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">epochs = <span class="number">150</span></span><br><span class="line"></span><br><span class="line">perceptron_sgd(X, Y, alpha, epochs)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([ 4.93, -6.98]), array([-3.3])) </span><br></pre></td></tr></table></figure><p>于是，我们求得的最佳分割线方程为：</p><p>$$<br>f(x) = 4.93 * x_{1} - 6.98 * x_{2} -3.3 \tag{12}<br>$$</p><p>此时，可以求解一下分类的正确率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">L = perceptron_sgd(X, Y, alpha, epochs)</span><br><span class="line">w1 = L[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">w2 = L[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">b = L[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">z = np.dot(X, np.array([w1, w2]).T) + b</span><br><span class="line">np.sign(z)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,</span><br><span class="line">       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,</span><br><span class="line">       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,</span><br><span class="line">       -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.]) </span><br></pre></td></tr></table></figure><p>为了方便，我们就直接使用 <code>scikit-learn</code> 提供的准确率计算方法 <code>accuracy_score()</code>，该方法相信你已经非常熟悉了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">accuracy_score(Y, np.sign(z))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9866666666666667 </span><br></pre></td></tr></table></figure><p>所以，最终的分类准确率约为 <code>0.987</code>。</p><h4 id="绘制决策边界线"><a href="#绘制决策边界线" class="headerlink" title="绘制决策边界线"></a>绘制决策边界线</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制轮廓线图，不需要掌握</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.scatter(df[<span class="string">&#x27;X0&#x27;</span>],df[<span class="string">&#x27;X1&#x27;</span>], c=df[<span class="string">&#x27;Y&#x27;</span>])</span><br><span class="line"></span><br><span class="line">x1_min, x1_max = df[<span class="string">&#x27;X0&#x27;</span>].<span class="built_in">min</span>(), df[<span class="string">&#x27;X0&#x27;</span>].<span class="built_in">max</span>(),</span><br><span class="line">x2_min, x2_max = df[<span class="string">&#x27;X1&#x27;</span>].<span class="built_in">min</span>(), df[<span class="string">&#x27;X1&#x27;</span>].<span class="built_in">max</span>(),</span><br><span class="line"></span><br><span class="line">xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))</span><br><span class="line">grid = np.c_[xx1.ravel(), xx2.ravel()]</span><br><span class="line"></span><br><span class="line">probs = (np.dot(grid, np.array([L[<span class="number">0</span>][<span class="number">0</span>], L[<span class="number">0</span>][<span class="number">1</span>]]).T) + L[<span class="number">1</span>]).reshape(xx1.shape)</span><br><span class="line">plt.contour(xx1, xx2, probs, [<span class="number">0</span>], linewidths=<span class="number">1</span>, colors=<span class="string">&#x27;red&#x27;</span>);</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_87_0.png"></center><p>可以看到，上图中的红色直线就是我们最终的分割线，分类的效果还是不错的。</p><h4 id="绘制损失函数变换曲线"><a href="#绘制损失函数变换曲线" class="headerlink" title="绘制损失函数变换曲线"></a>绘制损失函数变换曲线</h4><p>除了绘制决策边界，也就是分割线。我们也可以将损失函数的变化过程绘制处理，看一看梯度下降的执行过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算每次迭代后的损失函数值</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceptron_loss</span>(<span class="params">X, Y, alpha, epochs</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 自变量数据矩阵</span></span><br><span class="line"><span class="string">    Y -- 因变量数据矩阵</span></span><br><span class="line"><span class="string">    alpha -- lamda 参数</span></span><br><span class="line"><span class="string">    epochs -- 迭代次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    loss_list -- 每次迭代损失函数值列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    w = np.zeros(<span class="built_in">len</span>(X[<span class="number">0</span>])) <span class="comment"># 初始化参数为 0</span></span><br><span class="line">    b = np.zeros(<span class="number">1</span>)</span><br><span class="line">    loss_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs): <span class="comment"># 迭代</span></span><br><span class="line">        loss_init = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(X):</span><br><span class="line">            <span class="keyword">if</span> ((np.dot(X[i], w)+b)*Y[i]) &lt;= <span class="number">0</span>: <span class="comment"># 判断条件</span></span><br><span class="line">                loss_init += (((np.dot(X[i], w)+b)*Y[i]))</span><br><span class="line">                w = w + alpha*X[i]*Y[i] <span class="comment"># 更新参数</span></span><br><span class="line">                b = b + alpha*Y[i]</span><br><span class="line">        loss_list.append(loss_init * <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss_list</span><br><span class="line">    </span><br><span class="line">loss_list = perceptron_loss(X, Y, alpha, epochs)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(loss_list))], loss_list)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Learning rate &#123;&#125;, Epochs &#123;&#125;&quot;</span>.<span class="built_in">format</span>(alpha, epochs))</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss function&quot;</span>)</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_94_1.png"></center><p>如上图所示，你会发现，让我们按照 <code>0.1</code> 的学习率迭代 <code>150</code> 次后，损失函数依旧无法到达 <code>0</code>。一般情况下，当我们的数据不是线性可分时，损失函数就会出现如上图所示的震荡线性。</p><p>不过，如果你仔细观察上方数据的散点图，你会发现这个数据集看起来是线性可分的。那么，当数据集线性可分，却造成损失函数变换曲线震荡的原因一般有两点：<strong>学习率太大</strong>或者<strong>迭代次数太少</strong>。</p><p>其中，迭代次数太少很好理解，也就是说我们迭代的次数还不足以求得极小值。至于学习率太大，可以看下方的示意图。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/5.png"></center><p>如上图所示，当我们的学习率太大时，往往容易出现在损失函数底部来回震荡的现象而无法到达极小值点。所以，面对上面这种情况，我们可以采取减小学习率 + 增加迭代次数的方法找到损失函数极小值点。</p><p>所以，下面就再试一次。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">alpha &#x3D; 0.05 # 减小学习率</span><br><span class="line">epochs &#x3D; 1000 # 增加迭代次数</span><br><span class="line"></span><br><span class="line">loss_list &#x3D; perceptron_loss(X, Y, alpha, epochs)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(10, 6))</span><br><span class="line">plt.plot([i for i in range(len(loss_list))], loss_list)</span><br><span class="line">plt.xlabel(&quot;Learning rate &#123;&#125;, Epochs &#123;&#125;&quot;.format(alpha, epochs))</span><br><span class="line">plt.ylabel(&quot;Loss function&quot;)</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_101_1.png"></center><p>可以看到，当迭代次数约为 <code>700</code> 次，即上图后半段时，损失函数的值等于 <code>0</code>。根据我们在 <code>1.3</code> 小节中介绍的内容，当损失函数为 <code>0</code> 时，就代表没有误分类点存在。</p><p>此时，我们再一次计算分类准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">L = perceptron_sgd(X, Y, alpha, epochs)</span><br><span class="line">z = np.dot(X, L[<span class="number">0</span>].T) + L[<span class="number">1</span>]</span><br><span class="line">accuracy_score(Y, np.sign(z))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0 </span><br></pre></td></tr></table></figure><p>和损失函数变化曲线得到的结论一致，分类准确率已经 <code>100%</code>，表示全部数据点被正确分类。</p><h2 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h2><center><iframe src="//player.bilibili.com/player.html?aid=15532370&amp;cid=25368631&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="450px" width="100%"> </iframe></center><div style="color: #999;font-size: 12px;text-align: center;">神经网络的结构 | 视频来源：<a href="https://www.bilibili.com/video/av15532370" target="_blank" rel="noopener">3Blue1Brown</a></div><p>上面的内容中，我们已经了解到了什么是感知机，以及如何构建一个感知机分类模型。你会发现，感知机只能处理二分类问题，且必须是线性可分问题。如果是这样的话，该方法的局限性就比较大了。那么，面对线性不可分或者多分类问题时，我们有没有一个更好的方法呢？</p><h3 id="多层感知机与人工神经网络"><a href="#多层感知机与人工神经网络" class="headerlink" title="多层感知机与人工神经网络"></a>多层感知机与人工神经网络</h3><p>这里，就要提到本文的主角，也就是人工神经网络（英语：Artificial neural network，简称：ANN）。如果你第一次接触到人工神经网络，不要将其想的太神秘。其实，上面的感知机模型就是一个人工神经网络，只不过它是一个结构简单的单层神经网络。而如果我们要解决线性不可分或者多分类问题，往往会尝试将多个感知机组合在一起，变成一个更复杂的神经网络结构。</p><div style="color: #999;font-size: 12px;font-style: italic;">由于一些历史遗留问题，感知机、多层感知机、人工神经网络三种说法界限模糊，文中介绍到的人工神经网络从某种意义上代指多层感知机。</div><p>在上文 <code>1.2</code> 小节中，我们通过一张图展示了感知机的工作流程，我们将该流程图进一步精简如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/6.png"></center><p>这张图展示了一个感知机模型的执行流程。我们可以把输入称之为「输入层」，输出称之为「输出层」。对于像这样只包含一个输入层的网络结构就可以称之为单层神经网络结构。</p><p>单个感知机组成了单层神经网络，如果我们将一个感知机的输出作为另一个感知机的输入，就组成了多层感知机，也就是一个多层神经网络。其中，我们将输入和输出层之间的称为隐含层。如下图所示，这就是包含 <code>1</code> 个隐含层的神经网络结构。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/7.png"></center><p>一个神经网络结构在计算层数的时候，我们一般只计算输入和隐含层的数量，即上方是一个 <code>2</code> 层神经网络结构。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>目前，我们已经接触过逻辑回归、感知机、多层感知机与人工神经网络 <code>4</code> 个概念。你可能隐约感觉到，似乎这 <code>4</code> 种方法都与线性函数有关，而区别在于对线性函数的因变量的不同处理方式上面。</p><p>$$<br>f(x) = w_1x_1+w_2x_2+ \cdots +w_nx_n + b = WX+b \tag{13}<br>$$</p><ul><li>对于逻辑回归而言，我们是采用了 $sigmoid$ 函数将 $f(x)$ 转换为概率，最终实现二分类。</li><li>对于感知机而言，我们是采用了 $sign$ 函数将 $f(x)$ 转换为 <code>-1 和 +1</code> 最终实现二分类。</li><li>对于多层感知机而言，具有多层神经网络结构，在 $f(x)$ 的处理方式上，一般会有更多的操作。</li></ul><p>于是，$sigmoid$ 函数和 $sign$ 函数还有另外一个称谓，叫做「激活函数（Activation function）」。听到激活函数，大家首先不要觉得它有多么的高级。之所以有这样一个称谓，是因为函数本身有一些特点，但归根结底还是数学函数。下面，我们就列举一下常见的激活函数及其图像。</p><h4 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="$sigmoid$ 函数"></a>$sigmoid$ 函数</h4><p>$sigmoid$ 函数应该已经非常熟悉了吧，它的公式如下：</p><p>$$<br>sigmoid(x)=\frac{1}{1+e^{-x}} \tag{14}<br>$$</p><p>$sigmoid$ 函数的图像呈 S 型，函数值介于 $(0,1)$ 之间：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/8.png"></center><h4 id="Tanh-函数"><a href="#Tanh-函数" class="headerlink" title="$Tanh$ 函数"></a>$Tanh$ 函数</h4><p>$Tanh$ 函数与 $sigmoid$ 函数的图像很相似，都呈 S 型，只不过 $Tanh$ 函数值介于 $(-1,1)$ 之间，公式如下：</p><p>$$<br>tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}\tag{15}<br>$$</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/9.png"></center><h4 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="$ReLU$ 函数"></a>$ReLU$ 函数</h4><p>$ReLU$ 函数全称叫做 Rectified Linear Unit，也就是修正线性单元，公式如下：</p><p>$$<br>ReLU(x) = max(0,x)\tag{16}<br>$$</p><p>$ReLU$ 有很多优点，比如收敛速度会较快且不容易出现梯度消失。由于这次不会用到，我们之后再说。$ReLU$的图像如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/10.png"></center><h4 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h4><p>上面列举了 3 种常用的激活函数，其中 $sigmoid$ 函数是介绍的人工神经网络中十分常用的一种激活函数。谈到激活函数的作用，直白地讲就是针对数据进行非线性变换。只是不同的激活函数适用于不同的场景，而这些都是机器学习专家根据应用经验总结得到的。</p><p>在神经网络结构中，我们通过线性函数不断的连接输入和输出。你可以设想，在这种结构中，每一层输出都是上层输入的线性变换。于是，无论神经网络有多少层，最终输出都是输入的线性组合。这样的话，单层神经网络和多层神经网络有什么区别呢？（没有区别）</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/11.png"></center><p>如上图所示，线性变换的多重组合依旧还是线性变换。如果我们在网络结构中加入激活函数，就相当于引入了非线性因素，这样就可以解决线性模型无法完成的分类任务。</p><h3 id="反向传播算法（BP）直观认识"><a href="#反向传播算法（BP）直观认识" class="headerlink" title="反向传播算法（BP）直观认识"></a>反向传播算法（BP）直观认识</h3><center><iframe src="//player.bilibili.com/player.html?aid=16577449&amp;cid=27038097&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="450px" width="100%"> </iframe></center><div style="color: #999;font-size: 12px;text-align: center;">直观理解反向传播 | 视频来源：<a href="https://www.bilibili.com/video/av16577449/?p=1" target="_blank" rel="noopener">3Blue1Brown</a></div><p>前面感知机的章节中，我们定义了一个损失函数，并通过一种叫做随机梯度下降的方法去求解最优参数。如果你仔细观察随机梯度下降的过程，其实就是通过求解偏导数并组合成梯度用于更新权重 $W$ 和 $b$。感知机只有一层网络结构，求解梯度的过程还比较简单。但是，当我们组合成多层神经网络之后，更新权重的过程就变得复杂起来，而反向传播算法正是为了快速求解梯度而生。</p><p>反向传播的算法说起来很简单，但要顺利理解还比较复杂。这里，我们引用了波兰 AGH 科技大学的一篇 <a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html">科普文章</a> 中的配图来帮助理解反向传播的过程。</p><p>下图呈现了一个经典的 <code>3</code> 层神经网络结构，其包含有 <code>2</code> 个输入 $x_1$ 和 $x_2$ 以及 <code>1</code> 个输出 $y$。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/12.png"></center><p>网络中的每个紫色单元代表一个独立的神经元，它分别由两个单元组成。一个单元是权重和输入信号，而另一个则是上面提到的激活函数。其中，$e$ 代表激活信号，所以 $y=f(e)$ 就是被激活函数处理之后的非线性输出，也就是整个神经元的输出。</p><div style="color: #999;font-size: 12px;font-style: italic;">注：此处与下文使用g()作为激活函数稍有不同</div><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/13.png"></center><p>下面开始训练神经网络，训练数据由输入信号 $x_1$ 和 $x_2$ 以及期望输出 $z$ 组成，首先计算第 1 个隐含层中第 1 个神经元 $y1=f1(e)$ 对应的值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/14.png"></center><p>接下来，计算第 1 个隐含层中第 2 个神经元 $y2=f2(e)$ 对应的值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/15.png"></center><p>然后是计算第 1 个隐含层中第 3 个神经元 $y3=f3(e)$ 对应的值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/16.png"></center><p>与计算第 1 个隐含层的过程相似，我们可以计算第 2 个隐含层的数值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/17.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/18.png"></center><p>最后，得到输出层的结果：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/19.png"></center><p>上面这个过程被称为前向传播过程，那什么是反向传播呢？接着来看：</p><p>当我们得到输出结果 $y$ 时，可以与期望输出 $z$ 对比得到误差 $δ$。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/20.png"></center><p>然后，我们将计算得到的误差 $δ$ 沿着神经元回路反向传递到前 1 个隐含层，而每个神经元对应的误差为传递过来的误差乘以权重。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/21.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/22.png"></center><p>同理，我们将第 2 个隐含层的误差继续向第 1 个隐含层反向传递。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/23.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/24.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/25.png"></center><p>此时，我们就可以利用反向传递过来的误差对从输入层到第 1 个隐含层之间的权值 $w$ 进行更新，如下图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/26.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/27.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/28.png"></center><p>同样，对第 1 个隐含层与第 2 个隐含层之间的权值 $w$ 进行更新，如下图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/29.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/30.png"></center><p>最后，更新第 2 个隐含层与输出层之间的权值 $w$ ，如下图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/31.png"></center><p>图中的 $η$ 表示学习速率。这就完成了一个迭代过程。更新完权重之后，又开始下一轮的前向传播得到输出，再反向传播误差更新权重，依次迭代下去。</p><p>所以，反向传播其实代表的是反向传播误差。</p><h3 id="使用-Python-实现人工神经网络"><a href="#使用-Python-实现人工神经网络" class="headerlink" title="使用 Python 实现人工神经网络"></a>使用 Python 实现人工神经网络</h3><p>上面的内容，我们介绍了人工神经网络的构成和最重要的反向传播算法。接下来，尝试通过 Python 来实现一个神经网络运行的完整流程。</p><h4 id="定义神经网络结构"><a href="#定义神经网络结构" class="headerlink" title="定义神经网络结构"></a>定义神经网络结构</h4><p>为了让推导过程足够清晰，这里我们只构建包含 1 个隐含层的人工神经网络结构。其中，输入层为 2 个神经元，隐含层为 3 个神经元，并通过输出层实现 2 分类问题的求解。该神经网络的结构如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/32.png"></center><p>在此中，我们使用的激活函数为 $sigmoid$ 函数：</p><p>$$<br>\mathit{sigmoid}(x) = \frac{1}{1+e^{-x}}       \tag{17a}<br>$$</p><p>由于下面要使用 $sigmoid$ 函数的导数，所以同样将其导数公式写出来：</p><p>$$<br>\Delta \mathit{sigmoid}(x)  = \mathit{sigmoid}(x)(1 - \mathit{sigmoid}(x))    \tag{17b}<br>$$</p><p>然后，我们通过 Python 实现公式（17）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid 函数求导</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(x) * (<span class="number">1</span> - sigmoid(x))</span><br></pre></td></tr></table></figure><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>前向（正向）传播中，每一个神经元的计算流程为：<strong>线性变换 → 激活函数→输出值</strong>。</p><p>同时，我们约定：</p><ul><li>$Z$ 表示隐含层输出，$Y$ 则为输出层最终输出。</li><li>$w_ij$ 表示从第 $i$ 层的第 $j$ 个权重。</li></ul><p>于是，上图中的前向传播的代数计算过程如下。</p><p>神经网络的输入 $X$，第一层权重 $W_1$，第二层权重 $W_2$。为了演示方便，$X$ 为单样本，因为是矩阵运算，我们很容易就能扩充为多样本输入。</p><p>$$<br>X = \begin{bmatrix} x_{1} &amp; x_{2} \end{bmatrix} \tag{18}<br>$$</p><p>$$<br>W_1 = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13}\  w_{14} &amp; w_{15} &amp; w_{16}\  \end{bmatrix} \tag{19}<br>$$</p><p>$$<br>W_2 = \begin{bmatrix} w_{21} \ w_{22} \ w_{23} \end{bmatrix} \tag{20}<br>$$</p><p>接下来，计算隐含层神经元输出 $Z$（线性变换 → 激活函数）。同样，为了使计算过程足够清晰，我们这里将截距项表示为 0。</p><p>$$<br>Z = \mathit{sigmoid}(X \cdot W_{1}) \tag{21}<br>$$</p><p>最后，计算输出层 $Y$（线性变换 → 激活函数）：</p><p>$$<br>Y = \mathit{sigmoid}(Z \cdot W_{2}) \tag{22}<br>$$</p><p>下面实现前向传播计算过程，将上面的公式转化为代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例样本</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y = np.array([[<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">X, y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([[1, 1]]), array([[1]])) </span><br></pre></td></tr></table></figure><p>然后，随机初始化隐含层权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W1 = np.random.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">W2 = np.random.rand(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">W1, W2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array([[0.4985742 , 0.16703231, 0.51487393],</span><br><span class="line">        [0.63075313, 0.46386686, 0.44365266]]), array([[0.69320812],</span><br><span class="line">        [0.74352002],</span><br><span class="line">        [0.2403471 ]])) </span><br></pre></td></tr></table></figure><p>前向传播的过程实现基于公式（21）和公式（22）完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input_layer = X <span class="comment"># 输入层</span></span><br><span class="line">hidden_layer = sigmoid(np.dot(input_layer, W1)) <span class="comment"># 隐含层，公式 20</span></span><br><span class="line">output_layer = sigmoid(np.dot(hidden_layer, W2)) <span class="comment"># 输出层，公式 22</span></span><br><span class="line"></span><br><span class="line">output_layer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.76546658]]) </span><br></pre></td></tr></table></figure><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><center><div class="media-wrap"><iframe src="//player.bilibili.com/player.html?aid=16577449&amp;cid=27038098&amp;page=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="450px" width="100%"></iframe></div></center><div style="color: #999;font-size: 12px;text-align: center;">反向传播的微积分原理 | 视频来源：<a href="https://www.bilibili.com/video/av16577449/?p=2" target="_blank" rel="noopener">3Blue1Brown</a></div><p>接下来，我们使用梯度下降法的方式来优化神经网络的参数。那么首先需要定义损失函数，然后计算损失函数关于神经网络中各层的权重的偏导数（梯度）。</p><p>此时，设神经网络的输出值为 <code>Y</code>，真实值为 <code>y</code>。然后，定义平方损失函数如下：</p><p>$$<br>Loss(y, Y) = \sum (y - Y)^2 \tag{23}<br>$$</p><p>接下来，求解梯度 $\frac{\partial Loss(y, Y)}{\partial{W_2}}$，需要使用链式求导法则：</p><p>$$<br>\frac{\partial Loss(y, Y)}{\partial{W_2}} = \frac{\partial Loss(y, Y)}{\partial{Y}} \frac{\partial Y}{\partial{W_2}}\tag{24a}<br>$$</p><p>$$<br>\frac{\partial Loss(y, Y)}{\partial{W_2}} = 2(Y-y) * \Delta \mathit{sigmoid}(Z \cdot W_2) \cdot Z\tag{24b}<br>$$</p><p>同理，梯度 $\frac{\partial Loss(y, Y)}{\partial{W_1}}$ 得：</p><p>$$<br>\frac{\partial Loss(y, Y)}{\partial{W_1}} = \frac{\partial Loss(y, Y)}{\partial{Y}} \frac{\partial Y }{\partial{Z}} \frac{\partial Z}{\partial{W_1}} \tag{25a}<br>$$</p><p>$$<br>\frac{\partial Loss(y, Y)}{\partial{W_1}} = 2(Y-y) * \Delta \mathit{sigmoid}(Z \cdot W_2) \cdot W_2 * \Delta \mathit{sigmoid}(X \cdot W_1) \cdot X \tag{25b}<br>$$</p><p>其中，$\frac{\partial Y}{\partial{W_2}}$，$\frac{\partial Y}{\partial{W_1}}$ 分别通过公式（22）和（21）求得。接下来，我们基于公式对反向传播过程进行代码实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 公式 24</span></span><br><span class="line">d_W2 = np.dot(hidden_layer.T, (<span class="number">2</span> * (output_layer - y) * </span><br><span class="line">              sigmoid_derivative(np.dot(hidden_layer, W2))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 公式 25</span></span><br><span class="line">d_W1 = np.dot(input_layer.T,  (</span><br><span class="line">       np.dot(<span class="number">2</span> * (output_layer - y) * sigmoid_derivative(</span><br><span class="line">       np.dot(hidden_layer, W2)), W2.T) * sigmoid_derivative(np.dot(input_layer, W1))))</span><br><span class="line"></span><br><span class="line">d_W2, d_W1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array([[-0.06363904],</span><br><span class="line">        [-0.05496356],</span><br><span class="line">        [-0.06086952]]), array([[-0.01077667, -0.01419321, -0.00405499],</span><br><span class="line">        [-0.01077667, -0.01419321, -0.00405499]])) </span><br></pre></td></tr></table></figure><p>现在，就可以设置学习率，并对 $W_1$, $W_2$ 进行一次更新了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降更新权重, 学习率为 0.05</span></span><br><span class="line"></span><br><span class="line">W1 -= <span class="number">0.05</span> * d_W1 <span class="comment"># 如果上面是 y - output_layer，则改成 +=</span></span><br><span class="line">W2 -= <span class="number">0.05</span> * d_W2</span><br><span class="line"></span><br><span class="line">d_W2, d_W1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array([[-0.06363904],</span><br><span class="line">        [-0.05496356],</span><br><span class="line">        [-0.06086952]]), array([[-0.01077667, -0.01419321, -0.00405499],</span><br><span class="line">        [-0.01077667, -0.01419321, -0.00405499]])) </span><br></pre></td></tr></table></figure><p>以上，我们就实现了单个样本在神经网络中的 1 次前向 → 反向传递，并使用梯度下降完成 1 次权重更新。那么，下面我们完整实现该网络，并对多样本数据集进行学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例神经网络完整实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X, y, lr</span>):</span></span><br><span class="line">        self.input_layer = X</span><br><span class="line">        self.W1 = np.random.rand(self.input_layer.shape[<span class="number">1</span>], <span class="number">3</span>)</span><br><span class="line">        self.W2 = np.random.rand(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.y = y</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.output_layer = np.zeros(self.y.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.hidden_layer = sigmoid(np.dot(self.input_layer, self.W1))</span><br><span class="line">        self.output_layer = sigmoid(np.dot(self.hidden_layer, self.W2))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self</span>):</span></span><br><span class="line">        d_W2 = np.dot(self.hidden_layer.T, (<span class="number">2</span> * (self.output_layer - self.y) *</span><br><span class="line">                      sigmoid_derivative(np.dot(self.hidden_layer, self.W2))))</span><br><span class="line">        </span><br><span class="line">        d_W1 = np.dot(self.input_layer.T, (</span><br><span class="line">               np.dot(<span class="number">2</span> * (self.output_layer - self.y) * sigmoid_derivative(</span><br><span class="line">               np.dot(self.hidden_layer, self.W2)), self.W2.T) * sigmoid_derivative(</span><br><span class="line">               np.dot(self.input_layer, self.W1))))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        self.W1 -= self.lr * d_W1</span><br><span class="line">        self.W2 -= self.lr * d_W2</span><br></pre></td></tr></table></figure><p>接下来，我们使用实验一开始的示例数据集测试，首先我们要对数据形状进行调整，以满足需要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="string">&#x27;X0&#x27;</span>,<span class="string">&#x27;X1&#x27;</span>]].values <span class="comment"># 输入值</span></span><br><span class="line">y = df[<span class="string">&#x27;Y&#x27;</span>].values.reshape(<span class="built_in">len</span>(X), <span class="number">-1</span>) <span class="comment"># 真实 y，处理成 [[],...,[]] 形状</span></span><br></pre></td></tr></table></figure><p>接下来，我们将其输入到网络中，并迭代 100 次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">nn = NeuralNetwork(X, y, lr=<span class="number">0.001</span>) <span class="comment"># 定义模型</span></span><br><span class="line">loss_list = [] <span class="comment"># 存放损失数值变化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    nn.forward() <span class="comment"># 前向传播</span></span><br><span class="line">    nn.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    loss = np.<span class="built_in">sum</span>((y - nn.output_layer) ** <span class="number">2</span>) <span class="comment"># 计算平方损失</span></span><br><span class="line">    loss_list.append(loss)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;final loss:&quot;</span>, loss)</span><br><span class="line">plt.plot(loss_list) <span class="comment"># 绘制 loss 曲线变化图</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final loss: 133.4221126605534 </span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_239_2.png"></center><p>可以看到，损失函数逐渐减小并接近收敛，变化曲线比感知机计算会平滑很多。不过，由于我们去掉了截距项，且网络结构太过简单，导致收敛情况并不理想。本实验重点再于搞清楚 BP 的中间过程，准确度和学习难度不可两全。另外，需要注意的是由于权重是随机初始化，多次运行的结果会不同。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;人工神经网络是一种发展时间较早且十分常用的机器学习算法。因其模仿人类神经元工作的特点，在监督学习和非监督学习领域都给予了人工神经网络较高的期望。目前，由传统人工神经网络发展而来的卷积神经网络、循环神经网络已经成为了深度学习的基石。本篇文章中，我们将从人工神经网络的原型感知机出发，介绍机器学习中人工神经网络的特点及应用。  &lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Neural Network" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Neural-Network/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Neural Network" scheme="http://www.laugh12321.cn/blog/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>阿里云服务器ECS Ubuntu16.04 + Seafile 搭建私人网盘 （Seafile Pro）</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/24/seafile_pro_settings/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/24/seafile_pro_settings/</id>
    <published>2019-01-24T00:00:00.000Z</published>
    <updated>2020-10-21T06:26:48.071Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要讲述 使用 Ubuntu 16.04 云服务器 通过脚本实现对 Seafile Pro 的安装，完成私人网盘的搭建</p><a id="more"></a><p>首先给出 Seafile 专业版的下载地址（Linux）: 👉 <a href="https://download.seafile.com/d/6e5297246c/?p=/pro&mode=list">传送门</a></p><p>在本机下载好安装包后，通过 <a href="https://winscp.net/eng/download.php">WinSCP</a> 将安装包放在 <code>/opt/ </code> 目录下，并将专业版的安装包重命名为 <code>seafile-pro-server_6.3.11_x86-64.tar.gz</code> 的格式（方便安装）。这里使用的安装方式是使用官方给出的 <a href="https://github.com/haiwen/seafile-server-installer-cn">Seafile 安装脚本</a> 安装，优点是一步到位，坏处是安装失败需要还原到镜像。</p><h3 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h3><p>安装干净的 16.04 或 CentOS 7 系统，并<strong>做好镜像</strong> (如果安装失败需要还原到镜像)。</p><p>切换成 root 账号 (sudo -i)</p><h4 id="获取安装脚本"><a href="#获取安装脚本" class="headerlink" title="获取安装脚本"></a>获取安装脚本</h4><p>Ubuntu 16.04（适用于 6.0.0 及以上版本）:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;haiwen&#x2F;seafile-server-installer-cn&#x2F;master&#x2F;seafile-server-ubuntu-16-04-amd64-http</span><br></pre></td></tr></table></figure><h4 id="运行安装脚本并指定要安装的版本-6-3-11"><a href="#运行安装脚本并指定要安装的版本-6-3-11" class="headerlink" title="运行安装脚本并指定要安装的版本 (6.3.11)"></a>运行安装脚本并指定要安装的版本 (6.3.11)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash seafile-server-ubuntu-16-04-amd64-http 6.3.11</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_00.png"></p><p>输入 <code>2</code> 选择安装专业版</p><p>该脚本运行完后会在命令行中打印配置信息和管理员账号密码，请仔细阅读。(你也可以查看安装日志 <code>/opt/seafile/aio_seafile-server.log</code> )，MySQL 密码在 <code>/root/.my.cnf</code> 中。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_01.png"></p><h4 id="通过-Web-UI-对服务器进行配置"><a href="#通过-Web-UI-对服务器进行配置" class="headerlink" title="通过 Web UI 对服务器进行配置"></a>通过 Web UI 对服务器进行配置</h4><p>安装完成后，需要通过 Web UI 服务器进行基本的配置，以便能正常的从网页端进行文件的上传和下载：</p><ol><li><p>首先在浏览器中输入服务器的地址，并用管理员账号和初始密码登录</p></li><li><p>点击界面的右上角的头像按钮进入管理员界面</p><p> <img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_04.png"></p></li><li><p>进入设置页面填写正确的服务器对外的 SERVICE_URL 和 FILE_SERVER_ROOT，比如</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SERVICE_URL: http:&#x2F;&#x2F;126.488.125.111：8000</span><br><span class="line">FILE_SERVER_ROOT: &#39;http:&#x2F;&#x2F;126.488.125.111&#x2F;seafhttp&#39;</span><br></pre></td></tr></table></figure><p><strong><font color='red'>注意：</font></strong> <code>126.488.125.111</code> 是你服务器的公网 <code>ip</code></p></li></ol><p>对了，还要在还要在 <code>云服务器管理控制台</code> 设置新的安全组规则（<code>8082</code> 和 <code>80</code> 端口），可以参考下图自行配置</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_09.png"></p><p>现在可以退出管理员界面，并进行基本的测试。关于服务器的配置选项介绍和日常运维可以参考 <a href="http://manual-cn.seafile.com/config/index.html">http://manual-cn.seafile.com/config/index.html</a></p><h4 id="在本地打开-Web-UI"><a href="#在本地打开-Web-UI" class="headerlink" title="在本地打开 Web UI"></a>在本地打开 Web UI</h4><p>因为使用一键安装脚本安装，默认使用了 <code>nginx</code> 做反向代理，并且开启了防火墙，所以你需要直接通过 <code>80</code> 端口访问，而不是 <code>8000</code> 端口。</p><p><strong><font color='red'>注意：</font></strong>在本地输入的 <code>ip</code> 地址是你的云服务器的公网 <code>IP</code></p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_05.png"></p><h4 id="通过客户端登陆"><a href="#通过客户端登陆" class="headerlink" title="通过客户端登陆"></a>通过客户端登陆</h4><h5 id="Windows-客户端登陆"><a href="#Windows-客户端登陆" class="headerlink" title="Windows 客户端登陆"></a>Windows 客户端登陆</h5><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_06.png"></p><h5 id="Android-客户端登陆"><a href="#Android-客户端登陆" class="headerlink" title="Android 客户端登陆"></a>Android 客户端登陆</h5><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_08.jpg"></p><hr><p>至此，私人云盘已经搭建完毕</p><p>更多详细步骤请阅读： 👉 <a href="https://github.com/haiwen/seafile-server-installer-cn">官方脚本说明</a></p><article class="message message-immersive is-primary"><div class="message-body"><i class="fas fa-globe-americas mr-2"></i>更多详细步骤请阅读：<a href="https://github.com/haiwen/seafile-server-installer-cn">官方脚本说明</a>.</div></article><article class="message message-immersive is-warning"><div class="message-body"><i class="fas fa-question-circle mr-2"></i>文章内容有误？请点击<a href="https://github.com/ppoffice/hexo-theme-icarus/edit/site/source/_posts/zh-CN/Configuring-Theme.md">此处</a>提交修改。</div></article><p><a style="background-color:black;color:white;text-decoration:none;padding:4px 6px;font-size:12px;line-height:1.2;display:inline-block;border-radius:3px" href="https://www.vecteezy.com/free-vector/vector-landscape" target="_blank" rel="noopener noreferrer" title="Vector Landscape Vectors by Vecteezy"><span style="display:inline-block;padding:2px 3px"><svg xmlns="http://www.w3.org/2000/svg" style="height:12px;width:auto;position:relative;vertical-align:middle;top:-1px;fill:white" viewBox="0 0 32 32"><path d="M20.8 18.1c0 2.7-2.2 4.8-4.8 4.8s-4.8-2.1-4.8-4.8c0-2.7 2.2-4.8 4.8-4.8 2.7.1 4.8 2.2 4.8 4.8zm11.2-7.4v14.9c0 2.3-1.9 4.3-4.3 4.3h-23.4c-2.4 0-4.3-1.9-4.3-4.3v-15c0-2.3 1.9-4.3 4.3-4.3h3.7l.8-2.3c.4-1.1 1.7-2 2.9-2h8.6c1.2 0 2.5.9 2.9 2l.8 2.4h3.7c2.4 0 4.3 1.9 4.3 4.3zm-8.6 7.5c0-4.1-3.3-7.5-7.5-7.5-4.1 0-7.5 3.4-7.5 7.5s3.3 7.5 7.5 7.5c4.2-.1 7.5-3.4 7.5-7.5z"></path></svg></span><span style="display:inline-block;padding:2px 3px">Vector Landscape Vectors by Vecteezy</span></a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要讲述 使用 Ubuntu 16.04 云服务器 通过脚本实现对 Seafile Pro 的安装，完成私人网盘的搭建&lt;/p&gt;</summary>
    
    
    
    <category term="Ubuntu" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/"/>
    
    <category term="ECS" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/ECS/"/>
    
    <category term="Seafile" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/ECS/Seafile/"/>
    
    
    <category term="Ubuntu" scheme="http://www.laugh12321.cn/blog/tags/Ubuntu/"/>
    
    <category term="ECS" scheme="http://www.laugh12321.cn/blog/tags/ECS/"/>
    
    <category term="服务器" scheme="http://www.laugh12321.cn/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
    <category term="Seafile" scheme="http://www.laugh12321.cn/blog/tags/Seafile/"/>
    
  </entry>
  
  <entry>
    <title>阿里云服务器ECS Ubuntu16.04 初次使用配置教程(图形界面安装)</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/23/ubuntu16.04_setting/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/23/ubuntu16.04_setting/</id>
    <published>2019-01-23T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>前一阵子购买了阿里云的云服务器ECS（学生优惠），折腾了一阵子后对有些东西不太满意，所以就重新初始化了磁盘，刚好要重新安装图形界面，于是就顺手写了这么一篇文章。</p><a id="more"></a><h3 id="首次登陆"><a href="#首次登陆" class="headerlink" title="首次登陆"></a>首次登陆</h3><p>第一次登陆服务器时，是这个样子的：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_00.png"></p><h3 id="输入账号"><a href="#输入账号" class="headerlink" title="输入账号"></a>输入账号</h3><p>在  <code>login:</code> 后输入 <code>root</code> , 会出现 <code>Password：</code>， 然后输入你的实例密码<br><strong><font color="red">注意：</font></strong>你输入的密码是不会显示出来的<br>输入成功后效果如下：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_01.png"></p><h3 id="输入指令"><a href="#输入指令" class="headerlink" title="输入指令"></a>输入指令</h3><p>然后依次输入下面的命令（期间需要手动确认三次）：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 更新软件库</span><br><span class="line"><span class="selector-tag">apt-get</span> <span class="selector-tag">update</span></span><br><span class="line"></span><br><span class="line"># 升级软件</span><br><span class="line"><span class="selector-tag">apt-get</span> <span class="selector-tag">upgrade</span></span><br><span class="line"></span><br><span class="line"># 安装桌面系统</span><br><span class="line"><span class="selector-tag">apt-get</span> <span class="selector-tag">install</span> <span class="selector-tag">ubuntu-desktop</span></span><br></pre></td></tr></table></figure><p>输入<code>apt-get update</code> 后，效果如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_02.png"></p><p>然后输入 <code>apt-get upgrade</code> ，期间需要输入 <code>y</code> 进行确认，如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_03.png"></p><p>然后进行第二次确认，选择默认选项，如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_04.png"></p><p>软件升级完成后如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_05.png"></p><p>接下来就可以安装图形界面了，我们输入 <code>apt-get install ubuntu-desktop</code> 指令，输入后还要进行最后一次手动确认如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_06.png"></p><p>输入 <code>y</code> 即可，等到图形界面安装完成输入 <code>reboot</code> 指令进行重启，如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_07.png"></p><p>重启后可以发现我们是以访客身份登陆的，而且不能选择登陆用户并且不需要密码就可以登陆，登陆后还会出现警告信息。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_08.png"></p><p>桌面警告：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_09.png"></p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>打开 <code>usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf</code> 文件并修改</p><p>修改前：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_10.png"></p><p>修改后：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_11.png"></p><p>代码如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[Seat:*]</span></span><br><span class="line">user-session=ubuntu</span><br><span class="line">greeter-show-manual-login=true</span><br><span class="line">allow-guest=false</span><br></pre></td></tr></table></figure><p>重启服务器后可以用 <code>root</code> 用户登录，但是登录还是有警告，这个需要修改 <code>/root/.profile</code> 文件</p><p>修改前：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_12.png"></p><p>修改后：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_13.png"></p><p>代码如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># ~/.profile: executed by Bourne-compatible login shells.</span><br><span class="line"><span class="selector-tag">if</span> <span class="selector-attr">[ <span class="string">&quot;$BASH&quot;</span> ]</span>; <span class="selector-tag">then</span></span><br><span class="line">  <span class="selector-tag">if</span> <span class="selector-attr">[ -f ~/.bashrc ]</span>; <span class="selector-tag">then</span></span><br><span class="line">    . ~/.bashrc</span><br><span class="line">  <span class="selector-tag">fi</span></span><br><span class="line"><span class="selector-tag">fi</span></span><br><span class="line">tty -s &amp;&amp; mesg n || true</span><br></pre></td></tr></table></figure><p>重启后只有root用户，登录后没有警告信息。 </p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_14.png"></p><p>至此，<strong>服务器端安装桌面环境结束</strong>。</p><hr><p>参考博客：<a href="https://blog.csdn.net/qq_37608398/article/details/78155568">阿里云服务器ECS Ubuntu16.04-64-bit学习之一：配置桌面</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前一阵子购买了阿里云的云服务器ECS（学生优惠），折腾了一阵子后对有些东西不太满意，所以就重新初始化了磁盘，刚好要重新安装图形界面，于是就顺手写了这么一篇文章。&lt;/p&gt;</summary>
    
    
    
    <category term="Ubuntu" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/"/>
    
    <category term="ECS" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/ECS/"/>
    
    
    <category term="Ubuntu" scheme="http://www.laugh12321.cn/blog/tags/Ubuntu/"/>
    
    <category term="ECS" scheme="http://www.laugh12321.cn/blog/tags/ECS/"/>
    
    <category term="服务器" scheme="http://www.laugh12321.cn/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>机器学习| 支持向量机详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/20/support_vector_machine/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/20/support_vector_machine/</id>
    <published>2019-01-20T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。</p><a id="more"></a><h2 id="线性分类支持向量机"><a href="#线性分类支持向量机" class="headerlink" title="线性分类支持向量机"></a>线性分类支持向量机</h2><p>在逻辑回归中，我们尝试通过一条直线针对线性可分数据完成分类。同时，通过最小化对数损失函数来找到最优分割边界，也就是下图中的紫色直线。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/1.png"></p><p>逻辑回归是一种简单高效的线性分类方法。而在这里中，我们将接触到另一种针对线性可分数据进行分类的思路，并把这种方法称之为支持向量机（英语：Support vector machine，简称：SVM）。</p><p>如果你第一次接触支持向量机这个名字，可能会感觉读起来比较拗口。至少我当时初次接触支持向量机时，完全不知道为什么会有这样一个怪异的名字。假如你和当时的我一样，那么当你看完下面这段介绍内容后，就应该会对支持向量机这个名词有更深刻的认识了。</p><h3 id="支持向量机分类特点"><a href="#支持向量机分类特点" class="headerlink" title="支持向量机分类特点"></a>支持向量机分类特点</h3><p>假设给定一个训练数据集 $T=\lbrace(x_1,y_1),(x_2,y_2),\cdots ,(x_n,y_n)\rbrace$ 。同时，假定已经找到样本空间中的分割平面，其划分公式可以通过以下线性方程来描述：<br>$$<br>wx+b=0\tag{1}<br>$$<br>使用一条直线对线性可分数据集进行分类的过程中，我们已经知道这样的直线可能有很多条：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/4.png"></p><p>问题来了！<strong>哪一条直线是最优的划分方法呢？</strong></p><p>在逻辑回归中，我们引入了 S 形曲线和对数损失函数进行优化求解。如今，支持向量机给了一种从几何学上更加直观的方法进行求解，如下图所示：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/2.png"></p><p>上图展示了支持向量机分类的过程。图中 $wx-b=0$ 为分割直线，我们通过这条直线将数据点分开。与此同时，分割时会在直线的两边再设立两个互相平行的虚线，这两条虚线与分割直线的距离一致。这里的距离往往也被我们称之为「间隔」，而支持向量机的分割特点在于，要使得<strong>分割直线和虚线之间的间隔最大化</strong>。同时也就是两虚线之间的间隔最大化。</p><p>对于线性可分的正负样本点而言，位于 $wx-b=1$ 虚线外的点就是正样本点，而位于 $wx-b=-1$ 虚线外的点就是负样本点。另外，正好位于两条虚线上方的样本点就被我们称为支持向量，这也就是支持向量机的名字来源。</p><h3 id="支持向量机分类演示"><a href="#支持向量机分类演示" class="headerlink" title="支持向量机分类演示"></a>支持向量机分类演示</h3><p>下面，我们使用 Python 代码来演示支持向量机的分类过程。</p><p>首先，我们介绍一种新的示例数据生成方法。即通过 scikit-learn 提供的 <code>samples_generator()</code> 类完成。通过 <code>samples_generator()</code> 类下面提供的不同方法，可以产生不同分布状态的示例数据。首先要用到 <code>make_blobs</code> 方法，该方法可以生成团状数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> samples_generator</span><br><span class="line"></span><br><span class="line">x, y = samples_generator.make_blobs(n_samples=<span class="number">60</span>, centers=<span class="number">2</span>, random_state=<span class="number">30</span>, cluster_std=<span class="number">0.8</span>) <span class="comment"># 生成示例数据</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>)) <span class="comment"># 绘图</span></span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_00.png"></p><p>接下来，我们在示例数据中绘制任意 3 条分割线把示例数据分开。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 3 条不同的分割线</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">for</span> m, b <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">-8</span>), (<span class="number">0.5</span>, <span class="number">-6.5</span>), (<span class="number">-0.2</span>, <span class="number">-4.25</span>)]:</span><br><span class="line">    y_temp = m * x_temp + b</span><br><span class="line">    plt.plot(x_temp, y_temp, <span class="string">&#x27;-k&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_01.png"></p><p>然后，可以使用 <code>fill_between</code> 方法手动绘制出分类硬间隔。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 3 条不同的分割线</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">for</span> m, b, d <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">-8</span>, <span class="number">0.2</span>), (<span class="number">0.5</span>, <span class="number">-6.5</span>, <span class="number">0.55</span>), (<span class="number">-0.2</span>, <span class="number">-4.25</span>, <span class="number">0.75</span>)]:</span><br><span class="line">    y_temp = m * x_temp + b</span><br><span class="line">    plt.plot(x_temp, y_temp, <span class="string">&#x27;-k&#x27;</span>)</span><br><span class="line">    plt.fill_between(x_temp, y_temp - d, y_temp + d, color=<span class="string">&#x27;#f3e17d&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_02.png"></p><div style="color: #999;font-size: 12px;font-style: italic;"><strong>上图为了呈现出分类间隔的效果，手动指定了参数。</strong></div><p>可以看出，不同的分割线所对应的间隔大小是不一致的，而支持向量机的目标是找到最大的分类硬间隔所对应的分割线。</p><h3 id="硬间隔表示及求解"><a href="#硬间隔表示及求解" class="headerlink" title="硬间隔表示及求解"></a>硬间隔表示及求解</h3><p>我们已经知道支持向量机是根据最大间隔来划分，下面考虑如何求得一个几何间隔最大的分割线。</p><p>对于线性可分数据而言，几何间隔最大的分离超平面是唯一的，这里的间隔也被我们称之为「硬间隔」，而间隔最大化也就称为硬间隔最大化。上图实际上就是硬间隔的典型例子。</p><p>最大间隔分离超平面，我们希望最大化超平面 $(w,b)$ 关于训练数据集的几何间隔 $\gamma$，满足以下约束条件：每个训练样本点到超平面 $(w,b)$ 的几何间隔至少都是 $\gamma$ ，因此可以转化为以下的约束最优化问题： </p><p>$$<br>\max\limits_{w,b}\gamma =\frac{2}{\left |w\right |} \tag{2a}<br>$$</p><p>$$<br>\begin{equation}<br>\textrm s.t. y_i(\frac{w}{\left |w\right |}x_i+\frac{b}{\left |w\right |})\geq \frac{\gamma}{2} \tag{2b}<br>\end{equation}<br>$$</p><p>实际上，$\gamma$ 的取值并不会影响最优化问题的解，同时，我们根据数学对偶性原则，可以得到面向硬间隔的线性可分数据的支持向量机的最优化问题：<br>$$<br>\min\limits_{w,b}\frac{1}{2}\left |w\right |^2 \tag{3a}<br>$$</p><p>$$<br>\begin{equation}<br>\textrm s.t. y_i(wx_i+b)-1\geq 0\tag{3b}<br>\end{equation}<br>$$</p><p>我们通常使用拉格朗日乘子法来求解最优化问题，将原始问题转化为对偶问题，通过解对偶问题得到原始问题的解。对公式（3）使用拉格朗日乘子法可得到其「对偶问题」。具体来说，对每条约束添加拉格朗日乘子 $\alpha_i \geq 0$，则该问题的拉格朗日函数可写为：<br>$$<br>L(w,b,\alpha)=\frac{1}{2}\left | w\right |^2+\sum\limits_{i=1}^{m}\alpha_i(1-y_i(wx_i+b)) \tag{4}<br>$$<br>我们通过将公式（4）分别对 $w$ 和 $b$ 求偏导为 <code>0</code> 并代入原式中，可以将 $w$ 和 $b$ 消去，得到公式（3）的对偶问题：<br>$$<br>\max\limits_{\alpha} \sum\limits_{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j x_i x_j \tag{5a}<br>$$</p><p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_i y_i=0,\tag{5b}<br>$$</p><p>$$<br>\alpha_i \geq 0,i=1,2,\cdots,N  \tag{5c}<br>$$</p><p>解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{6}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{7}<br>$$</p><h3 id="软间隔表示及求解"><a href="#软间隔表示及求解" class="headerlink" title="软间隔表示及求解"></a>软间隔表示及求解</h3><p>上面，我们介绍了线性可分条件下的最大硬间隔的推导求解方法。在很多时候，我们还会遇到下面这种情况。你可以发现，在实心点和空心点中各混入了零星的不同类别的数据点。对于这种情况，数据集就变成了严格意义上的线性不可分。但是，造成这种线性不可分的原因往往是因为包含「噪声」数据，它同样可以被看作是不严格条件下的线性可分。</p><p>当我们使用支持向量机求解这类问题时，就会把最大间隔称之为最大「软间隔」，而软间隔就意味着可以容许零星噪声数据被误分类。</p><p><img width='300px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/3.png"></img></p><p>当出现上图所示的样本点不是严格线性可分的情况时，某些样本点 $(x_i,y_i)$ 就不能满足函数间隔 $\geqslant 1$ 的约束条件，即公式（3b）中的约束条件。为了解决这个问题，可以对每个样本点 $(x_i,y_i)$ 引入一个松弛变量 $\xi_i \geq 0$，使得函数间隔加上松弛变量 $\geqslant 1$，即约束条件转化为：<br>$$<br>y_i(wx_i+b) \geq 1-\xi_i \tag{8}<br>$$<br>同时，对每个松弛变量 $\xi_i$ 支付一个代价 $\xi_i$，目标函数由原来的 $\frac{1}{2}||w||^2$ 变成：<br>$$<br>\frac{1}{2}\left | w \right |^2+C\sum\limits_{j=1}^{N}\xi_i \tag{9}<br>$$<br>这里，$C&gt;0$ 称为惩罚参数，一般根据实际情况确定。$C$ 值越大对误分类的惩罚增大，最优化问题即为：<br>$$<br>\min\limits_{w,b,\xi} \frac{1}{2}\left | w \right |^2+C\sum\limits_{i=1}^{N}\xi_i \tag{10a}<br>$$</p><p>$$<br>s.t. y_i(wx_i+b) \geq 1-\xi_i,i=1,2,…,N \tag{10b}<br>$$</p><p>$$<br>\xi_i\geq 0,i=1,2,…,N \tag{10c}<br>$$</p><p>这就是软间隔支持向量机的表示过程。同理，我们可以使用拉格朗日乘子法将其转换为对偶问题求解：<br>$$<br>\max\limits_{\alpha}  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i*x_j)-\sum\limits_{i=1}^{N}\alpha_i \tag{11a}<br>$$</p><p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_iy_i=0 \tag{11b}<br>$$</p><p>$$<br>0 \leq \alpha_i \leq C ,i=1,2,…,N\tag{11c}<br>$$</p><p>解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{12}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{13}<br>$$</p><h3 id="线性支持向量机分类实现"><a href="#线性支持向量机分类实现" class="headerlink" title="线性支持向量机分类实现"></a>线性支持向量机分类实现</h3><p>上面，我们对硬间隔和软间隔支持向量机的求解过程进行了推演，推导过程比较复杂不需要完全掌握，但至少要知道硬间隔和软间隔区别。接下来，我们就使用 Python 对支持向量机找寻最大间隔的过程进行实战。由于支持向量机纯 Python 实现太过复杂，所以本次直接使用 scikit-learn 完成。</p><p>scikit-learn 中的支持向量机分类器对应的类及参数为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto&#x27;</span>, coef0=<span class="number">0.0</span>, shrinking=<span class="literal">True</span>, probability=<span class="literal">False</span>, tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, verbose=<span class="literal">False</span>, max_iter=<span class="number">-1</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>主要的参数如下：</p><ul><li><code>C</code>: 软间隔支持向量机对应的惩罚参数，详见公式（9）.</li><li><code>kernel</code>: 核函数，linear, poly, rbf, sigmoid, precomputed 可选，下文详细介绍。</li><li><code>degree</code>: poly 多项式核函数的指数。</li><li><code>tol</code>: 收敛停止的容许值。</li></ul><p>这里，我们还是使用上面生成的示例数据训练支持向量机模型。由于是线性可分数据，<code>kernel</code> 参数指定为 <code>linear</code> 即可。</p><p>首先，训练支持向量机线性分类模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">linear_svc = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">linear_svc.fit(x, y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,</span><br><span class="line">  decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto_deprecated&#x27;</span>,</span><br><span class="line">  kernel=<span class="string">&#x27;linear&#x27;</span>, max_iter=<span class="number">-1</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">  shrinking=<span class="literal">True</span>, tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>对于训练完成的模型，我们可以通过 <code>support_vectors_</code> 属性输出它对应的支持向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear_svc.support_vectors_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ <span class="number">2.57325754</span>, <span class="number">-3.92687452</span>],</span><br><span class="line">       [ <span class="number">2.49156506</span>, <span class="number">-5.96321164</span>],</span><br><span class="line">       [ <span class="number">4.62473719</span>, <span class="number">-6.02504452</span>]])</span><br></pre></td></tr></table></figure><p>可以看到，一共有 <code>3</code> 个支持向量。如果你输出 <code>x, y</code> 的坐标值，就能看到这 <code>3</code> 个支持向量所对应的数据。</p><p>接下来，我们可以使用 Matplotlib 绘制出训练完成的支持向量机对于的分割线和间隔。为了方便后文重复使用，这里将绘图操作写入到 <code>svc_plot()</code> 函数中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svc_plot</span>(<span class="params">model</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取到当前 Axes 子图数据，并为绘制分割线做准备</span></span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    x = np.linspace(ax.get_xlim()[<span class="number">0</span>], ax.get_xlim()[<span class="number">1</span>], <span class="number">50</span>)</span><br><span class="line">    y = np.linspace(ax.get_ylim()[<span class="number">0</span>], ax.get_ylim()[<span class="number">1</span>], <span class="number">50</span>)</span><br><span class="line">    Y, X = np.meshgrid(y, x)</span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用轮廓线方法绘制分割线</span></span><br><span class="line">    ax.contour(X, Y, P, colors=<span class="string">&#x27;green&#x27;</span>, levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], linestyles=[<span class="string">&#x27;--&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;--&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 标记出支持向量的位置</span></span><br><span class="line">    ax.scatter(model.support_vectors_[:, <span class="number">0</span>], model.support_vectors_[:, <span class="number">1</span>], c=<span class="string">&#x27;green&#x27;</span>, s=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制最大间隔支持向量图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">svc_plot(linear_svc)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_03.png"></p><p>如上图所示，绿色实线代表最终找到的分割线，绿色虚线之间的间隔也就是最大间隔。同时，绿色实心点即代表 <code>3</code> 个支持向量的位置。</p><p>上面的数据点可以被线性可分，所以得到的也就是硬间隔支持向量机的分类结果。那么，如果我们加入噪声使得数据集变成不完美线性可分，结果会怎么样呢？</p><p>接下来，我们就来还原软间隔支持向量机的分类过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向原数据集中加入噪声点</span></span><br><span class="line">x = np.concatenate((x, np.array([[<span class="number">3</span>, <span class="number">-4</span>], [<span class="number">4</span>, <span class="number">-3.8</span>], [<span class="number">2.5</span>, <span class="number">-6.3</span>], [<span class="number">3.3</span>, <span class="number">-5.8</span>]])))</span><br><span class="line">y = np.concatenate((y, np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_04.png"></p><p>可以看到，此时的红蓝数据团中各混入了两个噪声点。</p><p>训练软间隔支持向量机模型并绘制成分割线和最大间隔：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">linear_svc.fit(x, y) <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">svc_plot(linear_svc)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_05.png"></p><p>由于噪声点的混入，此时支持向量的数量由原来的 <code>3</code> 个变成了 <code>13</code> 个。</p><p>前面的实验中，我们提到了惩罚系数 $C$，下面可以通过更改 $C$ 的取值来观察支持向量的变化过程。与此同时，我们要引入一个可以在 Notebook 中实现交互操作的模块。你可以通过选择不同的 $C$ 查看最终绘图的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact</span><br><span class="line"><span class="keyword">import</span> ipywidgets <span class="keyword">as</span> widgets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_c</span>(<span class="params">c</span>):</span></span><br><span class="line">    linear_svc.C = c</span><br><span class="line">    linear_svc.fit(x, y)</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">    svc_plot(linear_svc)</span><br><span class="line">    </span><br><span class="line">interact(change_c, c=[<span class="number">1</span>, <span class="number">10000</span>, <span class="number">1000000</span>])</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_06.png"></p><h2 id="非线性分类支持向量机"><a href="#非线性分类支持向量机" class="headerlink" title="非线性分类支持向量机"></a>非线性分类支持向量机</h2><p>上面的内容中，我们假设样本是线性可分或不严格线性可分，然后通过支持向量机建立最大硬间隔或软间隔实现样本分类。然而，线性可分的样本往往只是理想情况，现实中的原始样本大多数情况下是线性不可分。此时，还能用支持向量机吗？</p><p>其实，对于线性不可分的数据集，我们也可以通过支持向量机去完成分类。但是，这里需要增加一个技巧把线性不可分数据转换为线性可分数据之后，再完成分类。</p><p>与此同时，<strong>我们把这种数据转换的技巧称作「核技巧」，实现数据转换的函数称之为「核函数」</strong>。</p><h3 id="核技巧与核函数"><a href="#核技巧与核函数" class="headerlink" title="核技巧与核函数"></a>核技巧与核函数</h3><p>根据上面的介绍，我们提到一个思路就是核技巧，即先把线性不可分数据转换为线性可分数据，然后再使用支持向量机去完成分类。那么，具体是怎样操作呢？</p><div style="text-align:center;color:blue;"><i>核技巧的关键在于空间映射，即将低维数据映射到高维空间中，使得数据集在高维空间能被线性可分。</i></div><div style="color: #999;font-size: 12px;font-style: italic;">* 核技巧是一种数学方法，仅针对于其在支持向量机中的应用场景进行讲解。</div><p><img width='500px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/5.png"></img></p><p>如上图所示，假设我们在二维空间中有蓝色和红色代表的两类数据点，很明显无法使用一条直线把这两类数据分开。此时，如果我们使用核技巧将其映射到三维空间中，就变成了可以被平面线性可分的状态。</p><p>对于「映射」过程，我们还可以这样理解：分布在二维桌面上的红蓝小球无法被线性分开，此时将手掌拍向桌面（好疼），小球在力的作用下跳跃到三维空间中，这也就是一个直观的映射过程。</p><p>同时，「映射」的过程也就是通过核函数转换的过程。这里需要补充说明一点，那就是将数据点从低维度空间转换到高维度空间的方法有很多，但往往涉及到庞大的计算量，而数学家们从中发现了几种特殊的函数，这类函数能大大降低计算的复杂度，于是被命名为「核函数」。也就是说，核技巧是一种特殊的「映射」技巧，而核函数是核技巧的实现方法。</p><p>下面，我们就认识几种常见的核函数：</p><h4 id="线性核函数"><a href="#线性核函数" class="headerlink" title="线性核函数"></a>线性核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=x_i*x_j \tag{14}<br>$$</p><h4 id="多项式核函数"><a href="#多项式核函数" class="headerlink" title="多项式核函数"></a>多项式核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=\left ( x_i*x_j \right )^d, d \geq 1 \tag{15}<br>$$</p><h4 id="高斯径向基核函数"><a href="#高斯径向基核函数" class="headerlink" title="高斯径向基核函数"></a>高斯径向基核函数</h4><p>$$<br>k\left ( x_i, x_j \right ) = \exp \left(-{\frac  {\left |{\mathbf  {x_i}}-{\mathbf  {x_j}}\right |<em>{2}^{2}}{2\sigma ^{2}}}\right)=exp\left ( -\gamma * \left | x_i-x_j \right |</em>{2} ^2 \right ), \gamma&gt;0 \tag{16}<br>$$</p><h4 id="Sigmoid-核函数"><a href="#Sigmoid-核函数" class="headerlink" title="Sigmoid 核函数"></a>Sigmoid 核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=tanh\left ( \beta * x_ix_j+\theta \right ), \beta &gt; 0 , \theta &lt; 0 \tag{17}<br>$$</p><p>这 <code>4</code> 个核函数也就分别对应着上文介绍 <code>sklearn</code> 中 <code>SVC</code> 方法中 <code>kernel</code> 参数的 <code>linear, poly, rbf, sigmoid</code> 等 <code>4</code> 种不同取值。</p><p>此外，核函数还可以通过函数组合得到，例如：</p><p>若 $k_1$ 和 $k_2$ 是核函数，那么对于任意正数 $\lambda_1,\lambda_2$，其线性组合：<br>$$<br>\lambda_1 k_1+\lambda_2 k_2 \tag{18}<br>$$</p><h3 id="引入核函数的间隔表示及求解"><a href="#引入核函数的间隔表示及求解" class="headerlink" title="引入核函数的间隔表示及求解"></a>引入核函数的间隔表示及求解</h3><p>我们通过直接引入核函数 $k(x_i,x_j)$，而不需要显式的定义高维特征空间和映射函数，就可以利用解线性分类问题的方法来求解非线性分类问题的支持向量机。引入核函数以后，对偶问题就变为：<br>$$<br>\max\limits_{\alpha}  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_jy_iy_jk(x_i*x_j)-\sum\limits_{i=1}^{N}\alpha_i \tag{19a}<br>$$</p><p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_iy_i=0 \tag{19b}<br>$$</p><p>$$<br>0 \leq \alpha_i \leq C ,i=1,2,…,N \tag{19c}<br>$$</p><p>同样，解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{20}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{21}<br>$$</p><h3 id="非线性支持向量机分类实现"><a href="#非线性支持向量机分类实现" class="headerlink" title="非线性支持向量机分类实现"></a>非线性支持向量机分类实现</h3><p>同样，我们使用 scikit-learn 中提供的 SVC 类来构建非线性支持向量机模型，并绘制决策边界。</p><p>首先，实验需要生成一组示例数据。上面我们使用了 <code>make_blobs</code> 生成一组线性可分数据，这里使用 <code>make_circles</code> 生成一组线性不可分数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x2, y2 = samples_generator.make_circles(<span class="number">150</span>, factor=<span class="number">.5</span>, noise=<span class="number">.1</span>, random_state=<span class="number">30</span>) <span class="comment"># 生成示例数据</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>)) <span class="comment"># 绘图</span></span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_07.png"></p><p>上图明显是一组线性不可分数据，当我们训练支持向量机模型时就需要引入核技巧。例如，我们这里使用下式做一个简单的非线性映射：<br>$$<br>k\left ( x_i, x_j \right )=x_i^2 + x_j^2 \tag{22}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_function</span>(<span class="params">xi, xj</span>):</span></span><br><span class="line">    poly = xi**<span class="number">2</span> + xj**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> poly</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact, fixed</span><br><span class="line"></span><br><span class="line">r = kernel_function(x2[:,<span class="number">0</span>], x2[:,<span class="number">1</span>])</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">ax = plt.subplot(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.scatter3D(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], r, c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;r&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_08.png"></p><p>上面展示了二维空间点映射到效果维空间的效果。接下来，我们使用 sklearn 中 SVC 方法提供的 RBF 高斯径向基核函数完成实验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbf_svc = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>)</span><br><span class="line">rbf_svc.fit(x2, y2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,</span><br><span class="line">  decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto_deprecated&#x27;</span>,</span><br><span class="line">  kernel=<span class="string">&#x27;rbf&#x27;</span>, max_iter=<span class="number">-1</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">  shrinking=<span class="literal">True</span>, tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line">svc_plot(rbf_svc)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_09.png"></p><p>同样，我们可以挑战不同的惩罚系数 $C$，看一看决策边界和支持向量的变化情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_c</span>(<span class="params">c</span>):</span></span><br><span class="line">    rbf_svc.C = c</span><br><span class="line">    rbf_svc.fit(x2, y2)</span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">    svc_plot(rbf_svc)</span><br><span class="line">    </span><br><span class="line">interact(change_c, c=[<span class="number">1</span>, <span class="number">100</span>, <span class="number">10000</span>])</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_10.png"></p><h2 id="多分类支持向量机"><a href="#多分类支持向量机" class="headerlink" title="多分类支持向量机"></a>多分类支持向量机</h2><p>支持向量机最初是为二分类问题设计的，当我们面对多分类问题时，其实同样可以使用支持向量机解决。而解决的方法就是通过组合多个二分类器来实现多分类器的构造。根据构造的方式又分为 2 种方法：</p><ul><li><strong>一对多法</strong>：即训练时依次把某个类别的样本归为一类，剩余的样本归为另一类，这样 $k$ 个类别的样本就构造出了 $k$ 个支持向量机。</li><li><strong>一对一法</strong>：即在任意两类样本之间构造一个支持向量机，因此 $k$ 个类别的样本就需要设计 $k(k-1) \div 2$ 个支持向量机。</li></ul><p>而在 scikit-learn，实现多分类支持向量机通过设定参数 <code>decision_function_shape</code> 来确定，其中：</p><ul><li><code>decision_function_shape=&#39;ovo&#39;</code>：代表一对一法。</li><li><code>decision_function_shape=&#39;ovr&#39;</code>：代表一对多法。</li></ul><p>由于这里只需要修改参数，所以就不再赘述了。</p><hr><p><strong>拓展阅读：</strong></p><ul><li><p><a href="https://zh.wikipedia.org/zh-hans/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">支持向量机 - 维基百科</a></p></li><li><p>[知乎上关于支持向量机的问题讨论](<a href="https://www.zhihu.com/question/21094489">https://www.zhihu.com/question/21094489</a></p></li><li><p><a href="https://cuijiahua.com/blog/2017/11/ml_8_svm_1.html">机器学习实战教程（八）：支持向量机原理篇之手撕线性SVM</a></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Support Vector Machine" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Support-Vector-Machine/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Support Vector Machine" scheme="http://www.laugh12321.cn/blog/tags/Support-Vector-Machine/"/>
    
  </entry>
  
  <entry>
    <title>机器学习| 朴素贝叶斯详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/19/naive_bayes_basic/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/19/naive_bayes_basic/</id>
    <published>2019-01-19T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。</p><a id="more"></a><h2 id="朴素贝叶斯基础"><a href="#朴素贝叶斯基础" class="headerlink" title="朴素贝叶斯基础"></a>朴素贝叶斯基础</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>朴素贝叶斯的数学理论基础源于概率论。所以，在学习朴素贝叶斯算法之前，首先对其中涉及到的概率论知识做简要讲解。</p><h4 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h4><p>条件概率就是指事件 $A$ 在另外一个事件 $B$ 已经发生条件下的概率。如图所示 ：</p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/1.png" width="370" height="370"><p>其中： </p><ul><li>$P(A)$ 表示 $A$ 事件发生的概率。</li><li>$P(B)$ 表示 $B$ 事件发生的概率。</li><li>$P(AB)$ 表示 $A, B$ 事件同时发生的概率。 </li></ul><p>而最终计算得到的 $P(A \mid B)$ 便是条件概率，表示在 $B$ 事件发生的情况下 $A$ 事件发生的概率。</p><h4 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h4><p>上面提到了条件概率的基本概念，那么当知道事件 $B$ 发生的情况下事件 $A$ 发生的概率 $P(A \mid B)$，如何求 $P(B \mid A)$ 呢？贝叶斯定理应运而生。根据条件概率公式可以得到:</p><p>$$<br>P(B \mid A)=\frac{P(AB)}{P(A)} \tag1<br>$$<br>而同样通过条件概率公式可以得到：</p><p>$$<br>P(AB)=P(A \mid B)*P(B) \tag2<br>$$</p><p>将 (2) 式带入 (1) 式便可得到完整的贝叶斯定理：</p><p>$$<br>P(B \mid A)=\frac{P(AB)}{P(A)}=\frac{P(A \mid B)*P(B)}{P(A)} \tag{3}<br>$$<br>以下，通过一张图来完整且形象的展示条件概率和贝叶斯定理的原理。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/2.png"></p><h4 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a>先验概率</h4><p>先验概率（Prior Probability）指的是根据以往经验和分析得到的概率。例如以上公式中的 $P(A), P(B)$,又例如：$X$ 表示投一枚质地均匀的硬币，正面朝上的概率，显然在我们根据以往的经验下，我们会认为 $X$ 的概率 $P(X) = 0.5$ 。其中 $P(X) = 0.5$ 就是先验概率。</p><h4 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h4><p>后验概率（Posterior Probability）是事件发生后求的反向条件概率；即基于先验概率通过贝叶斯公式求得的反向条件概率。例如公式中的 $P(B \mid A)$   就是通过先验概率 $P(A)$和$P(B)$ 得到的后验概率，其通俗的讲就是「执果寻因」中的「因」。</p><h3 id="什么是朴素贝叶斯"><a href="#什么是朴素贝叶斯" class="headerlink" title="什么是朴素贝叶斯"></a>什么是朴素贝叶斯</h3><p>朴素贝叶斯（Naive Bayes）就是将贝叶斯原理以及条件独立结合而成的算法，其思想非常的简单，根据贝叶斯公式：<br>$$<br>P(B \mid A)=\frac{P(A \mid B)*P(B)}{P(A)} \tag{4}<br>$$<br>变形表达式为：<br>$$<br>P(类别 \mid 特征)=\frac{P(特征 \mid 类别) * P(类别)}{P(特征)} \tag{5}<br>$$<br>公式（5）利用先验概率，即特征和类别的概率；再利用不同类别中各个特征的概率分布，最后计算得到后验概率，即各个特征分布下的预测不同的类别。</p><p>利用贝叶斯原理求解固然是一个很好的方法，但实际生活中数据的特征之间是有相互联系的，在计算 $P(特征\mid类别)$ 时，考虑特征之间的联系会比较麻烦，而朴素贝叶斯则人为的将各个特征割裂开，认定特征之间相互独立。</p><p>朴素贝叶斯中的「朴素」，即条件独立，表示其假设预测的各个属性都是相互独立的,每个属性独立地对分类结果产生影响，条件独立在数学上的表示为：$P(AB)=P(A)*P(B)$。这样，使得朴素贝叶斯算法变得简单，但有时会牺牲一定的分类准确率。对于预测数据，求解在该预测数据的属性出现时各个类别的出现概率，将概率值大的类别作为预测数据的类别。</p><h2 id="朴素贝叶斯算法实现"><a href="#朴素贝叶斯算法实现" class="headerlink" title="朴素贝叶斯算法实现"></a>朴素贝叶斯算法实现</h2><p>前面主要介绍了朴素贝叶斯算法中几个重要的概率论知识，接下来我们对其进行具体的实现，算法流程如下：</p><p><strong>第 1 步</strong>：设<br>$$<br>X = \left { a_{1},a_{2},a_{3},…,a_{n} \right }<br>$$<br>为预测数据，其中 $a_{i}$ 是预测数据的特征值。</p><p><strong>第 2 步</strong>：设<br>$$<br>Y = \left {y_{1},y_{2},y_{3},…,y_{m} \right }<br>$$<br>为类别集合。</p><p><strong>第 3 步</strong>：计算 $P(y_{1}\mid x)$, $P(y_{2}\mid x)$, $P(y_{3}\mid x)$, $…$, $P(y_{m}\mid x)$。</p><p><strong>第 4 步</strong>：寻找 $P(y_{1}\mid x)$, $P(y_{2}\mid x)$, $P(y_{3}\mid x)$, $…$, $P(y_{m}\mid x)$ 中最大的概率 $P(y_{k}\mid x)$ ，则 $x$ 属于类别 $y_{k}$。</p><h3 id="生成示例数据"><a href="#生成示例数据" class="headerlink" title="生成示例数据"></a>生成示例数据</h3><p>下面我们利用 python 完成一个朴素贝叶斯算法的分类。首先生成一组示例数据：由 <code>A</code> 和 <code>B</code>两个类别组成，每个类别包含 <code>x</code>,<code>y</code>两个特征值，其中 <code>x</code> 特征包含<code>r,g,b</code>（红，绿，蓝）三个类别，<code>y</code>特征包含<code>s,m,l</code>（小，中，大）三个类别，如同数据 $X = [g,l]$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;生成示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span>():</span></span><br><span class="line">    data = &#123;<span class="string">&quot;x&quot;</span>: [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>],</span><br><span class="line">            <span class="string">&quot;y&quot;</span>: [<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">            <span class="string">&quot;labels&quot;</span>: [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]&#125;</span><br><span class="line">    data = pd.DataFrame(data, columns=[<span class="string">&quot;labels&quot;</span>, <span class="string">&quot;x&quot;</span>, <span class="string">&quot;y&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>在创建好数据后，接下来进行加载数据，并进行预览。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;加载并预览数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">data = create_data()</span><br><span class="line">data</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/output_00.png"></p><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>根据朴素贝叶斯的原理，最终分类的决策因素是比较 $\left { P(类别 1 \mid 特征),P(类别 2 \mid 特征),…,P(类别 m \mid 特征) \right }$ 各个概率的大小，根据贝叶斯公式得知每一个概率计算的分母 $P(特征)$ 都是相同的，只需要比较分子 $P(类别)$ 和 $P(特征 \mid 类别)$ 乘积的大小。</p><p>那么如何得到 $P(类别)$,以及 $P(特征\mid 类别)$呢？在概率论中，可以应用<strong>极大似然估计法</strong>以及<strong>贝叶斯估计法</strong>来估计相应的概率。</p><h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>什么是极大似然？下面通过一个简单的例子让你有一个形象的了解：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/3.png"></p><blockquote><p><strong>前提条件：</strong>假如有两个外形完全相同箱子，甲箱中有 <code>99</code> 个白球，<code>1</code> 个黑球；乙箱中有 <code>99</code> 个黑球，<code>1</code> 个白球。</p></blockquote><blockquote><p><strong>问题：</strong>当我们进行一次实验，并取出一个球，取出的结果是白球。那么，请问白球是从哪一个箱子里取出的？</p></blockquote><p>我相信，你的第一印象很可能会是白球从甲箱中取出。因为甲箱中的白球数量多，所以这个推断符合人们经验。其中「最可能」就是「极大似然」。而极大似然估计的目的就是利用已知样本结果，反推最有可能造成这个结果的参数值。</p><p>极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：「模型已定，参数未知」。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p><p>在概率论中求解极大似然估计的方法比较复杂，基于实验，我们将讲解 $P(B)$ 和 $P(B/A)$ 是如何通过极大似然估计得到的。$P(种类)$ 用数学的方法表示 ：<br>$$<br>P(y_{i}=c_{k})=\frac{\sum_{N}^{i=1}I(y_{i}=c_{k})}{N},k=1,2,3,…,m \tag{6}<br>$$<br>公式(6)中的 $y_{i}$ 表示数据的类别，$c_{k}$ 表示每一条数据的类别。</p><p>你可以通俗的理解为，在现有的训练集中，每一个类别所占总数的比例，例如:<strong>生成的数据</strong>中 $P(Y=A)=\frac{8}{15}$，表示训练集中总共有 15 条数据，而类别为 <code>A</code> 的有 8 条数据。  </p><p>下面我们用 Python 代码来实现先验概率 $P(种类)$ 的求解：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;P(种类) 先验概率计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_P_labels</span>(<span class="params">labels</span>):</span></span><br><span class="line">    labels = <span class="built_in">list</span>(labels)  <span class="comment"># 转换为 list 类型</span></span><br><span class="line">    P_label = &#123;&#125;  <span class="comment"># 设置空字典用于存入 label 的概率</span></span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        P_label[label] = labels.count(label) / <span class="built_in">float</span>(<span class="built_in">len</span>(labels))  <span class="comment"># p = count(y) / count(Y)</span></span><br><span class="line">    <span class="keyword">return</span> P_label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">P_labels = get_P_labels(data[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line">P_labels</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">0.5333333333333333</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">0.4666666666666667</span>&#125;</span><br></pre></td></tr></table></figure><p>$P(特征 \mid 种类)$ 由于公式较为繁琐这里先不给出，直接用叙述的方式能更清晰地帮助理解：</p><p>实际需要求的先验估计是特征的每一个类别对应的每一个种类的概率，例如：<strong>生成数据</strong> 中 $P(x_{1}=”r” \mid Y=A)=\frac{4}{8}$， <code>A</code> 的数据有 8 条，而在种类为 <code>A</code> 的数据且特征 <code>x</code> 为 <code>r</code>的有 4 条。</p><p>同样我们用代码将先验概率 $P(特征 \mid 种类)$ 实现求解：</p><p>首先我们将特征按序号合并生成一个 <code>numpy</code> 类型的数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;导入特征数据并预览</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">train_data = np.array(data.iloc[:, <span class="number">1</span>:])</span><br><span class="line">train_data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>]], dtype=<span class="built_in">object</span>)</span><br></pre></td></tr></table></figure><p>在寻找属于某一类的某一个特征时，我们采用对比索引的方式来完成。<br>开始得到每一个类别的索引：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;类别 A,B 索引</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">labels = data[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line">label_index = []</span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> P_labels.keys():</span><br><span class="line">    temp_index = []</span><br><span class="line">    <span class="comment"># enumerate 函数返回 Series 类型数的索引和值，其中 i 为索引，label 为值</span></span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels):</span><br><span class="line">        <span class="keyword">if</span> (label == y):</span><br><span class="line">            temp_index.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    label_index.append(temp_index)</span><br><span class="line">label_index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]]</span><br></pre></td></tr></table></figure><p>得到 <code>A</code> 和 <code>B</code> 的索引，其中是<code>A</code>类别为前 $8$ 条数据，<code>B</code>类别为后 $7$ 条数据。</p><p>在得到类别的索引之后，接下来就是找到我们需要的特征为 <code>r</code>的索引值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;特征 x 为 r 的索引</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">x_index = [i <span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data[:, <span class="number">0</span>]) <span class="keyword">if</span> feature == <span class="string">&#x27;r&#x27;</span>]  <span class="comment"># 效果等同于求类别索引中 for 循环</span></span><br><span class="line">x_index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">11</span>]</span><br></pre></td></tr></table></figure><p>得到的结果为 $x$ 特征值为 $r$ 的数据索引值。</p><p>最后通过对比类别为 <code>A</code> 的索引值，计算出既符合 <code>x = r</code> 又符合 <code>A</code> 类别的数据在 <code>A</code> 类别中所占比例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_label = <span class="built_in">set</span>(x_index) &amp; <span class="built_in">set</span>(label_index[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">&#x27;既符合 x = r 又是 A 类别的索引值：&#x27;</span>, x_label)</span><br><span class="line">x_label_count = <span class="built_in">len</span>(x_label)</span><br><span class="line">print(<span class="string">&#x27;先验概率 P(r|A):&#x27;</span>, x_label_count / <span class="built_in">float</span>(<span class="built_in">len</span>(label_index[<span class="number">0</span>])))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">既符合 x = r 又是 A 类别的索引值： &#123;<span class="number">0</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>&#125;</span><br><span class="line">先验概率 P(r|A): <span class="number">0.5</span></span><br></pre></td></tr></table></figure><p>为了方便后面函数调用，我们将求 $P(特征\mid 种类)$ 代码整合为一个函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;P(特征∣种类) 先验概率计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_P_fea_lab</span>(<span class="params">P_label, features, data</span>):</span></span><br><span class="line">    P_fea_lab = &#123;&#125;</span><br><span class="line">    train_data = data.iloc[:, <span class="number">1</span>:]</span><br><span class="line">    train_data = np.array(train_data)</span><br><span class="line">    labels = data[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> each_label <span class="keyword">in</span> P_label.keys():</span><br><span class="line">        label_index = [i <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">            labels) <span class="keyword">if</span> label == each_label]  <span class="comment"># labels 中出现 y 值的所有数值的下标索引</span></span><br><span class="line">        <span class="comment"># features[0] 在 trainData[:,0] 中出现的值的所有下标索引</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features)):</span><br><span class="line">            feature_index = [i <span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">                train_data[:, j]) <span class="keyword">if</span> feature == features[j]]</span><br><span class="line">            <span class="comment"># set(x_index)&amp;set(y_index) 列出两个表相同的元素</span></span><br><span class="line">            fea_lab_count = <span class="built_in">len</span>(<span class="built_in">set</span>(feature_index) &amp; <span class="built_in">set</span>(label_index))</span><br><span class="line">            key = <span class="built_in">str</span>(features[j]) + <span class="string">&#x27;|&#x27;</span> + <span class="built_in">str</span>(each_label)</span><br><span class="line">            P_fea_lab[key] = fea_lab_count / <span class="built_in">float</span>(<span class="built_in">len</span>(label_index))</span><br><span class="line">    <span class="keyword">return</span> P_fea_lab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">features = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>]</span><br><span class="line">get_P_fea_lab(P_labels, features, data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;r|A&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line"> <span class="string">&#x27;m|A&#x27;</span>: <span class="number">0.375</span>,</span><br><span class="line"> <span class="string">&#x27;r|B&#x27;</span>: <span class="number">0.14285714285714285</span>,</span><br><span class="line"> <span class="string">&#x27;m|B&#x27;</span>: <span class="number">0.42857142857142855</span>&#125;</span><br></pre></td></tr></table></figure><p>可以得到当特征 <code>x</code> 和 <code>y</code> 的值为 <code>r</code> 和 <code>m</code> 时，在不同类别下的先验概率。</p><h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><p>在做极大似然估计时，若类别中缺少一些特征，则就会出现概率值为 <code>0</code> 的情况。此时，就会影响后验概率的计算结果，使得分类产生偏差。而解决这一问题最好的方法就是采用贝叶斯估计。  </p><p>在计算先验概率 $P(种类)$ 中，贝叶斯估计的数学表达式为：<br>$$<br>P(y_{i}=c_{k})=\frac{\sum_{N}^{i=1}I(y_{i}=c_{k})+\lambda }{N+k\lambda} \tag{8}<br>$$<br>其中 $\lambda \geq 0$ 等价于在随机变量各个取值的频数上赋予一个正数，当 $\lambda=0$ 时就是极大似然估计。在平时常取 $\lambda=1$，这时称为拉普拉斯平滑。例如：<strong>生成数据</strong> 中，$P(Y=A)=\frac{8+1}{15+2*1}=\frac{9}{17}$,取 $\lambda=1$ 此时由于一共有 <code>A</code>，<code>B</code> 两个类别，则 <code>k</code> 取 2。</p><p>同样计算 $P(特征 \mid 种类)$ 时，也是给计算时的分子分母加上拉普拉斯平滑。例如：<strong>生成数据</strong> 中，$P(x_{1}=”r” \mid Y=A)=\frac{4+1}{8+3*1}=\frac{5}{11}$ 同样取 $\lambda=1$ 此时由于 <code>x</code> 中有 <code>r</code>, <code>g</code>, <code>b</code> 三个种类，所以这里 k 取值为 3。</p><h3 id="朴素贝叶斯算法实现-1"><a href="#朴素贝叶斯算法实现-1" class="headerlink" title="朴素贝叶斯算法实现"></a>朴素贝叶斯算法实现</h3><p>通过上面的内容，相信你已经对朴素贝叶斯算法原理有一定印象。接下来，我们对朴素贝叶斯分类过程进行完整实现。其中，参数估计方法则使用极大似然估计。<br><em>注：分类器实现的公式，请参考《机器学习》- 周志华 P151 页</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;朴素贝叶斯分类器</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">data, features</span>):</span></span><br><span class="line">    <span class="comment"># 求 labels 中每个 label 的先验概率</span></span><br><span class="line">    labels = data[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">    P_label = get_P_labels(labels)</span><br><span class="line">    P_fea_lab = get_P_fea_lab(P_label, features, data)</span><br><span class="line"></span><br><span class="line">    P = &#123;&#125;</span><br><span class="line">    P_show = &#123;&#125;  <span class="comment"># 后验概率</span></span><br><span class="line">    <span class="keyword">for</span> each_label <span class="keyword">in</span> P_label:</span><br><span class="line">        P[each_label] = P_label[each_label]</span><br><span class="line">        <span class="keyword">for</span> each_feature <span class="keyword">in</span> features:</span><br><span class="line">            key = <span class="built_in">str</span>(each_label)+<span class="string">&#x27;|&#x27;</span>+<span class="built_in">str</span>(features)</span><br><span class="line">            P_show[key] = P[each_label] * \</span><br><span class="line">                P_fea_lab[<span class="built_in">str</span>(each_feature) + <span class="string">&#x27;|&#x27;</span> + <span class="built_in">str</span>(each_label)]</span><br><span class="line">            P[each_label] = P[each_label] * \</span><br><span class="line">                P_fea_lab[<span class="built_in">str</span>(each_feature) + <span class="string">&#x27;|&#x27;</span> +</span><br><span class="line">                          <span class="built_in">str</span>(each_label)]  <span class="comment"># 由于分母相同，只需要比较分子</span></span><br><span class="line">    print(P_show)</span><br><span class="line">    features_label = <span class="built_in">max</span>(P, key=P.get)  <span class="comment"># 概率最大值对应的类别</span></span><br><span class="line">    <span class="keyword">return</span> features_label</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classify(data, [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;A|[&#x27;r&#x27;, &#x27;m&#x27;]&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;B|[&#x27;r&#x27;, &#x27;m&#x27;]&quot;</span>: <span class="number">0.02857142857142857</span>&#125;</span><br><span class="line"><span class="string">&#x27;A&#x27;</span></span><br></pre></td></tr></table></figure><p>对于特征为 <code>[r,m]</code> 的数据通过朴素贝叶斯分类得到不同类别的概率值，经过比较后分为 <code>A</code> 类。</p><h3 id="朴素贝叶斯的三种常见模型"><a href="#朴素贝叶斯的三种常见模型" class="headerlink" title="朴素贝叶斯的三种常见模型"></a>朴素贝叶斯的三种常见模型</h3><p>了解完朴素贝叶斯算法原理后，在实际数据中，我们可以依照特征的数据类型不同，在计算先验概率方面对朴素贝叶斯模型进行划分，并分为：<strong>多项式模型</strong>，<strong>伯努利模型</strong>和<strong>高斯模型</strong>。</p><h4 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h4><p>当特征值为离散时，常常使用多项式模型。事实上，在以上实验的参数估计中，我们所应用的就是多项式模型。为避免概率值为 0 的情况出现，多项式模型采用的是贝叶斯估计。</p><h4 id="伯努利模型"><a href="#伯努利模型" class="headerlink" title="伯努利模型"></a>伯努利模型</h4><p>与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是 <code>1</code> 和 <code>0</code>（以文本分类为例，某个单词在文档中出现过，则其特征值为 <code>1</code>，否则为 <code>0</code>）。</p><p>在伯努利模型中，条件概率 $P(x_{i} \mid y_{k})$ 的计算方式为：</p><ul><li>当特征值 $x_{i}=1$ 时，$P(x_{i} \mid y_{k})=P(x_{i}=1 \mid y_{k})$;  </li><li>当特征值 $x_{i}=0$ 时，$P(x_{i} \mid y_{k})=P(x_{i}=0 \mid y_{k})$。</li></ul><h4 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h4><p>当特征是连续变量的时候，在不做平滑的情况下，运用多项式模型就会导致很多 $P(x_{i} \mid y_{k})=0$，此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，采用高斯模型。高斯模型是假设连续变量的特征数据是服从高斯分布的，高斯分布函数表达式为：<br>$$<br>P(x_{i}|y_{k})=\frac{1}{\sqrt{2\pi}\sigma_{y_{k},i}}exp(-\frac{(x-\mu_{y_{k},i}) ^{2}}{2\sigma ^{2}<em>{y</em>{k}},i})<br>$$<br>其中：</p><ul><li>$\mu_{y_{k},i}$ 表示类别为 $y_{k}$ 的样本中，第 $i$ 维特征的均值。  </li><li>$\sigma ^{2}<em>{y</em>{k}},i$ 表示类别为 $y_{k}$ 的样本中，第 $i$ 维特征的方差。  </li></ul><p>高斯分布示意图如下：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/4.png"></p><hr><p>关于贝叶斯定理，这里有一个有趣的视频，希望能加深大家对该定理的理解。</p><center><video width='800px' controls src="http://labfile.oss.aliyuncs.com/courses/1081/beyes_video.mp4" /></center><div style="color: #999;font-size: 12px;text-align: center;">如何用贝叶斯方法帮助内容审核 | 视频来源：[回形针PaperClip](https://weibo.com/u/6414205745?is_all=1)</div>]]></content>
    
    
    <summary type="html">&lt;p&gt;在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Naive Bayes" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Naive-Bayes/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Naive Bayes" scheme="http://www.laugh12321.cn/blog/tags/Naive-Bayes/"/>
    
  </entry>
  
  <entry>
    <title>机器学习| K-近邻算法详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/07/k_nearest_neighbors/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/07/k_nearest_neighbors/</id>
    <published>2019-01-07T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<h3 id="最近邻算法"><a href="#最近邻算法" class="headerlink" title="最近邻算法"></a>最近邻算法</h3><p>介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 $x$，在训练集中找到与 $x$ 最相似的训练样本 $y$，用 $y$ 的样本对应的类别作为未知类别数据 $x$ 的类别，从而达到分类的效果。</p><a id="more"></a><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/1.png"></p><p>如上图所示，通过计算数据 $X_{u}$ （未知样本）和已知类别 ${\omega_{1},\omega_{2},\omega_{3}}$ （已知样本）之间的距离，判断 $X_{u}$ 与不同训练集的相似度，最终判断 $X_{u}$ 的类别。显然，这里将<font color="green">绿色未知样本</font>类别判定与<font color="red">红色已知样本</font>类别相同较为合适。</p><h3 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K-近邻算法"></a>K-近邻算法</h3><p>K-近邻（K-Nearest Neighbors，简称：KNN）算法是最近邻（NN）算法的一个推广，也是机器学习分类算法中最简单的方法之一。KNN 算法的核心思想和最近邻算法思想相似，都是通过寻找和未知样本相似的类别进行分类。但 NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/2.png"></p><p>如上图所示，对于未知测试样本(图中<font color='red'> ？</font>所示)采用 KNN 算法进行分类，首先计算未知样本和训练样本之间的相似度，找出最近 K 个相邻样本（在图中 K 值为 3，圈定距离 ？最近的 3 个样本），再根据最近的 K 个样本最终判断未知样本的类别。</p><h2 id="K-近邻算法实现"><a href="#K-近邻算法实现" class="headerlink" title="K-近邻算法实现"></a>K-近邻算法实现</h2><p>KNN 算法在理论上已经非常成熟，其简单、易于理解的思想以及良好的分类准确度使得 KNN 算法应用非常广泛。算法的具体流程主要是以下的 4 个步骤：</p><ol><li><strong>数据准备</strong>：通过数据清洗，数据处理，将每条数据整理成向量。  </li><li><strong>计算距离</strong>：计算测试数据与训练数据之间的距离。  </li><li><strong>寻找邻居</strong>：找到与测试数据距离最近的 K 个训练数据样本。  </li><li><strong>决策分类</strong>：根据决策规则，从 K 个邻居得到测试数据的类别。</li></ol><p><img width='900px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/3.png"></img></p><h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><p>下面，我们尝试完成一个 KNN 分类流程。首先，生成一组示例数据，共包含 2 个类别（<code>A</code>和<code>B</code>），其中每一条数据包含两个特征（<code>x</code>和<code>y</code>）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;生成示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span>():</span></span><br><span class="line">    features = np.array(</span><br><span class="line">        [[<span class="number">2.88</span>, <span class="number">3.05</span>], [<span class="number">3.1</span>, <span class="number">2.45</span>], [<span class="number">3.05</span>, <span class="number">2.8</span>], [<span class="number">2.9</span>, <span class="number">2.7</span>], [<span class="number">2.75</span>, <span class="number">3.4</span>],</span><br><span class="line">         [<span class="number">3.23</span>, <span class="number">2.9</span>], [<span class="number">3.2</span>, <span class="number">3.75</span>], [<span class="number">3.5</span>, <span class="number">2.9</span>], [<span class="number">3.65</span>, <span class="number">3.6</span>], [<span class="number">3.35</span>, <span class="number">3.3</span>]])</span><br><span class="line">    labels = [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure><p>然后，我们尝试加载并打印这些数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;打印示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">features, labels = create_data()</span><br><span class="line">print(<span class="string">&#x27;features: \n&#x27;</span>, features)</span><br><span class="line">print(<span class="string">&#x27;labels: \n&#x27;</span>, labels)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">features: </span><br><span class="line"> [[<span class="number">2.88</span> <span class="number">3.05</span>]</span><br><span class="line"> [<span class="number">3.1</span>  <span class="number">2.45</span>]</span><br><span class="line"> [<span class="number">3.05</span> <span class="number">2.8</span> ]</span><br><span class="line"> [<span class="number">2.9</span>  <span class="number">2.7</span> ]</span><br><span class="line"> [<span class="number">2.75</span> <span class="number">3.4</span> ]</span><br><span class="line"> [<span class="number">3.23</span> <span class="number">2.9</span> ]</span><br><span class="line"> [<span class="number">3.2</span>  <span class="number">2.75</span>]</span><br><span class="line"> [<span class="number">3.5</span>  <span class="number">2.9</span> ]</span><br><span class="line"> [<span class="number">3.65</span> <span class="number">3.6</span> ]</span><br><span class="line"> [<span class="number">3.35</span> <span class="number">3.3</span> ]]</span><br><span class="line">labels: </span><br><span class="line"> [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]</span><br></pre></td></tr></table></figure><p>为了更直观地理解数据，接下来用 Matplotlib 下的 pyplot 包来对数据集进行可视化。为了代码的简洁，我们使用了 <code>map</code> 函数和 <code>lamda</code> 表达式对数据进行处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;示例数据绘图</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">plt.ylim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">x_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], features))  <span class="comment"># 返回每个数据的x特征值</span></span><br><span class="line">y_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], features))</span><br><span class="line">plt.scatter(x_feature[:<span class="number">5</span>], y_feature[:<span class="number">5</span>], c=<span class="string">&quot;b&quot;</span>)  <span class="comment"># 在画布上绘画出&quot;A&quot;类标签的数据点</span></span><br><span class="line">plt.scatter(x_feature[<span class="number">5</span>:], y_feature[<span class="number">5</span>:], c=<span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.scatter([<span class="number">3.18</span>], [<span class="number">3.15</span>], c=<span class="string">&quot;r&quot;</span>, marker=<span class="string">&quot;x&quot;</span>)  <span class="comment"># 待测试点的坐标为 [3.1，3.2]</span></span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/out_00.png"></p><p>由上图所示，标签为 <code>A</code>（蓝色圆点）的数据在画布的左下角位置，而标签为 <code>B</code>（绿色圆点）的数据在画布的右上角位置，通过图像可以清楚看出不同标签数据的分布情况。其中<font color="red">红色 x 点</font>即表示需预测类别的测试数据。</p><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>在计算两个样本间的相似度时，可以通过计算样本之间特征值的距离进行表示。若两个样本距离值越大（相距越远），则表示两个样本相似度低，相反，若两个样本值越小（相距越近），则表示两个样本相似度越高。</p><p>计算距离的方法有很多，本实验介绍两个最为常用的距离公式：<strong>曼哈顿距离</strong>和<strong>欧式距离</strong>。这两个距离的计算图示如下：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/4.png"></p><h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><p>曼哈顿距离又称马氏距离，出租车距离，是计算距离最简单的方式之一。公式如下：</p><p>$$<br>d_{man}=\sum_{i=1}^{N}\left | X_{i}-Y_{i} \right |<br>$$<br>其中： </p><ul><li>$X$,$Y$：两个数据点</li><li>$N$：每个数据中有 $N$ 个特征值</li><li>$X_{i}$ ：数据 $X$ 的第 $i$ 个特征值  </li></ul><p>公式表示为将两个数据 $X$ 和 $Y$ 中每一个对应特征值之间差值的绝对值，再求和，便得到曼哈顿距离。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;曼哈顿距离计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_man</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    d = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(x - y))</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">3.1</span>, <span class="number">3.2</span>])</span><br><span class="line">print(<span class="string">&quot;x:&quot;</span>, x)</span><br><span class="line">y = np.array([<span class="number">2.5</span>, <span class="number">2.8</span>])</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>, y)</span><br><span class="line">d_man = d_man(x, y)</span><br><span class="line">print(d_man)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x: [<span class="number">3.1</span> <span class="number">3.2</span>]</span><br><span class="line">y: [<span class="number">2.5</span> <span class="number">2.8</span>]</span><br><span class="line"><span class="number">1.0000000000000004</span></span><br></pre></td></tr></table></figure><h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><p>欧式距离源自 $N$ 维欧氏空间中两点之间的距离公式。表达式如下:</p><p>$$<br>d_{euc}= \sqrt{\sum_{i=1}^{N}(X_{i}-Y_{i})^{2}}<br>$$<br>其中：</p><ul><li>$X$, $Y$ ：两个数据点</li><li>$N$：每个数据中有 $N$ 个特征值</li><li>$X_{i}$ ：数据 $X$ 的第 $i$ 个特征值  </li></ul><p>公式表示为将两个数据 X 和 Y 中的每一个对应特征值之间差值的平方，再求和，最后开平方，便是欧式距离。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;欧氏距离的计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_euc</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    d = np.sqrt(np.<span class="built_in">sum</span>(np.square(x - y)))</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.random.random(<span class="number">10</span>)  <span class="comment"># 随机生成10个数的数组作为x特征的值</span></span><br><span class="line">print(<span class="string">&quot;x:&quot;</span>, x)</span><br><span class="line">y = np.random.random(<span class="number">10</span>)</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>, y)</span><br><span class="line">distance_euc = d_euc(x, y)</span><br><span class="line">print(distance_euc)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x: [<span class="number">0.10725148</span> <span class="number">0.78394185</span> <span class="number">0.85568109</span> <span class="number">0.5774587</span>  <span class="number">0.96974919</span> <span class="number">0.79467734</span></span><br><span class="line"> <span class="number">0.26009361</span> <span class="number">0.93204</span>    <span class="number">0.08424034</span> <span class="number">0.16970618</span>]</span><br><span class="line">y: [<span class="number">0.88013554</span> <span class="number">0.5943479</span>  <span class="number">0.31357311</span> <span class="number">0.20830397</span> <span class="number">0.20686205</span> <span class="number">0.9475627</span></span><br><span class="line"> <span class="number">0.61453761</span> <span class="number">0.27882129</span> <span class="number">0.61228018</span> <span class="number">0.75968914</span>]</span><br><span class="line"><span class="number">1.6876178018976438</span></span><br></pre></td></tr></table></figure><h3 id="决策规则"><a href="#决策规则" class="headerlink" title="决策规则"></a>决策规则</h3><p>在得到测试样本和训练样本之间的相似度后，通过相似度的排名，可以得到每一个测试样本的 K 个相邻的训练样本，那如何通过 K 个邻居来判断测试样本的最终类别呢？可以根据数据特征对决策规则进行选取，不同的决策规则会产生不同的预测结果，最常用的决策规则是：  </p><ul><li><strong>多数表决法</strong>：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。  </li><li><strong>加权表决法</strong>：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。</li></ul><p>这里推荐使用多数表决法，这种方法更加简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;多数表决法</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majority_voting</span>(<span class="params">class_count</span>):</span></span><br><span class="line">    sorted_class_count = <span class="built_in">sorted</span>(</span><br><span class="line">        class_count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_class_count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">2</span>, <span class="string">&quot;C&quot;</span>: <span class="number">6</span>, <span class="string">&quot;D&quot;</span>: <span class="number">5</span>&#125;</span><br><span class="line">majority_voting(arr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;C&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;D&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure><p>在多数表决法的定义中，我们导入了 <code>operater</code> 计算模块，目的是对字典类型结构排序。可以从结果中看出函数返回的结果为票数最多的 <code>C</code>，得票为 <code>6</code> 次。</p><h3 id="KNN-算法实现"><a href="#KNN-算法实现" class="headerlink" title="KNN 算法实现"></a>KNN 算法实现</h3><p>在学习完以上的各个步骤之后，KNN 算法也逐渐被勾勒出来。以下就是对 KNN 算法的完整实现，本次的距离计算采用<strong>欧式距离</strong>，分类的决策规则为<strong>多数表决法</strong>，定义函数 <code>knn_classify()</code>，其中函数的参数包括：</p><ul><li><code>test_data</code>：用于分类的输入向量。</li><li><code>train_data</code>：输入的训练样本集。</li><li><code>labels</code>：样本数据的类标签向量。</li><li><code>k</code>：用于选择最近邻居的数目。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;KNN 方法完整实现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn_classify</span>(<span class="params">test_data, train_data, labels, k</span>):</span></span><br><span class="line">    distances = np.array([])  <span class="comment"># 创建一个空的数组用于存放距离</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each_data <span class="keyword">in</span> train_data:  <span class="comment"># 使用欧式距离计算数据相似度</span></span><br><span class="line">        d = d_euc(test_data, each_data)</span><br><span class="line">        distances = np.append(distances, d)</span><br><span class="line"></span><br><span class="line">    sorted_distance_index = distances.argsort()  <span class="comment"># 获取按距离大小排序后的索引</span></span><br><span class="line">    sorted_distance = np.sort(distances)</span><br><span class="line">    r = (sorted_distance[k]+sorted_distance[k<span class="number">-1</span>])/<span class="number">2</span>  <span class="comment"># 计算</span></span><br><span class="line"></span><br><span class="line">    class_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):  <span class="comment"># 多数表决</span></span><br><span class="line">        vote_label = labels[sorted_distance_index[i]]</span><br><span class="line">        class_count[vote_label] = class_count.get(vote_label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    final_label = majority_voting(class_count)</span><br><span class="line">    <span class="keyword">return</span> final_label, r</span><br></pre></td></tr></table></figure><h3 id="分类预测"><a href="#分类预测" class="headerlink" title="分类预测"></a>分类预测</h3><p>在实现 KNN 算法之后，接下来就可以对我们未知数据<code>[3.18,3.15]</code>开始分类,假定我们 K 值初始设定为 5，让我们看看分类的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_data = np.array([<span class="number">3.18</span>, <span class="number">3.15</span>])</span><br><span class="line">final_label, r = knn_classify(test_data, features, labels, <span class="number">5</span>)</span><br><span class="line">final_label</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;B&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure><h3 id="可视化展示"><a href="#可视化展示" class="headerlink" title="可视化展示"></a>可视化展示</h3><p>在对数据 <code>[3.18,3.15]</code> 实现分类之后，接下来我们同样用画图的方式形象化展示 KNN 算法决策方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">circle</span>(<span class="params">r, a, b</span>):</span>  <span class="comment"># 为了画出圆，这里采用极坐标的方式对圆进行表示 ：x=r*cosθ，y=r*sinθ。</span></span><br><span class="line">    theta = np.arange(<span class="number">0</span>, <span class="number">2</span>*np.pi, <span class="number">0.01</span>)</span><br><span class="line">    x = a+r * np.cos(theta)</span><br><span class="line">    y = b+r * np.sin(theta)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k_circle_x, k_circle_y = circle(r, <span class="number">3.18</span>, <span class="number">3.15</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">plt.ylim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">x_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], features))  <span class="comment"># 返回每个数据的x特征值</span></span><br><span class="line">y_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], features))</span><br><span class="line">plt.scatter(x_feature[:<span class="number">5</span>], y_feature[:<span class="number">5</span>], c=<span class="string">&quot;b&quot;</span>)  <span class="comment"># 在画布上绘画出&quot;A&quot;类标签的数据点</span></span><br><span class="line">plt.scatter(x_feature[<span class="number">5</span>:], y_feature[<span class="number">5</span>:], c=<span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.scatter([<span class="number">3.18</span>], [<span class="number">3.15</span>], c=<span class="string">&quot;r&quot;</span>, marker=<span class="string">&quot;x&quot;</span>)  <span class="comment"># 待测试点的坐标为 [3.1，3.2]</span></span><br><span class="line">plt.plot(k_circle_x, k_circle_y)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/out_01.png"></p><p>如图所示，当我们 <code>K</code> 值为 <code>5</code> 时，与测试样本距离最近的 <code>5</code> 个训练数据（如蓝色圆圈所示）中属于 <code>B</code> 类的有 <code>3</code> 个，属于 <code>A</code> 类的有 <code>2</code> 个，根据多数表决法决策出测试样本的数据为 <code>B</code> 类。</p><p>通过尝试不同的 K 值我们会发现，不同的 K 值预测出不同的结果。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;最近邻算法&quot;&gt;&lt;a href=&quot;#最近邻算法&quot; class=&quot;headerlink&quot; title=&quot;最近邻算法&quot;&gt;&lt;/a&gt;最近邻算法&lt;/h3&gt;&lt;p&gt;介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 $x$，在训练集中找到与 $x$ 最相似的训练样本 $y$，用 $y$ 的样本对应的类别作为未知类别数据 $x$ 的类别，从而达到分类的效果。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="K-Nearest Neighbors" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/K-Nearest-Neighbors/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="K-Nearest Neighbors" scheme="http://www.laugh12321.cn/blog/tags/K-Nearest-Neighbors/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|多项式回归算法详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/04/polynomial_regression/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/04/polynomial_regression/</id>
    <published>2019-01-04T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多项式回归介绍"><a href="#多项式回归介绍" class="headerlink" title="多项式回归介绍"></a>多项式回归介绍</h2><p>在线性回归中，我们通过建立自变量 <code>x</code> 的一次方程来拟合数据。而非线性回归中，则需要建立因变量和自变量之间的非线性关系。从直观上讲，也就是拟合的直线变成了「曲线」。</p><a id="more"></a><p>如下图所示，是某地区人口数量的变化数据。如果我们使用线性方差去拟合数据，那么就会存在「肉眼可见」的误差。而对于这样的数据，使用一条曲线去拟合则更符合数据的发展趋势。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/1.png" alt="此处输入图片的描述"></p><p>对于非线性回归问题而言，最简单也是最常见的方法就是本次实验要讲解的「多项式回归」。多项式是中学时期就会接触到的概念，这里引用 <a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E9%A0%85%E5%BC%8F">维基百科</a> 的定义如下：</p><blockquote><p>多项式（Polynomial）是代数学中的基础概念，是由称为未知数的变量和称为系数的常量通过有限次加法、加减法、乘法以及自然数幂次的乘方运算得到的代数表达式。多项式是整式的一种。未知数只有一个的多项式称为一元多项式；例如 $x^2-3x+4$ 就是一个一元多项式。未知数不止一个的多项式称为多元多项式，例如 $x^3-2xyz^2+2yz+1$ 就是一个三元多项式。</p></blockquote><h2 id="多项式回归基础"><a href="#多项式回归基础" class="headerlink" title="多项式回归基础"></a>多项式回归基础</h2><p>首先，我们通过一组示例数据来认识多项式回归</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载示例数据</span></span><br><span class="line">x = [<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">25</span>, <span class="number">32</span>, <span class="number">43</span>, <span class="number">58</span>, <span class="number">63</span>, <span class="number">69</span>, <span class="number">79</span>]</span><br><span class="line">y = [<span class="number">20</span>, <span class="number">33</span>, <span class="number">50</span>, <span class="number">56</span>, <span class="number">42</span>, <span class="number">31</span>, <span class="number">33</span>, <span class="number">46</span>, <span class="number">65</span>, <span class="number">75</span>]</span><br></pre></td></tr></table></figure><p>示例数据一共有 10 组，分别对应着横坐标和纵坐标。接下来，通过 Matplotlib 绘制数据，查看其变化趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/2.png"></p><h3 id="实现-2-次多项式拟合"><a href="#实现-2-次多项式拟合" class="headerlink" title="实现 2 次多项式拟合"></a>实现 2 次多项式拟合</h3><p>接下来，通过多项式来拟合上面的散点数据。首先，一个标准的一元高阶多项式函数如下所示：</p><p>$$<br>y(x, w) = w_0 + w_1x + w_2x^2 +…+w_mx^m = \sum\limits_{j=0}^{m}w_jx^j \tag{1}<br>$$</p><p>其中，m 表示多项式的阶数，x^j 表示 x 的 j 次幂，w 则代表该多项式的系数。</p><p>当我们使用上面的多项式去拟合散点时，需要确定两个要素，分别是：多项式系数 $w$ 以及多项式阶数 $m$，这也是多项式的两个基本要素。</p><p>如果通过手动指定多项式阶数 $m$ 的大小，那么就只需要确定多项式系数 $w$ 的值是多少。例如，这里首先指定 $m=2$，多项式就变成了：<br>$$<br>y(x, w) = w_0 + w_1x + w_2x^2= \sum\limits_{j=0}^{2}w_jx^j \tag{2}<br>$$<br>当我们确定 $w$ 的值的大小时，就回到了前面线性回归中学习到的内容。</p><p>首先，我们构造两个函数，分别是用于拟合的多项式函数，以及误差函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;实现 2 次多项式函数及误差函数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">p, x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据公式，定义 2 次多项式函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w0, w1, w2 = p</span><br><span class="line">    f = w0 + w1*x + w2*x*x</span><br><span class="line">    <span class="keyword">return</span> f</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">err_func</span>(<span class="params">p, x, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差函数（观测值与拟合值之间的差距）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ret = func(p, x) - y</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p>接下来，使用 NumPy 提供的随机数方法初始化 3 个 $w$ 参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">p_init = np.random.randn(<span class="number">3</span>) <span class="comment"># 生成 3 个随机数</span></span><br><span class="line"></span><br><span class="line">p_init</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">0.60995017</span>,  <span class="number">1.32614407</span>, <span class="number">-1.22657863</span>])</span><br></pre></td></tr></table></figure><p>接下来，就是使用最小二乘法求解最优参数的过程。这里为了方便，我们直接使用 Scipy 提供的最小二乘法类，得到最佳拟合参数。当然，你完全可以按照线性回归实验中最小二乘法公式自行求解参数。不过，实际工作中为了快速实现，往往会使用像 Scipy 这样现成的函数，这里也是为了给大家多介绍一种方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;使用 Scipy 提供的最小二乘法函数得到最佳拟合参数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> leastsq</span><br><span class="line"></span><br><span class="line">parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Fitting Parameters: &#x27;</span>, parameters[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><blockquote><p>关于 <code>scipy.optimize.leastsq()</code> 的具体使用介绍，可以阅读 <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html">官方文档</a>。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Fitting Parameters:  [ <span class="number">3.76893126e+01</span> <span class="number">-2.60474221e-01</span>  <span class="number">8.00078171e-03</span>]</span><br></pre></td></tr></table></figure><p>我们这里得到的最佳拟合参数 $w_0$, $w_1$, $w_2$ 依次为 <code>3.76893117e+01</code>, <code>-2.60474147e-01</code> 和 <code>8.00078082e-03</code>。也就是说，我们拟合后的函数（保留两位有效数字）为：</p><p>$$<br>y(x) = 37 - 0.26<em>x + 0.0080</em>x^2 \tag{3}<br>$$</p><p>然后，我们尝试绘制出拟合后的图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制 2 次多项式拟合图像</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 绘制拟合图像时需要的临时点</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">80</span>, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制拟合函数曲线</span></span><br><span class="line">plt.plot(x_temp, func(parameters[<span class="number">0</span>], x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制原数据点</span></span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/3.png"></p><h3 id="实现-N-次多项式拟合"><a href="#实现-N-次多项式拟合" class="headerlink" title="实现 N 次多项式拟合"></a>实现 N 次多项式拟合</h3><p>你会发现，上面采用 <code>2</code> 次多项式拟合的结果也不能恰当地反映散点的变化趋势。此时，我们可以尝试 <code>3</code> 次及更高次多项式拟合。接下来的代码中，我们将针对上面 <code>2</code> 次多项式拟合的代码稍作修改，实现一个 <code>N</code> 次多项式拟合的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;实现 n 次多项式拟合</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_func</span>(<span class="params">p, x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据公式，定义 n 次多项式函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    f = np.poly1d(p)</span><br><span class="line">    <span class="keyword">return</span> f(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">err_func</span>(<span class="params">p, x, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差函数（观测值与拟合值之间的差距）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ret = fit_func(p, x) - y</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">n_poly</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;n 次多项式拟合</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p_init = np.random.randn(n) <span class="comment"># 生成 n 个随机数</span></span><br><span class="line">    parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))</span><br><span class="line">    <span class="keyword">return</span> parameters[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>可以使用 <code>n=3</code> 验证一下上面的代码是否可用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_poly(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">8.00077828e-03</span>, <span class="number">-2.60473932e-01</span>,  <span class="number">3.76893089e+01</span>])</span><br></pre></td></tr></table></figure><p>此时得到的参数结果和公式（3）的结果一致，只是顺序有出入。这是因为 NumPy 中的多项式函数 <code>np.poly1d(3)</code> 默认的样式是：</p><p>$$<br>y(x) = 0.0080<em>x^2 - 0.26</em>x + 37\tag{4}<br>$$<br>接下来，我们绘制出 <code>4，5，6，7, 8, 9</code> 次多项式的拟合结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制出 4，5，6，7, 8, 9 次多项式的拟合图像</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制拟合图像时需要的临时点</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">80</span>, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制子图</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>,<span class="number">0</span>].plot(x_temp, fit_func(n_poly(<span class="number">4</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">0</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">0</span>].set_title(<span class="string">&quot;m = 4&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>,<span class="number">1</span>].plot(x_temp, fit_func(n_poly(<span class="number">5</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">1</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">1</span>].set_title(<span class="string">&quot;m = 5&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>,<span class="number">2</span>].plot(x_temp, fit_func(n_poly(<span class="number">6</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">2</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">2</span>].set_title(<span class="string">&quot;m = 6&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>,<span class="number">0</span>].plot(x_temp, fit_func(n_poly(<span class="number">7</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">0</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">0</span>].set_title(<span class="string">&quot;m = 7&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>,<span class="number">1</span>].plot(x_temp, fit_func(n_poly(<span class="number">8</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">1</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">1</span>].set_title(<span class="string">&quot;m = 8&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>,<span class="number">2</span>].plot(x_temp, fit_func(n_poly(<span class="number">9</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">2</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">2</span>].set_title(<span class="string">&quot;m = 9&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/4.png"></p><p>从上面的 <code>6</code> 张图可以看出，当 <code>m=4</code>（4 次多项式） 时，图像拟合的效果已经明显优于 <code>m=3</code> 的结果。但是随着 m 次数的增加，当 m=8 时，曲线呈现出明显的震荡，这也就是线性回归实验中所讲到的过拟和（Overfitting）现象。</p><h3 id="使用-scikit-learn-进行多项式拟合"><a href="#使用-scikit-learn-进行多项式拟合" class="headerlink" title="使用 scikit-learn 进行多项式拟合"></a>使用 scikit-learn 进行多项式拟合</h3><p>除了像上面我们自己去定义多项式及实现多项式回归拟合过程，也可以使用 <code>scikit-learn</code> 提供的多项式回归方法来完成。这里，我们会用到<code>sklearn.preprocessing.PolynomialFeatures()</code> 这个类。<code>PolynomialFeatures()</code> 主要的作用是产生多项式特征矩阵。<strong>如果你第一次接触这个概念，可能需要仔细理解下面的内容。</strong></p><p>对于一个二次多项式而言，我们知道它的标准形式为：$ y(x, w) = w_0 + w_1x + w_2x^2 $。但是，多项式回归却相当于线性回归的特殊形式。例如，我们这里令 $x = x_1$, $x^2 = x_2$ ，那么原方程就转换为：$ y(x, w) = w_0 + w_1<em>x_1 + w_2</em>x_2 $，这也就变成了多元线性回归。这就完成了<strong>一元高次多项式到多元一次多项式之间的转换</strong>。</p><p>举例说明，对于自变量向量 $X$ 和因变量 $y$，如果 $X$：</p><p>$$<br>\mathbf{X} = \begin{bmatrix}<br>       2    \[0.3em]<br>       -1 \[0.3em]<br>       3<br>     \end{bmatrix} \tag{5a}<br>$$<br>我们可以通过 $ y = w_1 x + w_0$ 线性回归模型进行拟合。同样，如果对于一元二次多项式 $ y(x, w) = w_0 + w_1x + w_2x^2 $，如果能得到由 $x = x_1$, $x^2 = x_2$ 构成的特征矩阵，即：</p><p>$$<br>\mathbf{X} = \left [ X, X^2 \right ] = \begin{bmatrix}<br> 2&amp; 4\ -1<br> &amp; 1\ 3<br> &amp; 9<br>\end{bmatrix}<br>\tag{5b}<br>$$<br>那么也就可以通过线性回归进行拟合了。</p><p>你可以手动计算上面的结果，但是<strong>当多项式为一元高次或者多元高次时，特征矩阵的表达和计算过程就变得比较复杂了</strong>。例如，下面是二元二次多项式的特征矩阵表达式。</p><p>$$<br>\mathbf{X} = \left [ X_{1}, X_{2}, X_{1}^2, X_{1}X_{2}, X_{2}^2 \right ]<br>\tag{5c}<br>$$<br>还好，在 scikit-learn 中，我们可以通过 <code>PolynomialFeatures()</code> 类自动产生多项式特征矩阵，<code>PolynomialFeatures()</code> 类的默认参数及常用参数定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.preprocessing.PolynomialFeatures(degree=<span class="number">2</span>, interaction_only=<span class="literal">False</span>, include_bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><ul><li><code>degree</code>: 多项式次数，默认为 2 次多项式</li><li><code>interaction_only</code>: 默认为 False，如果为 True 则产生相互影响的特征集。</li><li><code>include_bias</code>: 默认为 True，包含多项式中的截距项。</li></ul><p>对应上面的特征向量，我们使用 <code>PolynomialFeatures()</code> 的主要作用是产生 2 次多项式对应的特征矩阵，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;使用 PolynomialFeatures 自动生成特征矩阵</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">X=[<span class="number">2</span>, <span class="number">-1</span>, <span class="number">3</span>]</span><br><span class="line">X_reshape = np.array(X).reshape(<span class="built_in">len</span>(X), <span class="number">1</span>) <span class="comment"># 转换为列向量</span></span><br><span class="line">PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>).fit_transform(X_reshape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ <span class="number">2.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [<span class="number">-1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">9.</span>]])</span><br></pre></td></tr></table></figure><p>对于上方单元格中的矩阵，第 1 列为 $X^1$，第 2 列为 $X^2$。我们就可以通过多元线性方程 $ y(x, w) = w_0 + w_1<em>x_1 + w_2</em>x_2 $ 对数据进行拟合。</p><blockquote><p>注意：本篇文章中，你会看到大量的 <code>reshape</code> 操作，它们的目的都是为了满足某些类传参的数组形状。这些操作在本实验中是必须的，因为数据原始形状（如上面的一维数组）可能无法直接传入某些特定类中。但在实际工作中并不是必须的，因为你手中的原始数据集形状可能支持直接传入。所以，不必为这些 <code>reshape</code> 操作感到疑惑，也不要死记硬背。</p></blockquote><p>回到 <code>2.1</code> 小节中的示例数据，其自变量应该是 $x$，而因变量是 $y$。如果我们使用 2 次多项式拟合，那么首先使用 <code>PolynomialFeatures()</code> 得到特征矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;使用 sklearn 得到 2 次多项式回归特征矩阵</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">x = np.array(x).reshape(<span class="built_in">len</span>(x), <span class="number">1</span>) <span class="comment"># 转换为列向量</span></span><br><span class="line">y = np.array(y).reshape(<span class="built_in">len</span>(y), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">poly_x = poly_features.fit_transform(x)</span><br><span class="line"></span><br><span class="line">poly_x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">4.000e+00</span>, <span class="number">1.600e+01</span>],</span><br><span class="line">       [<span class="number">8.000e+00</span>, <span class="number">6.400e+01</span>],</span><br><span class="line">       [<span class="number">1.200e+01</span>, <span class="number">1.440e+02</span>],</span><br><span class="line">       [<span class="number">2.500e+01</span>, <span class="number">6.250e+02</span>],</span><br><span class="line">       [<span class="number">3.200e+01</span>, <span class="number">1.024e+03</span>],</span><br><span class="line">       [<span class="number">4.300e+01</span>, <span class="number">1.849e+03</span>],</span><br><span class="line">       [<span class="number">5.800e+01</span>, <span class="number">3.364e+03</span>],</span><br><span class="line">       [<span class="number">6.300e+01</span>, <span class="number">3.969e+03</span>],</span><br><span class="line">       [<span class="number">6.900e+01</span>, <span class="number">4.761e+03</span>],</span><br><span class="line">       [<span class="number">7.900e+01</span>, <span class="number">6.241e+03</span>]])</span><br></pre></td></tr></table></figure><p>可以看到，输出结果正好对应一元二次多项式特征矩阵公式：$\left [ X, X^2 \right ]$</p><p>然后，我们使用 scikit-learn 训练线性回归模型。这里将会使用到 <code>LinearRegression()</code> 类，<code>LinearRegression()</code> 类的默认参数及常用参数定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.LinearRegression(fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, copy_X=<span class="literal">True</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li><code>fit_intercept</code>: 默认为 True，计算截距项。</li><li><code>normalize</code>: 默认为 False，不针对数据进行标准化处理。</li><li><code>copy_X</code>: 默认为 True，即使用数据的副本进行操作，防止影响原数据。</li><li><code>n_jobs</code>: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;转换为线性回归预测</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(poly_x, y) <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到模型拟合参数</span></span><br><span class="line">model.intercept_, model.coef_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([<span class="number">2.13162821e-14</span>]), array([[<span class="number">1.00000000e+00</span>, <span class="number">4.35999447e-18</span>]]))</span><br></pre></td></tr></table></figure><p>你会发现，这里得到的参数值和公式（3），（4）一致。为了更加直观，这里同样绘制出拟合后的图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制拟合图像</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">x_temp = np.array(x_temp).reshape(<span class="built_in">len</span>(x_temp),<span class="number">1</span>)</span><br><span class="line">poly_x_temp = poly_features.fit_transform(x_temp)</span><br><span class="line"></span><br><span class="line">plt.plot(x_temp, model.predict(poly_x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/5.png"></p><p>你会发现，上图似曾相识。它和公式（3）下方的图其实是一致的。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;多项式回归介绍&quot;&gt;&lt;a href=&quot;#多项式回归介绍&quot; class=&quot;headerlink&quot; title=&quot;多项式回归介绍&quot;&gt;&lt;/a&gt;多项式回归介绍&lt;/h2&gt;&lt;p&gt;在线性回归中，我们通过建立自变量 &lt;code&gt;x&lt;/code&gt; 的一次方程来拟合数据。而非线性回归中，则需要建立因变量和自变量之间的非线性关系。从直观上讲，也就是拟合的直线变成了「曲线」。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Polynomial Regression" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Polynomial-Regression/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Polynomial Regression" scheme="http://www.laugh12321.cn/blog/tags/Polynomial-Regression/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|线性回归三大评价指标实现『MAE, MSE, MAPE』</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/02/evaluation_index_with_linear_regression/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/02/evaluation_index_with_linear_regression/</id>
    <published>2019-01-02T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>对于回归预测结果，通常会有平均绝对误差、平均绝对百分比误差、均方误差等多个指标进行评价。这里，我们先介绍最常用的3个：</p><a id="more"></a><p><strong>平均绝对误差（MAE）</strong><br>就是绝对误差的平均值，它的计算公式如下：<br>$$<br>MAE(y,\hat{y}) = \frac{1}{n}(\sum_{i = 1}^{n}\left | y - \hat{y} \right |)<br>$$<br>其中，$y_{i}$ 表示真实值，$\hat y_{i}$ 表示预测值，$n$ 则表示值的个数。MAE 的值越小，说明预测模型拥有更好的精确度。我们可以尝试使用 Python 实现 MAE 计算函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mae_value</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    y_true -- 测试集目标真实值</span></span><br><span class="line"><span class="string">    y_pred -- 测试集目标预测值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    mae -- MAE 评价指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    n = <span class="built_in">len</span>(y_true)</span><br><span class="line">    mae = <span class="built_in">sum</span>(np.<span class="built_in">abs</span>(y_true - y_pred))/n</span><br><span class="line">    <span class="keyword">return</span> mae</span><br></pre></td></tr></table></figure><p><strong>均方误差（MSE）</strong><br>它表示误差的平方的期望值，它的计算公式如下：<br>$$<br>{MSE}(y, \hat{y} ) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y})^{2}<br>$$</p><p>其中，$y_{i}$ 表示真实值，$\hat y_{i}$ 表示预测值，$n$ 则表示值的个数。MSE 的值越小，说明预测模型拥有更好的精确度。同样，我们可以尝试使用 Python 实现 MSE 计算函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse_value</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    y_true -- 测试集目标真实值</span></span><br><span class="line"><span class="string">    y_pred -- 测试集目标预测值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    mse -- MSE 评价指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    n = <span class="built_in">len</span>(y_true)</span><br><span class="line">    mse = <span class="built_in">sum</span>(np.square(y_true - y_pred))/n</span><br><span class="line">    <span class="keyword">return</span> mse</span><br></pre></td></tr></table></figure><p>**平均绝对百分比误差 $MAPE$**。</p><p>$MAPE$ 是 $MAD$ 的变形，它是一个百分比值，因此比其他统计量更容易理解。例如，如果 $MAPE$ 为 $5$，则表示预测结果较真实结果平均偏离 $5%$。$MAPE$ 的计算公式如下：<br>$$<br>{MAPE}(y, \hat{y} ) = \frac{\sum_{i=1}^{n}{|\frac{y_{i}-\hat y_{i}}{y_{i}}|}}{n} \times 100<br>$$</p><p>其中，$y_{i}$ 表示真实值，$\hat y_{i}$ 表示预测值，$n$ 则表示值的个数。$MAPE$ 的值越小，说明预测模型拥有更好的精确度。使用 Python 实现 MSE 计算函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mape</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    y_true -- 测试集目标真实值</span></span><br><span class="line"><span class="string">    y_pred -- 测试集目标预测值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    mape -- MAPE 评价指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    n = <span class="built_in">len</span>(y_true)</span><br><span class="line">    mape = <span class="built_in">sum</span>(np.<span class="built_in">abs</span>((y_true - y_pred)/y_true))/n*<span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> mape</span><br></pre></td></tr></table></figure><hr><p><strong>参考</strong>：</p><ul><li><a href="https://blog.csdn.net/cqfdcw/article/details/78173839">方差（variance）、标准差（Standard Deviation）、均方差、均方根值（RMS）、均方误差（MSE）、均方根误差（RMSE）</a></li><li><a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean squared error-Wikipedia</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;对于回归预测结果，通常会有平均绝对误差、平均绝对百分比误差、均方误差等多个指标进行评价。这里，我们先介绍最常用的3个：&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Linear Regression" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Linear-Regression/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Linear Regression" scheme="http://www.laugh12321.cn/blog/tags/Linear-Regression/"/>
    
    <category term="MAE" scheme="http://www.laugh12321.cn/blog/tags/MAE/"/>
    
    <category term="MSE" scheme="http://www.laugh12321.cn/blog/tags/MSE/"/>
    
    <category term="MAPE" scheme="http://www.laugh12321.cn/blog/tags/MAPE/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|线性回归算法详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/01/linear_regression/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/01/linear_regression/</id>
    <published>2019-01-01T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。</p><a id="more"></a><h2 id="线性回归介绍"><a href="#线性回归介绍" class="headerlink" title="线性回归介绍"></a>线性回归介绍</h2><p>在了解线性回归之前，我们得先了解分类和回归问题的区别。</p><p>首先，回归问题和分类问题一样，训练数据都包含标签，这也是监督学习的特点。而不同之处在于，分类问题预测的是类别，回归问题预测的是连续值。</p><p>例如，回归问题往往解决：</p><ul><li>股票价格预测</li><li>房价预测</li><li>洪水水位线</li></ul><p>上面列举的问题，我们需要预测的目标都不是类别，而是实数连续值。</p><p><img width='800px' src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/00.png"></img></p><p>也就是说，回归问题旨在实现对连续值的预测，例如股票的价格、房价的趋势等。比如，下方展现了一个房屋面积和价格的对应关系图。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/1.png" alt="此处输入图片的描述"></p><p>如上图所示，不同的房屋面积对应着不同的价格。现在，假设我手中有一套房屋想要出售，而出售时就需要预先对房屋进行估值。于是，我想通过上图，也就是其他房屋的售价来判断手中的房产价值是多少。应该怎么做呢？</p><p>我采用的方法是这样的。如下图所示，首先画了一条<font color="red">红色</font>的直线，让其大致验证<font color="orange">橙色</font>点分布的延伸趋势。然后，我将已知房屋的面积大小对应到红色直线上，也就是<font color="blue">蓝色</font>点所在位置。最后，再找到蓝色点对应于房屋的价格作为房屋最终的预估价值。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/2.png" alt="此处输入图片的描述"></p><p>在上图呈现的这个过程中，通过找到一条直线去拟合数据点的分布趋势的过程，就是<strong>线性回归</strong>的过程。而线性回归中的「线性」代指线性关系，也就是图中所绘制的红色直线。</p><p>此时，你可能心中会有一个疑问。上图中的红色直线是怎么绘制出来的呢？为什么不可以像下图中另外两条绿色虚线，而偏偏要选择红色直线呢？</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/3.png" alt="此处输入图片的描述"></p><p>上图中的绿色虚线的确也能反应数据点的分布趋势。所以，找到最适合的那一条红色直线，也是线性回归中需要解决的重要问题之一。</p><p>通过上面这个小例子，相信你对线性回归已经有一点点印象了，至少大致明白它能做什么。接下来的内容中，我们将了解线性回归背后的数学原理，以及使用 Python 代码对其实现。</p><h2 id="线性回归原理及实现"><a href="#线性回归原理及实现" class="headerlink" title="线性回归原理及实现"></a>线性回归原理及实现</h2><h3 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h3><p>上面针对线性回归的介绍内容中，我们列举了一个房屋面积与房价变化的例子。其中，房屋面积为自变量，而房价则为因变量。另外，我们将只有 1 个自变量的线性拟合过程叫做一元线性回归。</p><p>下面，我们就生成一组房屋面积和房价变化的示例数据。<code>x</code> 为房屋面积，单位是平方米; <code>y</code> 为房价，单位是万元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">56</span>, <span class="number">72</span>, <span class="number">69</span>, <span class="number">88</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">76</span>, <span class="number">79</span>, <span class="number">94</span>, <span class="number">74</span>])</span><br><span class="line">y = np.array([<span class="number">92</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">110</span>, <span class="number">130</span>, <span class="number">99</span>, <span class="number">96</span>, <span class="number">102</span>, <span class="number">105</span>, <span class="number">92</span>])</span><br></pre></td></tr></table></figure><p>示例数据由 <code>10</code> 组房屋面积及价格对应组成。接下来，通过 Matplotlib 绘制数据点，<code>x, y</code> 分别对应着横坐标和纵坐标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Area&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Price&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/out_00.png"></p><p>正如上面所说，线性回归即通过线性方程（<code>1</code> 次函数）去拟合数据点。那么，我们令函数的表达式为：</p><p>$$ y(x, w) = w_0 + w_1x \tag{1} $$</p><p>公式（1）是典型的一元一次函数表达式，我们通过组合不同的 $w_0$ 和 $w_1$ 的值得到不同的拟合直线。我们对公式（1）进行代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x, w0, w1</span>):</span></span><br><span class="line">    y = w0 + w1 * x</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>那么，<strong>哪一条直线最能反应出数据的变化趋势呢？</strong></p><p>如下图所示，当我们使用 $y(x, w) = w_0 + w_1x$ 对数据进行拟合时，我们能得到拟合的整体误差，即图中蓝色线段的长度总和。如果某一条直线对应的误差值最小，是不是就代表这条直线最能反映数据点的分布趋势呢？</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/4.png"></p><h3 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h3><p>正如上面所说，如果一个数据点为 ($x_{i}$, $y_{i}$)，那么它对应的误差就为:</p><p>$$y_{i}-(w_0 + w_1x_{i}) \tag2$$</p><p>上面的误差往往也称之为残差。但是在机器学习中，我们更喜欢称作「损失」，即真实值和预测值之间的偏离程度。那么，对应 n 个全部数据点而言，其对应的残差损失总和就为：<br>$$<br>\sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i})) \tag3<br>$$<br>在线性回归中，我们更偏向于使用均方误差作为衡量损失的指标，而均方误差即为残差的平方和。公式如下：</p><p>$$<br>\sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2\tag4<br>$$<br>对于公式（4）而言，机器学习中有一个专门的名词，那就是「平方损失函数」。而为了得到拟合参数 $w_0$ 和 $w_1$ 最优的数值，我们的目标就是让公式（4）对应的平方损失函数最小。</p><p>同样，我们可以对公式（4）进行代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span>(<span class="params">x, y, w0, w1</span>):</span></span><br><span class="line">    loss = <span class="built_in">sum</span>(np.square(y - (w0 + w1*x)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="最小二乘法及代数求解"><a href="#最小二乘法及代数求解" class="headerlink" title="最小二乘法及代数求解"></a>最小二乘法及代数求解</h3><p>最小二乘法是用于求解线性回归拟合参数 $w$ 的一种常用方法。最小二乘法中的「二乘」代表平方，最小二乘也就是最小平方。而这里的平方就是指代上面的平方损失函数。</p><p>简单来讲，最小二乘法也就是求解平方损失函数最小值的方法。那么，到底该怎样求解呢？这就需要使用到高等数学中的知识。推导如下：</p><p>首先，平方损失函数为：</p><p>$$<br>f = \sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2 \tag5<br>$$<br>我们的目标是求取平方损失函数 $min(f)$ 最小时，对应的 $w$。首先求 $f$ 的 <code>1</code> 阶偏导数：</p><p>$$<br>\frac{\partial f}{\partial w_{0}}=-2(\sum_{i=1}^{n}{y_i}-nw_{0}-w_{1}\sum_{i=1}^{n}{x_i})\<br>\frac{\partial f}{\partial w_{1}}=-2(\sum_{i=1}^{n}{x_iy_i}-w_{0}\sum_{i=1}^{n}{x_i}-w_{1}\sum_{i=1}^{n}{x_i}^2) \tag6<br>$$<br>然后，我们令 $\frac{\partial f}{\partial w_{0}}=0$ 以及  $\frac{\partial f}{\partial w_{1}}=0$，解得：<br>$$<br>w_{1}=\frac {n\sum_{}^{}{x_iy_i}-\sum_{}^{}{x_i}\sum_{}^{}{y_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2}\<br>w_{0}=\frac {\sum_{}^{}{x_i}^2\sum_{}^{}{y_i}-\sum_{}^{}{x_i}\sum_{}^{}{x_iy_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2}\tag7<br>$$<br>到目前为止，已经求出了平方损失函数最小时对应的 $w$ 参数值，这也就是最佳拟合直线。</p><h3 id="线性回归-Python-实现"><a href="#线性回归-Python-实现" class="headerlink" title="线性回归 Python 实现"></a>线性回归 Python 实现</h3><p>我们将公式（7）求解得到 $w$ 的过程进行代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w_calculator</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(x)</span><br><span class="line">    w1 = (n*<span class="built_in">sum</span>(x*y) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(y))/(n*<span class="built_in">sum</span>(x*x) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x))</span><br><span class="line">    w0 = (<span class="built_in">sum</span>(x*x)*<span class="built_in">sum</span>(y) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x*y))/(n*<span class="built_in">sum</span>(x*x)-<span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x))</span><br><span class="line">    <span class="keyword">return</span> w0, w1</span><br></pre></td></tr></table></figure><p>于是，可以向函数 <code>w_calculator(x, y)</code> 中传入 <code>x</code> 和 <code>y</code> 得到 $w_0$ 和 $w_1$ 的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w_calculator(x, y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">41.33509168550616</span>, <span class="number">0.7545842753077117</span>)</span><br></pre></td></tr></table></figure><p>当然，我们也可以求得此时对应的平方损失的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w0 = w_calculator(x, y)[<span class="number">0</span>]</span><br><span class="line">w1 = w_calculator(x, y)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">square_loss(x, y, w0, w1)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">447.69153479025357</span></span><br></pre></td></tr></table></figure><p>接下来，我们尝试将拟合得到的直线绘制到原图中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_temp = np.linspace(<span class="number">50</span>,<span class="number">120</span>,<span class="number">100</span>) <span class="comment"># 绘制直线生成的临时点</span></span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x_temp, x_temp*w1 + w0, <span class="string">&#x27;r&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/out_01.png"></p><p>从上图可以看出，拟合的效果还是不错的。那么，如果你手中有一套 <code>150</code> 平米的房产想售卖，获得预估报价就只需要带入方程即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(<span class="number">150</span>, w0, w1)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">154.5227329816629</span></span><br></pre></td></tr></table></figure><p>这里得到的预估售价约为 <code>154</code> 万元。</p><h3 id="线性回归-scikit-learn-实现"><a href="#线性回归-scikit-learn-实现" class="headerlink" title="线性回归 scikit-learn 实现"></a>线性回归 scikit-learn 实现</h3><p>上面的内容中，我们学习了什么是最小二乘法，以及使用 Python 对最小二乘线性回归进行了完整实现。那么，我们如何利用机器学习开源模块 scikit-learn 实现最小二乘线性回归方法呢？</p><p>使用 scikit-learn 实现线性回归的过程会简单很多，这里要用到 <code>LinearRegression()</code> 类。看一下其中的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.LinearRegression(fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, copy_X=<span class="literal">True</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>fit_intercept</code>: 默认为 True，计算截距项。</li><li><code>normalize</code>: 默认为 False，不针对数据进行标准化处理。</li><li><code>copy_X</code>: 默认为 True，即使用数据的副本进行操作，防止影响原数据。</li><li><code>n_jobs</code>: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;scikit-learn 线性回归拟合</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(x.reshape(<span class="built_in">len</span>(x),<span class="number">1</span>), y) <span class="comment"># 训练, reshape 操作把数据处理成 fit 能接受的形状</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到模型拟合参数</span></span><br><span class="line">model.intercept_, model.coef_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">41.33509168550615</span>, array([<span class="number">0.75458428</span>]))</span><br></pre></td></tr></table></figure><p>我们通过 <code>model.intercept_</code> 得到拟合的截距项，即上面的 $w_{0}$，通过 <code>model.coef_</code> 得到 $x$ 的系数，即上面的 $w_{1}$。对比发现，结果是<strong>完全一致</strong>的。</p><p>同样，我们可以预测 <code>150</code> 平米房产的价格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict([[<span class="number">150</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">154.52273298</span>])</span><br></pre></td></tr></table></figure><p>可以看到，这里得出的结果和自行实现计算结果一致。</p><h2 id="最小二乘法的矩阵推导及实现"><a href="#最小二乘法的矩阵推导及实现" class="headerlink" title="最小二乘法的矩阵推导及实现"></a>最小二乘法的矩阵推导及实现</h2><p>学习完上面的内容，相信你已经了解了什么是最小二乘法，以及如何使用最小二乘法进行线性回归拟合。上面，实验采用了求偏导数的方法，并通过代数求解找到了最佳拟合参数 <code>w</code> 的值。</p><p>这里，我们尝试另外一种方法，即通过矩阵的变换来计算参数 <code>w</code> 。推导如下：</p><p>首先，一元线性函数的表达式为 $ y(x, w) = w_0 + w_1x$，表达成矩阵形式为：</p><p>$$\begin{bmatrix}1, x_{1} \ 1, x_{2} \ … \ 1, x_{9} \ 1, x_{10} \end{bmatrix} * \begin{bmatrix}w_{0} \ w_{1} \end{bmatrix} = \begin{bmatrix}y_{1} \ y_{2} \ … \ y_{9} \ y_{10} \end{bmatrix} \Rightarrow \begin{bmatrix}1, 56 \ 1, 72 \ … \ 1, 94 \ 1, 74 \end{bmatrix}* \begin{bmatrix}w_{0} \ w_{1} \end{bmatrix}= \begin{bmatrix}92 \ 102 \ … \ 105 \ 92 \end{bmatrix} \tag{8a}$$</p><p>即：<br>$$ y(x, w) = XW \tag{8b} $$</p><p>（8）式中，$W$ 为 $\begin{bmatrix}w_{0}<br>\ w_{1}<br>\end{bmatrix}$，而 $X$ 则是 $\begin{bmatrix}1, x_{1}<br>\ 1, x_{2}<br>\ …<br>\ 1, x_{9}<br>\ 1, x_{10}<br>\end{bmatrix}$ 矩阵。然后，平方损失函数为：<br>$$<br>f = \sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2 =(y-XW)^T(y-XW)\tag{9}<br>$$<br>此时，对矩阵求偏导数（超纲）得到：</p><p>$$<br>\frac{\partial f}{\partial W}=2<em>X^TXW-2</em>X^Ty=0 \tag{10}<br>$$<br>当矩阵 $X^TX$ 满秩（不满秩后面的实验中会讨论）时，$(X^TX)^{-1}X^TX=E$，且 $EW=W$。所以，$(X^TX)^{-1}X^TXW=(X^TX)^{-1}X^Ty$。最终得到：<br>$$<br>W=(X^TX)^{-1}X^Ty \tag{11}<br>$$<br>我们可以针对公式（11）进行代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w_matrix</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    w = (x.T * x).I * x.T * y</span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure><p>我们针对原 <code>x</code> 数据添加截距项系数 <code>1</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.matrix([[<span class="number">1</span>,<span class="number">56</span>],[<span class="number">1</span>,<span class="number">72</span>],[<span class="number">1</span>,<span class="number">69</span>],[<span class="number">1</span>,<span class="number">88</span>],[<span class="number">1</span>,<span class="number">102</span>],[<span class="number">1</span>,<span class="number">86</span>],[<span class="number">1</span>,<span class="number">76</span>],[<span class="number">1</span>,<span class="number">79</span>],[<span class="number">1</span>,<span class="number">94</span>],[<span class="number">1</span>,<span class="number">74</span>]])</span><br><span class="line">y = np.matrix([<span class="number">92</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">110</span>, <span class="number">130</span>, <span class="number">99</span>, <span class="number">96</span>, <span class="number">102</span>, <span class="number">105</span>, <span class="number">92</span>])</span><br><span class="line"></span><br><span class="line">w_matrix(x, y.reshape(<span class="number">10</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">matrix([[<span class="number">41.33509169</span>],</span><br><span class="line">        [ <span class="number">0.75458428</span>]])</span><br></pre></td></tr></table></figure><p>可以看到，矩阵计算结果和前面的代数计算结果一致。你可能会有疑问，那就是为什么要采用矩阵变换的方式计算？一开始学习的代数计算方法不好吗？</p><p>其实，并不是说代数计算方式不好，在小数据集下二者运算效率接近。但是，当我们面对十万或百万规模的数据时，矩阵计算的效率就会高很多，这就是为什么要学习矩阵计算的原因。</p><p><strong>参考</strong>：</p><ul><li><a href="https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">最小二乘法-维基百科</a></li><li><a href="https://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8">线性回归-维基百科</a></li><li><a href="https://www.zhihu.com/question/37031188">知乎问答-最小二乘法的本质是什么？</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;线性回归&lt;/h1&gt;&lt;p&gt;线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    <category term="Linear Regression" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/Linear-Regression/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Linear Regression" scheme="http://www.laugh12321.cn/blog/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Jekyll + NexT + GitHub Pages 主题深度优化</title>
    <link href="http://www.laugh12321.cn/blog/2018/12/24/update_next_for_jekyll/"/>
    <id>http://www.laugh12321.cn/blog/2018/12/24/update_next_for_jekyll/</id>
    <published>2018-12-24T00:00:00.000Z</published>
    <updated>2020-10-21T05:58:58.651Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>笔者在用 Jekyll 搭建个人博客时踩了很多的坑，最后发现了一款不错的主题 <a href="https://github.com/simpleyyt/jekyll-theme-next">jekyll-theme-next</a>，但网上关于 Jekyll 版的 Next 主题优化教程少之又少，于是就决定自己写一篇以供参考。</p><blockquote><p>本文仅讲述 Next (Jekyll) 主题的深度优化操作，关于主题的基础配置请移步<a href="http://theme-next.simpleyyt.com/">官方文档</a>。</p></blockquote><a id="more"></a><h1 id="主题优化"><a href="#主题优化" class="headerlink" title="主题优化"></a>主题优化</h1><h2 id="修改内容区域的宽度"><a href="#修改内容区域的宽度" class="headerlink" title="修改内容区域的宽度"></a>修改内容区域的宽度</h2><p>打开 <code>_sass/_custom/custom.scss</code> 文件，新增变量：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改成你期望的宽度</span></span><br><span class="line">$content-desktop = 700px</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当视窗超过 1600px 后的宽度</span></span><br><span class="line">$content-desktop-large = 900px</span><br></pre></td></tr></table></figure><blockquote><p>此方法不适用于 Pisces Scheme</p></blockquote><p>当你使用Pisces风格时可以用下面的方法：</p><ul><li><p>编辑 Pisces Scheme 的  <code>_sass/_schemes/Pisces/_layout.scss</code> 文件，在最底部添加如下代码：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">header</span>&#123; <span class="attribute">width</span>: <span class="number">90%</span>; &#125;</span><br><span class="line"><span class="selector-class">.container</span> <span class="selector-class">.main-inner</span> &#123; <span class="attribute">width</span>: <span class="number">90%</span>; &#125;</span><br><span class="line"><span class="selector-class">.content-wrap</span> &#123; <span class="attribute">width</span>: calc(<span class="number">100%</span> - <span class="number">260px</span>); &#125;</span><br></pre></td></tr></table></figure><blockquote><p>对于有些浏览器或是移动设备，效果可能不是太好</p></blockquote></li><li><p>编辑 Pisces Scheme 的  <code>_sass/_schemes/Pisces/_layout.scss</code> 文件，修改以下内容：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将 .header 中的 </span></span><br><span class="line"><span class="attribute">width</span>: <span class="variable">$main-desktop</span>;</span><br><span class="line"><span class="comment">// 改为：</span></span><br><span class="line"><span class="attribute">width</span>: <span class="number">80%</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 .container .main-inner 中的：</span></span><br><span class="line"><span class="attribute">width</span>: <span class="variable">$main-desktop</span>;</span><br><span class="line"><span class="comment">// 改为：</span></span><br><span class="line"><span class="attribute">width</span>: <span class="number">80%</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 .content-wrap 中的：</span></span><br><span class="line"><span class="attribute">width</span>: <span class="variable">$content-desktop</span>;</span><br><span class="line"><span class="comment">// 改为：</span></span><br><span class="line"><span class="attribute">width</span>: calc(<span class="number">100%</span> - <span class="number">260px</span>);</span><br></pre></td></tr></table></figure><blockquote><p>还是不知道如何修改的话，可以直接赋值笔者改好的 👉 <a href="https://github.com/laugh12321/blog/blob/master/_sass/_schemes/Pisces/_layout.scss">传送门</a></p></blockquote></li></ul><h2 id="背景透明度"><a href="#背景透明度" class="headerlink" title="背景透明度"></a>背景透明度</h2><p>打开 <code>_sass/_custom/custom.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文章内容背景改成了半透明</span></span><br><span class="line"><span class="selector-class">.content-wrap</span> &#123;</span><br><span class="line">    <span class="attribute">background</span>: rgba(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.8</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.sidebar</span> &#123;</span><br><span class="line">    <span class="attribute">background</span>: rgba(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.1</span>);</span><br><span class="line">    <span class="attribute">box-shadow</span>: <span class="number">0</span> <span class="number">2px</span> <span class="number">6px</span> <span class="number">#dbdbdb</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.site-nav</span>&#123;</span><br><span class="line">    <span class="attribute">box-shadow</span>: <span class="number">0px</span> <span class="number">0px</span> <span class="number">0px</span> <span class="number">0px</span> rgba(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.8</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.sidebar-inner</span> &#123;</span><br><span class="line"><span class="attribute">background</span>: rgba(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.8</span>);</span><br><span class="line"><span class="attribute">box-shadow</span>: <span class="number">0</span> <span class="number">2px</span> <span class="number">6px</span> <span class="number">#dbdbdb</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.header-inner</span> &#123;</span><br><span class="line">    <span class="attribute">background</span>: rgba(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.8</span>);</span><br><span class="line">    <span class="attribute">box-shadow</span>: <span class="number">0</span> <span class="number">2px</span> <span class="number">6px</span> <span class="number">#dbdbdb</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.footer</span> &#123;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">14px</span>;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#434343</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="自定义背景图片"><a href="#自定义背景图片" class="headerlink" title="自定义背景图片"></a>自定义背景图片</h2><p>打开 <code>_sass/_custom/custom.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">body</span>&#123;</span><br><span class="line">    <span class="attribute">background</span>:url(https://images8.alphacoders.com/<span class="number">929</span>/<span class="number">929202</span>.jpg);</span><br><span class="line">    <span class="attribute">background-size</span>:cover;</span><br><span class="line">    <span class="attribute">background-repeat</span>:no-repeat;</span><br><span class="line">    <span class="attribute">background-attachment</span>:fixed;</span><br><span class="line">    <span class="attribute">background-position</span>:center;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>url()</code> 中可以时本地图片，也可以是图片链接</p></blockquote><h2 id="彩色时间轴"><a href="#彩色时间轴" class="headerlink" title="彩色时间轴"></a>彩色时间轴</h2><p>打开 <code>_sass/_custom/custom.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 时间轴样式</span></span><br><span class="line"><span class="selector-class">.posts-collapse</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">50px</span> <span class="number">0px</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">@media</span> (max-width: <span class="number">1023px</span>) &#123;</span><br><span class="line">    <span class="selector-class">.posts-collapse</span> &#123;</span><br><span class="line">        <span class="attribute">margin</span>: <span class="number">50px</span> <span class="number">20px</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 时间轴左边线条</span></span><br><span class="line"><span class="selector-class">.posts-collapse</span><span class="selector-pseudo">::after</span> &#123;</span><br><span class="line">    <span class="attribute">margin-left</span>: -<span class="number">2px</span>;</span><br><span class="line">    <span class="attribute">background-image</span>: linear-gradient(<span class="number">180deg</span>,<span class="number">#f79533</span> <span class="number">0</span>,<span class="number">#f37055</span> <span class="number">15%</span>,<span class="number">#ef4e7b</span> <span class="number">30%</span>,<span class="number">#a166ab</span> <span class="number">44%</span>,<span class="number">#5073b8</span> <span class="number">58%</span>,<span class="number">#1098ad</span> <span class="number">72%</span>,<span class="number">#07b39b</span> <span class="number">86%</span>,<span class="number">#6dba82</span> <span class="number">100%</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 时间轴左边线条圆点颜色</span></span><br><span class="line"><span class="selector-class">.posts-collapse</span> <span class="selector-class">.collection-title</span><span class="selector-pseudo">::before</span> &#123;</span><br><span class="line">    <span class="attribute">background-color</span>: rgb(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 时间轴文章标题左边圆点颜色</span></span><br><span class="line"><span class="selector-class">.posts-collapse</span> <span class="selector-class">.post-header</span><span class="selector-pseudo">:hover</span><span class="selector-pseudo">::before</span> &#123;</span><br><span class="line">    <span class="attribute">background-color</span>: rgb(<span class="number">161</span>, <span class="number">102</span>, <span class="number">171</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 时间轴年份</span></span><br><span class="line"><span class="selector-class">.posts-collapse</span> <span class="selector-class">.collection-title</span> <span class="selector-tag">h1</span>, <span class="selector-class">.posts-collapse</span> <span class="selector-class">.collection-title</span> <span class="selector-tag">h2</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: rgb(<span class="number">102</span>, <span class="number">102</span>, <span class="number">102</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 时间轴文章标题</span></span><br><span class="line"><span class="selector-class">.posts-collapse</span> <span class="selector-class">.post-title</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: rgb(<span class="number">80</span>, <span class="number">115</span>, <span class="number">184</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.posts-collapse</span> <span class="selector-class">.post-title</span> <span class="selector-tag">a</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: rgb(<span class="number">161</span>, <span class="number">102</span>, <span class="number">171</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 时间轴文章标题底部虚线</span></span><br><span class="line"><span class="selector-class">.posts-collapse</span> <span class="selector-class">.post-header</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">border-bottom-color</span>: rgb(<span class="number">161</span>, <span class="number">102</span>, <span class="number">171</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// archives页面顶部文字</span></span><br><span class="line"><span class="selector-class">.page-archive</span> <span class="selector-class">.archive-page-counter</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: rgb(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// archives页面时间轴左边线条第一个圆点颜色</span></span><br><span class="line"><span class="selector-class">.page-archive</span> <span class="selector-class">.posts-collapse</span> <span class="selector-class">.archive-move-on</span> &#123;</span><br><span class="line">    <span class="attribute">top</span>: <span class="number">10px</span>;</span><br><span class="line">    <span class="attribute">opacity</span>: <span class="number">1</span>;</span><br><span class="line">    <span class="attribute">background-color</span>: rgb(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>);</span><br><span class="line">    <span class="attribute">box-shadow</span>: <span class="number">0px</span> <span class="number">0px</span> <span class="number">10px</span> <span class="number">0px</span> rgba(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.5</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="友链居中"><a href="#友链居中" class="headerlink" title="友链居中"></a>友链居中</h2><p>打开 <code>_sass/_custom/custom.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//友链居中</span></span><br><span class="line"><span class="selector-class">.links-of-blogroll-title</span> &#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="修改友链文本颜色"><a href="#修改友链文本颜色" class="headerlink" title="修改友链文本颜色"></a>修改友链文本颜色</h2><p>打开 <code>_sass/_custom/custom.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//友链文本颜色</span></span><br><span class="line"><span class="comment">//将链接文本设置为蓝色，鼠标划过时文字颜色加深，并显示下划线</span></span><br><span class="line"><span class="selector-class">.post-body</span> <span class="selector-tag">p</span> <span class="selector-tag">a</span>&#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#434343</span>;</span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line">  &amp;<span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#5073b8</span>;</span><br><span class="line">    <span class="attribute">text-decoration</span>: underline;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="修改友链样式"><a href="#修改友链样式" class="headerlink" title="修改友链样式"></a>修改友链样式</h2><p>打开 <code>_sass/_custom/custom.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//修改友情链接样式</span></span><br><span class="line"><span class="selector-class">.links-of-blogroll-item</span> <span class="selector-tag">a</span>&#123;</span><br><span class="line">  <span class="attribute">text-align</span>: center;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#434343</span>;</span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line">  &amp;<span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#5073b8</span>;</span><br><span class="line">    <span class="attribute">text-decoration</span>: underline;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="自定义页脚的心样式"><a href="#自定义页脚的心样式" class="headerlink" title="自定义页脚的心样式"></a>自定义页脚的心样式</h2><p>打开 <code>_sass/_custom/custom.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义页脚的心样式</span></span><br><span class="line"><span class="keyword">@keyframes</span> heartAnimate &#123;</span><br><span class="line">    0%,100%&#123;<span class="attribute">transform</span>:scale(<span class="number">1</span>);&#125;</span><br><span class="line">    10%,30%&#123;<span class="attribute">transform</span>:scale(<span class="number">0.9</span>);&#125;</span><br><span class="line">    20%,40%,60%,80%&#123;<span class="attribute">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">    50%,70%&#123;<span class="attribute">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-id">#heart</span> &#123;</span><br><span class="line">    <span class="attribute">animation</span>: heartAnimate <span class="number">1.33s</span> ease-in-out infinite;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.with-love</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: rgb(<span class="number">255</span>, <span class="number">113</span>, <span class="number">168</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="设置头像边框为圆形框"><a href="#设置头像边框为圆形框" class="headerlink" title="设置头像边框为圆形框"></a>设置头像边框为圆形框</h2><p>打开 <code>_sass/_common/components/sidebar/sidebar-author.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.site-author-image</span> &#123;</span><br><span class="line">  <span class="attribute">display</span>: block;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">0</span> auto;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="variable">$site-author-image-padding</span>;</span><br><span class="line">  <span class="attribute">max-width</span>: <span class="variable">$site-author-image-width</span>;</span><br><span class="line">  <span class="attribute">height</span>: <span class="variable">$site-author-image-height</span>;</span><br><span class="line">  <span class="attribute">border</span>: <span class="variable">$site-author-image-border-width</span> solid <span class="variable">$site-author-image-border-color</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 头像圆形 */</span></span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="number">80px</span>;</span><br><span class="line">  -webkit-<span class="attribute">border-radius</span>: <span class="number">80px</span>;</span><br><span class="line">  -moz-<span class="attribute">border-radius</span>: <span class="number">80px</span>;</span><br><span class="line">  <span class="attribute">box-shadow</span>: inset <span class="number">0</span> -<span class="number">1px</span> <span class="number">0</span> <span class="number">#333</span>sf;</span><br></pre></td></tr></table></figure><h2 id="特效：鼠标放置头像上旋转"><a href="#特效：鼠标放置头像上旋转" class="headerlink" title="特效：鼠标放置头像上旋转"></a>特效：鼠标放置头像上旋转</h2><p>打开 <code>_sass/_common/components/sidebar/sidebar-author.scss</code> 文件，新增变量：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 </span></span><br><span class="line"><span class="comment">    (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  -webkit-<span class="attribute">transition</span>: -webkit-transform <span class="number">1.0s</span> ease-out;</span><br><span class="line">  -moz-<span class="attribute">transition</span>: -moz-transform <span class="number">1.0s</span> ease-out;</span><br><span class="line">  <span class="attribute">transition</span>: transform <span class="number">1.0s</span> ease-out;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">img</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">  <span class="comment">/* 鼠标经过停止头像旋转 </span></span><br><span class="line"><span class="comment">  -webkit-animation-play-state:paused;</span></span><br><span class="line"><span class="comment">  animation-play-state:paused;*/</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  -webkit-<span class="attribute">transform</span>: rotateZ(<span class="number">360deg</span>);</span><br><span class="line">  -moz-<span class="attribute">transform</span>: rotateZ(<span class="number">360deg</span>);</span><br><span class="line">  <span class="attribute">transform</span>: rotateZ(<span class="number">360deg</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Z 轴旋转动画 */</span></span><br><span class="line"><span class="keyword">@-webkit-keyframes</span> play &#123;</span><br><span class="line">  0% &#123;</span><br><span class="line">    -webkit-<span class="attribute">transform</span>: rotateZ(<span class="number">0deg</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  100% &#123;</span><br><span class="line">    -webkit-<span class="attribute">transform</span>: rotateZ(-<span class="number">360deg</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">@-moz-keyframes</span> play &#123;</span><br><span class="line">  0% &#123;</span><br><span class="line">    -moz-<span class="attribute">transform</span>: rotateZ(<span class="number">0deg</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  100% &#123;</span><br><span class="line">    -moz-<span class="attribute">transform</span>: rotateZ(-<span class="number">360deg</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">@keyframes</span> play &#123;</span><br><span class="line">  0% &#123;</span><br><span class="line">    <span class="attribute">transform</span>: rotateZ(<span class="number">0deg</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  100% &#123;</span><br><span class="line">    <span class="attribute">transform</span>: rotateZ(-<span class="number">360deg</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Bug-修复"><a href="#Bug-修复" class="headerlink" title="Bug 修复"></a>Bug 修复</h1><h2 id="打赏文字抖动修复"><a href="#打赏文字抖动修复" class="headerlink" title="打赏文字抖动修复"></a>打赏文字抖动修复</h2><p>打开  <code>_sass/_common/components/post/post-reward.scss</code> 文件，然后注释其中的函数 <code>wechat:hover</code> 和 <code>alipay:hover</code> ，如下：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  #wechat:hover p&#123;</span></span><br><span class="line"><span class="comment">      animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">      -webkit-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">      -moz-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">  #alipay:hover p&#123;</span></span><br><span class="line"><span class="comment">      animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">      -webkit-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">      -moz-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h2 id="修改文章底部的带-号标签"><a href="#修改文章底部的带-号标签" class="headerlink" title="修改文章底部的带#号标签"></a>修改文章底部的带#号标签</h2><p>打开 <code>_includes/_macro/post.html</code> 文件,搜索 <code>rel=&quot;tag&quot;&gt;#</code> ,将 <code>#</code> 换成 <code>&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;post-tags&quot;</span>&gt;</span></span><br><span class="line">    &#123;% for tag in post.tags %&#125;</span><br><span class="line">    &#123;% assign tag_url_encode = tag | url_encode | replace: &#x27;+&#x27;, &#x27;%20&#x27; %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;&#123;&#123; &#x27;/tag/#/&#x27; | relative_url | append: tag_url_encode &#125;&#125;&quot;</span> <span class="attr">rel</span>=<span class="string">&quot;tag&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fa fa-tag&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span> &#123;&#123; tag &#125;&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    &#123;% endfor %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="插件配置"><a href="#插件配置" class="headerlink" title="插件配置"></a>插件配置</h1><h2 id="阅读次数统计（LeanCloud）"><a href="#阅读次数统计（LeanCloud）" class="headerlink" title="阅读次数统计（LeanCloud）"></a>阅读次数统计（LeanCloud）</h2><ul><li><p>请查看 <a href="https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud">为NexT主题添加文章阅读量统计功能</a></p></li><li><p>打开 <code>config.yml</code> 文件，搜索 <code>leancloud_visitors</code> , 进行如下更改：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">leancloud_visitors:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">app_id:</span> <span class="string">&lt;app_id&gt;</span></span><br><span class="line">  <span class="attr">app_key:</span> <span class="string">&lt;app_key&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p><code>app_id</code> 和 <code>app_key</code> 分别是 你的LearnCloud 账号的 <code>AppID</code> 和 <code>AppKey</code></p></blockquote></li></ul><h2 id="阅读次数美化"><a href="#阅读次数美化" class="headerlink" title="阅读次数美化"></a>阅读次数美化</h2><p>效果👉：<img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Jekyll-theme-next_00.png"></p><ul><li><p>打开 <code>_data/languages/zh-Hans.yml</code> 文件，将 <code>post</code> 中的 <code>visitors:阅读次数 </code> 改为：<code>visitors: 热度</code>。</p></li><li><p>打开 <code>_includes/_macro/post.html</code> 文件,搜索 <code>leancloud-visitors-count</code> ,在 <code>&lt;span&gt;&lt;/span&gt;</code> 之间添加 <code>℃</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;&#123;&#123; post.url | relative_url &#125;&#125;&quot;</span> <span class="attr">class</span>=<span class="string">&quot;leancloud_visitors&quot;</span> <span class="attr">data-flag-title</span>=<span class="string">&quot;&#123;&#123; post.title &#125;&#125;&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-meta-divider&quot;</span>&gt;</span>|<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-meta-item-icon&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fa fa-eye&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">  &#123;% if site.post_meta.item_text %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-meta-item-text&quot;</span>&gt;</span>&#123;&#123;__.post.visitors&#125;&#125; <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;leancloud-visitors-count&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span>&gt;</span>℃<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="在网站底部加上访问量"><a href="#在网站底部加上访问量" class="headerlink" title="在网站底部加上访问量"></a>在网站底部加上访问量</h2><p>效果👉：<img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Jekyll-theme-next_01.png"></p><ul><li><p>打开 <code>_includes/_partials/footer.html</code> 文件，在 <code>&lt;div class=&quot;copyright&quot; &gt;</code> 之前加入下面的代码：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">async</span> <span class="attr">src</span>=<span class="string">&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>在 <code>if site.copyright </code> 之后加入下面的代码：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;powered-by&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fa fa-user-md&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;busuanzi_container_site_uv&quot;</span>&gt;</span></span><br><span class="line">  本站访客数:<span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;busuanzi_value_site_uv&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><hr><blockquote><p>部分样式可参考我的博客👉：<a href="https://www.laugh12321.cn/">Laugh’s Blog</a></p><p>参考文章：</p><p><a href="http://www.dragonstyle.win/3358042383.html">Hexo+Next主题优化</a></p><p><a href="http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html">hexo的next主题个性化教程:打造炫酷网站</a></p><p>参考博客：</p><p><a href="https://www.ds-vip.top/">DS Blog</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;笔者在用 Jekyll 搭建个人博客时踩了很多的坑，最后发现了一款不错的主题 &lt;a href=&quot;https://github.com/simpleyyt/jekyll-theme-next&quot;&gt;jekyll-theme-next&lt;/a&gt;，但网上关于 Jekyll 版的 Next 主题优化教程少之又少，于是就决定自己写一篇以供参考。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文仅讲述 Next (Jekyll) 主题的深度优化操作，关于主题的基础配置请移步&lt;a href=&quot;http://theme-next.simpleyyt.com/&quot;&gt;官方文档&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Blog" scheme="http://www.laugh12321.cn/blog/categories/Blog/"/>
    
    
    <category term="Blog" scheme="http://www.laugh12321.cn/blog/tags/Blog/"/>
    
    <category term="GitHub Pages" scheme="http://www.laugh12321.cn/blog/tags/GitHub-Pages/"/>
    
    <category term="Jekyll" scheme="http://www.laugh12321.cn/blog/tags/Jekyll/"/>
    
    <category term="Next" scheme="http://www.laugh12321.cn/blog/tags/Next/"/>
    
  </entry>
  
  <entry>
    <title>一站式搭建 GitHub Pages 博客</title>
    <link href="http://www.laugh12321.cn/blog/2018/12/16/build_the_blog/"/>
    <id>http://www.laugh12321.cn/blog/2018/12/16/build_the_blog/</id>
    <published>2018-12-16T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>本文将详细讲解如何快速搭建 GitHub Pages 博客页面</p><p>关于博客主题，博客信息更改，上传文章等将会在 一站式搭建 GitHub Pages 博客 (二) 中进行详细讲解</p><a id="more"></a><h2 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h2><ul><li>注册 GitHub 账号</li><li>安装 GitHub Desktop</li></ul><h2 id="在-GitHub-建立名为-username-github-io-的仓库"><a href="#在-GitHub-建立名为-username-github-io-的仓库" class="headerlink" title="在 GitHub 建立名为 username.github.io 的仓库"></a>在 GitHub 建立名为 username.github.io 的仓库</h2><p>首先登陆 GitHub 账户，点击右上角的 ➕  ，选择New  repository.<br><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/myblog_00.png"></p><p>点击后，按照图示进行填写：<br><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/myblog_01.png"></p><p><strong><font color='red'>[注意]</font></strong><br>Repository name 格式：username.github.io</p><h2 id="将仓库变更为-GitHub-Pages"><a href="#将仓库变更为-GitHub-Pages" class="headerlink" title="将仓库变更为 GitHub Pages"></a>将仓库变更为 GitHub Pages</h2><p>本地仓库建好后点击 Settings<br><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/myblog_02.png"></p><p>将页面下拉到 GitHub Pages, 将 None 改为 master branch 并保存<br><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/myblog_03.png"></p><p>如果发现更改不了，则点击 Change theme 随便选择一个主题就可以啦~</p><p>这是更改后的效果图：<br><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/myblog_04.png"><br>上面的链接就是你的个人博客地址，到这里简易的个人博客就已经搭建完成了！</p><blockquote><p>在 一站式搭建 GitHub Pages 博客 (二) 中， 笔者将会讲解个人博客的详细配置。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文将详细讲解如何快速搭建 GitHub Pages 博客页面&lt;/p&gt;
&lt;p&gt;关于博客主题，博客信息更改，上传文章等将会在 一站式搭建 GitHub Pages 博客 (二) 中进行详细讲解&lt;/p&gt;</summary>
    
    
    
    <category term="Blog" scheme="http://www.laugh12321.cn/blog/categories/Blog/"/>
    
    
    <category term="Blog" scheme="http://www.laugh12321.cn/blog/tags/Blog/"/>
    
    <category term="GitHub Pages" scheme="http://www.laugh12321.cn/blog/tags/GitHub-Pages/"/>
    
  </entry>
  
  <entry>
    <title>【排序算法】—— 更快的排序</title>
    <link href="http://www.laugh12321.cn/blog/2018/11/30/python_faster_sort/"/>
    <id>http://www.laugh12321.cn/blog/2018/11/30/python_faster_sort/</id>
    <published>2018-11-30T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇<a href="http://laugh12321.cn/2018/11/28/python-basic-sort/">【Python 排序算法】—— 基本排序算法</a>中，介绍的 3 中排序算法都拥有 $O(n^2)$ 的运行时间。这些排序算法还有几种变体，其中的稍微快一些。但是，在最坏的情况和平均情况下，它们的性能还是 $O(n^2)$。然而，我们可以利用一些复杂度为 $O(nlogn)$ 的更好的算法。这些更好的算法的秘诀就是，采用分而治之$(divide-and-conquer)$的策略。也就是说，每一个算法都找了一种方法，将列表分解为更小的子列表。随后，这些子列表在递归地排序。理想情况下，如果这些子列表的复杂度为 $log(n)$，而重新排列每一个子列表中的数据所需的工作量为 $n$，那么，这样的排序算法总的复杂度就是 $O(nlogn)$。</p><a id="more"></a><p>这里将介绍两种排序算法，他们都突破了 $n^2$ 复杂度的障碍，它们是快速排序和合并排序。</p><h2 id="快速排序简介"><a href="#快速排序简介" class="headerlink" title="快速排序简介"></a>快速排序简介</h2><p>快速排序所使用的策略可以概括如下：</p><ol><li>首先，从列表的中点位置选取一项。这一项叫做基准点$(pivot)$。</li><li>将列表中的项分区，以便小于基准点的所有项都移动到基准点的左边，而剩下的项都移动到基准点的右边。根据相关的实际项，基准点自身的最终位置也是变化的。例如，如果基准点自身是最大的项，它会位于列表的最右边，如果基准点是最小值，它会位于最左边。但是，不管基准点最终位于何处，这个位置都是它在完全排序的列表中的最终位置。</li><li>分而治之。对于在基准点分割列表而形成的子列表，递归地重复应用该过程。一个子列表包含了基准点左边的所有的项（现在是最小的项），另一个子列表包含了基准点右边的所有的项（现在是较大的项）。</li><li>每次遇到少于2个项的一个子列表，就结束这个过程。</li></ol><h3 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h3><p>该算法最复杂的部分就是对子列表中的项进行分割的操作。有两种主要的方式用来进行分割。有一种方法较为容易，如何对任何子列表应用该方法的步骤如下：</p><ol><li>将基准点和子列表的最后一项交换。</li><li>在已知小于基准点的项和剩余的项之间建立一个边界。一开始，这个边界就放在第 1 个项之前。</li><li>从子列表中的第 1 项开始，扫描整个子列表。每次遇到小于基准点的项，就将其与边界之后的第 1 项交换，且边界向后移动。</li><li>将基准点和边界之后的第 1 项交换，从而完成这个过程。</li></ol><p>下图说明了对于数字<code>12, 19, 17, 18, 14, 11, 15, 13</code> 和 <code>16</code> 应用这些步骤的过程。在第 1 步中，建立了基准点并且将其与最后一项交换。在第 2 步中，在第 1 项之前建立了边界。在第 3 步到第 6 步，扫描了子列表以找到比基准点小的项，这些项将要和边界之后的第 1 项交换，并且边界向后移动。注意，边界左边的项总是小于基准点。最后，在第 7 步中，基准点和边界之后的第 1 项交换，子列表已经成功第分割好了。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/python_basic_sort_01.png"></p><p>在分割好一个子列表之后，对于左边和右边的子列表 （12，11，13 和 16，19，15，17，18）重复应用这个过程，直到子列表的长度最大为 1。</p><h3 id="快速排序的实现"><a href="#快速排序的实现" class="headerlink" title="快速排序的实现"></a>快速排序的实现</h3><p>快速排序使用递归算法更容易编码。如下脚本定义了一个顶层的<code>quicksort</code> 函数；一个递归的<code>quicksortHelper</code>函数，它隐藏了用与子列表终点的额外参数；还有一个<code>partition</code>函数。如下脚本实在20个随机排序的整数组成的一个列表上执行快速排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quicksort</span>(<span class="params">lyst</span>):</span></span><br><span class="line">    quicksortHelper(lyst, <span class="number">0</span>, <span class="built_in">len</span>(lyst) - <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quicksortHelper</span>(<span class="params">lyst, left, right</span>):</span></span><br><span class="line">    <span class="keyword">if</span> left &lt; right:</span><br><span class="line">        pivotLocation = partition(lyst, left, right)</span><br><span class="line">        quicksortHelper(lyst, left, pivotLocation - <span class="number">1</span>)</span><br><span class="line">        quicksortHelper(lyst, pivotLocation + <span class="number">1</span>, right)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span>(<span class="params">lyst, left, right</span>):</span></span><br><span class="line">    <span class="comment"># Find the pivot and exchange it with the last item</span></span><br><span class="line">    middle = (left + right) // <span class="number">2</span></span><br><span class="line">    pivot = lyst[middle]</span><br><span class="line">    lyst[middle] = lyst[right]</span><br><span class="line">    lyst[right] = pivot</span><br><span class="line">    <span class="comment"># Set boundary point to first position</span></span><br><span class="line">    boundary = left</span><br><span class="line">    <span class="comment"># Move items less than pivot to the left</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(left, right):</span><br><span class="line">        <span class="keyword">if</span> lyst[index] &lt; pivot:</span><br><span class="line">            swap(lyst, index, boundary)</span><br><span class="line">            boundary += <span class="number">1</span></span><br><span class="line">    <span class="comment"># Exchange the pivot item and the boundary item</span></span><br><span class="line">    swap(lyst, right, boundary)</span><br><span class="line">    <span class="keyword">return</span> boundary</span><br><span class="line"></span><br><span class="line"><span class="comment"># Earlier definition of the swap function goes here</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">size = <span class="number">20</span>, sort = quicksort</span>):</span></span><br><span class="line">    lyst = []</span><br><span class="line">    <span class="keyword">for</span> count <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">        lyst.append(random.randint(<span class="number">1</span>, size + <span class="number">1</span>))</span><br><span class="line">    print(lyst)</span><br><span class="line">    sort(lyst)</span><br><span class="line">    print(lyst)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;在上篇&lt;a href=&quot;http://laugh12321.cn/2018/11/28/python-basic-sort/&quot;&gt;【Python 排序算法】—— 基本排序算法&lt;/a&gt;中，介绍的 3 中排序算法都拥有 $O(n^2)$ 的运行时间。这些排序算法还有几种变体，其中的稍微快一些。但是，在最坏的情况和平均情况下，它们的性能还是 $O(n^2)$。然而，我们可以利用一些复杂度为 $O(nlogn)$ 的更好的算法。这些更好的算法的秘诀就是，采用分而治之$(divide-and-conquer)$的策略。也就是说，每一个算法都找了一种方法，将列表分解为更小的子列表。随后，这些子列表在递归地排序。理想情况下，如果这些子列表的复杂度为 $log(n)$，而重新排列每一个子列表中的数据所需的工作量为 $n$，那么，这样的排序算法总的复杂度就是 $O(nlogn)$。&lt;/p&gt;</summary>
    
    
    
    <category term="Algorithm" scheme="http://www.laugh12321.cn/blog/categories/Algorithm/"/>
    
    <category term="Sorting algorithm" scheme="http://www.laugh12321.cn/blog/categories/Algorithm/Sorting-algorithm/"/>
    
    
    <category term="Algorithm" scheme="http://www.laugh12321.cn/blog/tags/Algorithm/"/>
    
    <category term="Sorting algorithm" scheme="http://www.laugh12321.cn/blog/tags/Sorting-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>【排序算法】—— 基本排序算法</title>
    <link href="http://www.laugh12321.cn/blog/2018/11/28/python_basic_sort/"/>
    <id>http://www.laugh12321.cn/blog/2018/11/28/python_basic_sort/</id>
    <published>2018-11-28T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>​        在讲解基础的排序算法之前，先来介绍排序算法经常会用到的 $ swap $ 函数——用来交换列表中的两项的位置</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">swap</span>(<span class="params">lyst, i, j</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Exchanges the items at positions i and j.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># You could say lyst[i], lyst[j] = lyst[j], lyst[i]</span></span><br><span class="line">    <span class="comment"># but the following code shows what is really going on</span></span><br><span class="line">    temp = lyst[i]</span><br><span class="line">    lyst[i] = lyst[j]</span><br><span class="line">    lyst[j] = temp</span><br></pre></td></tr></table></figure><h2 id="选择排序-selection-sort"><a href="#选择排序-selection-sort" class="headerlink" title="选择排序 | $ selection , sort $"></a>选择排序 | $ selection , sort $</h2><p>​        可能最简单的策略就是搜索整个列表，找到最小项的位置。如果该位置不是列表的第一个位置，算法就会交换这两个位置的项。然后，算法回到第 2 个位置并且重复这个过程，如果必要的话，将最小项和第 2 个位置的项交换。当算法到达整个过程的最后一个位置，列表就是排序好的了。这就是选择排序算法的基本思路。<br>​        下表展示了对于 5 个项的一个列表进行选择排序，在每一次搜索和交换之后的状态。因为每次经过主循环时，都会选择一个要移动的项，即在每一轮都只是交换两项，这两项的后面用 <code>* </code> 表示，并且表中已经排好序的部分用阴影表示。</p><table>    <tr>        <th>未排序的列表</th><th>第 1 轮后</th><th>第 2 轮后</th><th>第 3 轮后</th><th>第 4 轮后</th>    </tr>    <tr>        <td><p>5</p></td><td bgcolor="DarkGray"><p>1*</p></td><td bgcolor="DarkGray"><p>1</p></td><td bgcolor="DarkGray"><p>1</p></td><td bgcolor="DarkGray"><p>1</p></td>    </tr>    <tr>        <td><p>3</p></td><td><p>3</p></td><td bgcolor="DarkGray"><p>2*</p></td><td bgcolor="DarkGray"><p>2</p></td><td bgcolor="DarkGray"><p>2</p></td>    </tr>    <tr>        <td><p>1</p></td><td><p>5*</p></td><td><p>5</p></td><td bgcolor="DarkGray"><p>3*</p></td><td bgcolor="DarkGray"><p>3</p></td>    </tr>    <tr>        <td><p>2</p></td><td><p>2</p></td><td><p>3*</p></td><td><p>5*</p></td><td bgcolor="DarkGray"><p>4*</p></td>    </tr>    <tr>        <td><p>4</p></td><td><p>4</p></td><td><p>4</p></td><td><p>4</p></td><td><p>5*</p></td>    </tr></table><p>如下是选择排序的 Python 描述：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectionSort</span>(<span class="params">lyst</span>):</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(lyst) - <span class="number">1</span>:      <span class="comment"># Do n - 1 searches</span></span><br><span class="line">        minIndex = i              <span class="comment"># for the smallest</span></span><br><span class="line">        j = i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> j &lt; <span class="built_in">len</span>(lyst):      <span class="comment">#  Start a search</span></span><br><span class="line">            <span class="keyword">if</span> lyst[j] &lt; lyst[minIndex]:</span><br><span class="line">                minIndex = j</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> minIndex != i:         <span class="comment"># Exchange if need</span></span><br><span class="line">            swap(lyst, minIndex, i)</span><br><span class="line">        i += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>​        该算法的复杂度为 $ O(n^2) $。由于数据项交换只是在外围循环中进行，所以在最坏情况和平均情况下，选择排序的这一额外开销都是线性的。</p><h2 id="冒泡排序-bubble-sort"><a href="#冒泡排序-bubble-sort" class="headerlink" title="冒泡排序 | $ bubble ,sort $"></a>冒泡排序 | $ bubble ,sort $</h2><p>​        冒泡排序法相对容易理解和编码。其策略是从列表的开头出开始，并且比较一对数据项，直到移动到列表的末尾。每当成对的两项之间顺序不正确的时候，算法就交换其位置。这个过程的效果就是将最大的项以冒泡的方式排到列表的末尾。然后，算法从列表头到倒数第 2 个列表项重复这一过程，依此类推，直到该算法从列表的最后一项开始执行。此时，列表是已经排序好的。<br>​        下表展示了对 5 个项的一个列表进行冒泡排序的过程。这个过程把嵌套的循环执行了 4 次，将最大的项冒泡的列表的末尾。再一次，只有交换的项用<code>*</code>标出，并且排好序的部分用阴影表示。</p><table>    <tr>        <th>未排序的列表</th><th>第 1 轮后</th><th>第 2 轮后</th><th>第 3 轮后</th><th>第 4 轮后</th>    </tr>    <tr>        <td><p>5</p></td><td><p>4*</p></td><td><p>4</p></td><td><p>4</p></td><td><p>4</p></td>    </tr>    <tr>        <td><p>4</p></td><td><p>5*</p></td><td><p>2*</p></td><td><p>2</p></td><td><p>2</p></td>    </tr>    <tr>        <td><p>2</p></td><td><p>2</p></td><td><p>5*</p></td><td><p>1*</p></td><td><p>1</p></td>    </tr>    <tr>        <td><p>1</p></td><td><p>1</p></td><td><p>1</p></td><td><p>5*</p></td><td><p>3</p></td>    </tr>    <tr>        <td><p>3</p></td><td><p>3</p></td><td><p>3</p></td><td><p>3</p></td><td bgcolor="DarkGray"><p>5*</p></td>    </tr></table>如下是冒泡排序的 Python 描述：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubbleSort</span>(<span class="params">lyst</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(lyst)   </span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">1</span>:                           <span class="comment"># Do n - 1 bubbles</span></span><br><span class="line">        i = <span class="number">1</span>                              <span class="comment"># Start each bubble</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n:</span><br><span class="line">            <span class="keyword">if</span> lyst[i] &lt; lyst[i - <span class="number">1</span>]:      <span class="comment"># Exchange if need</span></span><br><span class="line">                swap(lyst, i, i - <span class="number">1</span>)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        n -= <span class="number">1</span></span><br></pre></td></tr></table></figure><p>​        和选择排序一样，冒泡排序的复杂度也是 $ O(n^2) $。 如果列表是已经排好的，冒泡排序不会执行任何交换。然而，在最坏的情况下，冒泡排序的交换超过线性方式。<br>​        可以对冒泡排序进行一个小的调整，将其在最好的情况下的性能提高到线性阶。如果在通过主循环的时候，没有发生交换，那么列表就是已经排序的。这种情况可能发生在任何一轮，但是，在最好的情况下，第 1 轮就会发生。可以使用一个布尔标记来记录交换动作的出现，并且当内部循环没有设置这个标记的时候，就从函数返回。如下是修改后的冒泡排序函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubbleSortWithTweak</span>(<span class="params">lyst</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(lyst)</span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">1</span>:</span><br><span class="line">        swapped = <span class="literal">False</span></span><br><span class="line">        i = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n:</span><br><span class="line">            <span class="keyword">if</span> lyst[i] &lt; lyst[i - <span class="number">1</span>]:       <span class="comment"># Exchange if need</span></span><br><span class="line">                swap(lyst, i, i - <span class="number">1</span>)</span><br><span class="line">                swapped = <span class="literal">True</span></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> swapped: <span class="keyword">return</span>              <span class="comment"># Return if no swaps</span></span><br><span class="line">        n -= <span class="number">1</span></span><br></pre></td></tr></table></figure><p><font color='red'><strong>注意：</strong> </font> 这一修改只是改进了最好情况下的行为。在平均情况下，这个版本的复杂度仍为 $ O(n^2) $。</p><h2 id="插入排序-insertion-sort"><a href="#插入排序-insertion-sort" class="headerlink" title="插入排序 | $ insertion , sort $"></a>插入排序 | $ insertion , sort $</h2><p>​        修改过后的冒泡排序，对于已经排好的列表来说，其性能比选择排序要好。但是，如果列表中的项是没有顺序的，修改过后的冒泡排序的性能任然是很糟糕的。</p><p>插入排序法试图以一种不同的方式来对列表进行排列，其策略如下：</p><ul><li><p>在第 i 轮通过列表的时候（$ 1 \leq i \leq n-1 $）,第 i 个项应该插入到列表的前 i 个项之中的正确位置。</p></li><li><p>在第 i 轮之后，前 i 个项应该是排好序的。</p></li><li><p>这个过程类似于排列手中扑克牌的顺序。即，如果你按照顺序放好了前 i-1 张牌，抓取了第 i 张牌，并且将其与手中的牌进行比较，直到找到合适的位置。</p></li><li><p>和其他排序算法一样，插入排序包含两个循环。外围的循环遍历从 1 到 n-1 的位置。对于这个循环中的每一个位置 i， 我们都保存该项并且从位置 i-1 开始内部循环。对于这个循环中的每一个位置 j，我们都将项移动到位置 j+1，直到找到了给保存的项（第 i 项）的插入位置。</p></li></ul><p>如下是 insertionSort 函数的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertionSort</span>(<span class="params">lyst</span>):</span></span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(lyst):</span><br><span class="line">        itemToInsert = lyst[i]</span><br><span class="line">        j = i - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> j &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> itemToInsert &lt; lyst[j]:</span><br><span class="line">                lyst[j + <span class="number">1</span>] = lyst[j]</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        lyst[j + <span class="number">1</span>] = itemToInsert</span><br><span class="line">        i += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>​        下表展示了对 5 个项的一个列表进行插入排序，以及在每一次通过外围循环后的状态。在下一个轮次中插入的项用一个箭头标记出来，在将这个项插入之后，用<code>*</code>将其标记。</p><table>    <tr>        <th>未排序的列表</th><th>第 1 轮后</th><th>第 2 轮后</th><th>第 3 轮后</th><th>第 4 轮后</th>    </tr>    <tr>        <td><p>2</p></td><td><p>2</p></td><td bgcolor="DarkGray"><p>1*</p></td><td bgcolor="DarkGray"><p>1</p></td><td bgcolor="DarkGray"><p>1</p></td>    </tr>    <tr>        <td><p>5&#8592;</p></td><td><p>5(没有插入)</p></td><td><p>2</p></td><td bgcolor="DarkGray"><p>2</p></td><td bgcolor="DarkGray"><p>2</p></td>    </tr>    <tr>        <td><p>1</p></td><td><p>1&#8592;</p></td><td><p>5</p></td><td><p>4*</p></td><td bgcolor="DarkGray"><p>3*</p></td>    </tr>    <tr>        <td><p>4</p></td><td><p>4</p></td><td><p>4&#8592;</p></td><td><p>5</p></td><td bgcolor="DarkGray"><p>4</p></td>    </tr>    <tr>        <td><p>3</p></td><td><p>3</p></td><td><p>3</p></td><td><p>3&#8592;</p></td><td bgcolor="DarkGray"><p>5</p></td>    </tr></table><p>​        插入排序的最坏情况的复杂的为 $ O(n^2) $。列表中排好序的项越多，插入排序的效果越好，在最好的情况下，列表本来就是有序的，那么，插入排序的复杂度是线性阶的。然而，在平均情况下，插入排序的复杂度仍然是二次方阶的。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​        在讲解基础的排序算法之前，先来介绍排序算法经常会用到的 $ swap $ 函数——用来交换列表中的两项的位置&lt;/p&gt;</summary>
    
    
    
    <category term="Algorithm" scheme="http://www.laugh12321.cn/blog/categories/Algorithm/"/>
    
    <category term="Sorting algorithm" scheme="http://www.laugh12321.cn/blog/categories/Algorithm/Sorting-algorithm/"/>
    
    
    <category term="Algorithm" scheme="http://www.laugh12321.cn/blog/tags/Algorithm/"/>
    
    <category term="Sorting algorithm" scheme="http://www.laugh12321.cn/blog/tags/Sorting-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>深度学习环境搭建指南</title>
    <link href="http://www.laugh12321.cn/blog/2018/11/24/deep_learning_environment_building_guide/"/>
    <id>http://www.laugh12321.cn/blog/2018/11/24/deep_learning_environment_building_guide/</id>
    <published>2018-11-24T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="检查-GPU-是否支持"><a href="#检查-GPU-是否支持" class="headerlink" title="检查 GPU 是否支持"></a>检查 GPU 是否支持</h2><p>如果你想搭建深度学习环境，那么首先得有一块 NVIDIA GPU。很遗憾，目前 AMD 系列 GPU 对深度学习并不友好。</p><p>有了 NVIDIA GPU 之后，首先需要检查其型号是否符合深度学习的最低配置，目前热门的深度学习框架对老旧型号的 NVIDIA GPU 并不支持。其中，判断的原则是 NVIDIA GPU 的计算性能指数（Compute Capability）<strong>大于或等于 3.0</strong>。 你可以访问 <a href="https://developer.nvidia.com/cuda-gpus">官方性能指数</a> 页面查看。</p><a id="more"></a><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_01.png"></p><h2 id="更新-NVIDIA-GPU-驱动"><a href="#更新-NVIDIA-GPU-驱动" class="headerlink" title="更新 NVIDIA GPU 驱动"></a>更新 NVIDIA GPU 驱动</h2><p>接下来，你需要更新 NVIDIA GPU 驱动到最新版本。这一步骤非常简单，只需要到 <a href="https://www.nvidia.com/Download/index.aspx">官方驱动页面</a> 找到对应型号下载安装即可。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_02.png"></p><h2 id="安装-CUDA-架构"><a href="#安装-CUDA-架构" class="headerlink" title="安装 CUDA 架构"></a>安装 CUDA 架构</h2><p>CUDA（Compute Unified Device Architecture，统一计算架构）是由 NVIDIA 所推出的一种集成技术，深度学习需要 CUDA 并行计算架构的支持。</p><p>目前，CUDA 的版本是 <code>9.x</code>，这里推荐 <code>9.0</code> 版本即可，你可以访问 <a href="https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1604&target_type=deblocal">官方页面</a> 下载。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_03.png"></p><p>目前，CUDA 支持 Windows，Linux 和 macOS 操作系统。</p><h2 id="安装-cuDNN-深度神经网络库"><a href="#安装-cuDNN-深度神经网络库" class="headerlink" title="安装 cuDNN 深度神经网络库"></a>安装 cuDNN 深度神经网络库</h2><p>cuDNN 是 NVIDIA 为深度学习专门研发的神经网络加速库，并支持当前主流的 TensorFlow、PyTorch 等深度行学习框架。根据官方介绍，cuDNN 能更好地调度 GPU 资源，使得神经网络的训练过程更加高效。</p><p>安装 cuDNN 首先需要访问 <a href="https://developer.nvidia.com/cudnn">NVIDIA Developer 网站</a> 注册账号。该网站国内访问速度慢，可能需要耐心等待或采取其他手段。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_04.png"></p><p>登陆之后，需要根据之前安装的 CUDA 版本选择对应版本的 cuDNN 安装。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_05.png"></p><h2 id="安装结果检查"><a href="#安装结果检查" class="headerlink" title="安装结果检查"></a>安装结果检查</h2><p>更新驱动 + 安装 CUDA + 安装 cuDNN 等三个步骤完成之后，我们需要检查深度学习框架是否能正常监测到 GPU 并调用。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_06.png"></p><p>这里推荐使用 TensorFlow，首先需要安装 GPU 版本的 TensorFlow。你可以阅读 <a href="https://www.tensorflow.org/install/">官方安装指南</a>。</p><p>安装完成之后，运行 TensorFlow 官方给出的 GPU 检测示例代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creates a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"><span class="comment"># Creates a session with log_device_placement set to True.</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># Runs the op.</span></span><br><span class="line">print(sess.run(c))</span><br></pre></td></tr></table></figure><p>如果能在输出中看到 GPU 的字样，即代表安装成功。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K40c, pci bus</span><br><span class="line"><span class="built_in">id</span>: <span class="number">0000</span>:<span class="number">05</span>:<span class="number">00.0</span></span><br><span class="line">b: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">a: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">MatMul: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">[[ <span class="number">22.</span>  <span class="number">28.</span>]</span><br><span class="line"> [ <span class="number">49.</span>  <span class="number">64.</span>]]</span><br></pre></td></tr></table></figure><h2 id="使用云主机搭建"><a href="#使用云主机搭建" class="headerlink" title="使用云主机搭建"></a>使用云主机搭建</h2><p>本地安装深度学习环境看起来很简单，但实际上因为系统环境等原因很容易碰到各种「坑」。很多时候，也非常推荐在云主机上完成深度学习训练。</p><p>当你选择在云主机上进行深度学习时，往往不需要自行配置环境，而是直接启动已配置好的镜像。无论是国内的 <a href="https://market.aliyun.com/">阿里云</a>，还是国外的 <a href="https://aws.amazon.com/marketplace">AWS</a>，都会提供最新的深度学习镜像。</p><p>例如，阿里云提供的 <a href="https://market.aliyun.com/products/57742013/cmjj022697.html?spm=5176.730006-53366009-57742013-cmjj021670/A.recommend.9.xDBKmW">Ubuntu16.04（预装 NVIDIA GPU 驱动和 CUDA9.0）</a> 镜像，已经为你配置好了驱动、CUDA、cuDNN。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_07.png"></p><p>你只需要在启动实例时加载即可，这样就省去了自行配置环境的麻烦。</p><p>除此之外，AWS 提供的 <a href="https://aws.amazon.com/sagemaker">Amazon SageMaker 服务</a> 可以让你一键启动 Jupyter Notebook，并在后端挂载相应的 GPU 实例，可以说再方便不过了。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_08.png"></p><h2 id="在线-Notebook-环境推荐"><a href="#在线-Notebook-环境推荐" class="headerlink" title="在线 Notebook 环境推荐"></a>在线 Notebook 环境推荐</h2><p>除了自行搭建环境，在此推荐 2 个免费的在线 Jupyter Notebook 环境。</p><h3 id="Microsoft-Azure-Notebooks"><a href="#Microsoft-Azure-Notebooks" class="headerlink" title="Microsoft Azure Notebooks"></a>Microsoft Azure Notebooks</h3><p><a href="https://notebooks.azure.com/">Microsoft Azure Notebooks</a> 是微软推出的免费 Jupyter Notebook 环境，非常方便。不过，Microsoft Azure Notebooks 仅提供 CPU 环境，无法完成深度学习模型训练。但是，平常的数据分析任务处理起来游刃有余了。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_09.png" alt="image"></p><h3 id="Google-Colab"><a href="#Google-Colab" class="headerlink" title="Google Colab"></a>Google Colab</h3><p><a href="https://colab.research.google.com/">Google Colab</a> 是 Google 推出的线上 Notebook 环境。相对于 Microsoft Azure Notebooks 而言，Google Colab 的最大问题存在一些访问上的阻碍。不过，Google Colab 的最大优势在于提供了 Nvidia M40 系列 GPU，并免费开放使用。</p><p>使用时，只需要在 <strong>代码执行程序 → 更改运行类型</strong> 中选择 GPU，即可开启免费 GPU 环境。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/dlebg_10.png" alt="image"></p><p>Google Colab 提供的免费 GPU 并不是无限制使用，如果你请求频次过高或者连续运行时间太长，都有可能被强制中断。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;检查-GPU-是否支持&quot;&gt;&lt;a href=&quot;#检查-GPU-是否支持&quot; class=&quot;headerlink&quot; title=&quot;检查 GPU 是否支持&quot;&gt;&lt;/a&gt;检查 GPU 是否支持&lt;/h2&gt;&lt;p&gt;如果你想搭建深度学习环境，那么首先得有一块 NVIDIA GPU。很遗憾，目前 AMD 系列 GPU 对深度学习并不友好。&lt;/p&gt;
&lt;p&gt;有了 NVIDIA GPU 之后，首先需要检查其型号是否符合深度学习的最低配置，目前热门的深度学习框架对老旧型号的 NVIDIA GPU 并不支持。其中，判断的原则是 NVIDIA GPU 的计算性能指数（Compute Capability）&lt;strong&gt;大于或等于 3.0&lt;/strong&gt;。 你可以访问 &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus&quot;&gt;官方性能指数&lt;/a&gt; 页面查看。&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="http://www.laugh12321.cn/blog/categories/Deep-Learning/"/>
    
    
    <category term="Deep Learning" scheme="http://www.laugh12321.cn/blog/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习术语表</title>
    <link href="http://www.laugh12321.cn/blog/2018/11/13/machine_learning_glossary/"/>
    <id>http://www.laugh12321.cn/blog/2018/11/13/machine_learning_glossary/</id>
    <published>2018-11-13T00:00:00.000Z</published>
    <updated>2020-10-21T05:59:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>本术语表中列出了一般的机器学习术语和 TensorFlow 专用术语的定义。</p><div style="color: #999;font-size: 12px;font-style: italic;">提示：你可以通过中文名称拼音首字母快速检索。</div><a id="more"></a><h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><h3 id="超参数｜Hyperparameter"><a href="#超参数｜Hyperparameter" class="headerlink" title="超参数｜Hyperparameter"></a>超参数｜Hyperparameter</h3><p>在机器学习中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给模型选择一组最优超参数，以提高学习的性能和效果。</p><h3 id="超平面｜Hyperplane"><a href="#超平面｜Hyperplane" class="headerlink" title="超平面｜Hyperplane"></a>超平面｜Hyperplane</h3><p>将一个空间划分为两个子空间的边界。例如，在二维空间中，直线就是一个超平面，在三维空间中，平面则是一个超平面。在机器学习中更典型的是：超平面是分隔高维度空间的边界。核支持向量机利用超平面将正类别和负类别区分开来（通常是在极高维度空间中）。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/mlg_01.png"></p><hr><h3 id="参数｜Parameter"><a href="#参数｜Parameter" class="headerlink" title="参数｜Parameter"></a>参数｜Parameter</h3><p>机器学习系统自行训练的模型的变量。</p><p>例如，权重就是一种参数，它们的值是机器学习系统通过连续的训练迭代逐渐学习到的。参数的概念与超参数相对应。</p><h3 id="测试集｜Test-Set"><a href="#测试集｜Test-Set" class="headerlink" title="测试集｜Test Set"></a>测试集｜Test Set</h3><p>数据集的子集，用于在模型经过验证集验证之后测试模型。当然，有时候我们不设置验证集（主要用于模型调参），直接使用训练数据训练模型后就进行测试。</p><h2 id="D"><a href="#D" class="headerlink" title="D"></a>D</h2><hr><h3 id="独热编码｜One-Hot-Encoding"><a href="#独热编码｜One-Hot-Encoding" class="headerlink" title="独热编码｜One-Hot Encoding"></a>独热编码｜One-Hot Encoding</h3><p>一种稀疏向量，其中：</p><ul><li>一个元素设为 1。</li><li>所有其他元素均设为 0。</li></ul><p>One-Hot 编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为 One-Hot 向量，向量的大小为 15000。</p><h3 id="独立同分布"><a href="#独立同分布" class="headerlink" title="独立同分布"></a>独立同分布</h3><p>独立就是每次抽样之间是没有关系的,不会相互影响。</p><p>同分布，意味着随机变量 $X_1$ 和 $X_2$ 具有相同的分布形状和相同的分布参数，对离散随机变量具有相同的分布律，对连续随机变量具有相同的概率密度函数，有着相同的分布函数，相同的期望、方差。</p><p>例如，某个网页的访问者在短时间内的分布可能为独立同分布，即分布在该短时间内没有变化，且一位用户的访问行为通常与另一位用户的访问行为无关。</p><h3 id="迭代｜Iteration"><a href="#迭代｜Iteration" class="headerlink" title="迭代｜Iteration"></a>迭代｜Iteration</h3><p>模型的权重在训练期间的一次更新，迭代包含计算参数在单个<strong>批量</strong>数据的梯度损失。</p><h2 id="F"><a href="#F" class="headerlink" title="F"></a>F</h2><hr><h3 id="泛化｜Generalization"><a href="#泛化｜Generalization" class="headerlink" title="泛化｜Generalization"></a>泛化｜Generalization</h3><p>指的是模型依据训练时采用的数据，针对以前未见过的新数据做出正确预测的能力。</p><h3 id="反向传播算法｜Backpropagation"><a href="#反向传播算法｜Backpropagation" class="headerlink" title="反向传播算法｜Backpropagation"></a>反向传播算法｜Backpropagation</h3><p>在神经网络上执行梯度下降法的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数。</p><h2 id="G"><a href="#G" class="headerlink" title="G"></a>G</h2><hr><h3 id="过拟合｜Overfitting"><a href="#过拟合｜Overfitting" class="headerlink" title="过拟合｜Overfitting"></a>过拟合｜Overfitting</h3><p>创建的模型与训练数据过于匹配，以致于模型无法根据新数据做出正确的预测。</p><p>如图，绿线代表过拟合模型，黑线代表正则化模型。虽然绿线完美的匹配训练数据，但太过依赖，并且与黑线相比，对于新的测试数据上具有更高的错误率。</p><img style='width:300px' src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/mlg_02.png"><div style="color: #888; font-size: 10px; text-align: right;"><a href="https://zh.wikipedia.org/wiki/%E9%81%8E%E9%81%A9">©️ 图片来源</a></div><h2 id="H"><a href="#H" class="headerlink" title="H"></a>H</h2><hr><h3 id="混淆矩阵｜Confusion-Matrix"><a href="#混淆矩阵｜Confusion-Matrix" class="headerlink" title="混淆矩阵｜Confusion Matrix"></a>混淆矩阵｜Confusion Matrix</h3><p>一种 NxN 表格，用于总结分类模型的预测成效；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在二元分类问题中，N=2。例如，下面显示了一个二元分类问题的混淆矩阵示例：</p><table><thead><tr><th></th><th>肿瘤（预测的标签）</th><th>非肿瘤（预测的标签）</th></tr></thead><tbody><tr><td>肿瘤（实际标签）</td><td>18</td><td>1</td></tr><tr><td>非肿瘤（实际标签）</td><td>6</td><td>452</td></tr></tbody></table><p>上面的混淆矩阵显示，在 19 个实际有肿瘤的样本中，该模型正确地将 18 个归类为有肿瘤（18 个真正例），错误地将 1 个归类为没有肿瘤（1 个假负例）。同样，在 458 个实际没有肿瘤的样本中，模型归类正确的有 452 个（452 个真负例），归类错误的有 6 个（6 个假正例）。</p><p>多类别分类问题的混淆矩阵有助于确定出错模式。例如，某个混淆矩阵可以揭示，某个经过训练以识别手写数字的模型往往会将 4 错误地预测为 9，将 7 错误地预测为 1。混淆矩阵包含计算各种效果指标（包括精确率和召回率）所需的充足信息。</p><h2 id="J"><a href="#J" class="headerlink" title="J"></a>J</h2><hr><h3 id="集成学习｜Ensemble"><a href="#集成学习｜Ensemble" class="headerlink" title="集成学习｜Ensemble"></a>集成学习｜Ensemble</h3><p>多个<strong>模型</strong>的预测结果的并集。</p><p>通俗来讲，集成学习把大大小小的多种算法融合在一起，共同协作来解决一个问题。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等。</p><p>你可以通过以下一项或多项来创建集成学习：</p><ul><li>不同的初始化</li><li>不同的超参数</li><li>不同的整体结构</li></ul><h3 id="决策边界｜Decision-Boundary"><a href="#决策边界｜Decision-Boundary" class="headerlink" title="决策边界｜Decision Boundary"></a>决策边界｜Decision Boundary</h3><p>在二元分类或多类别分类问题中，模型学到的类别之间的分界线。例如，在以下表示某个二元分类问题的图片中，决策边界是橙色类别和蓝色类别之间的分界线：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/mlg_03.png"></p><h3 id="精确率｜Precision"><a href="#精确率｜Precision" class="headerlink" title="精确率｜Precision"></a>精确率｜Precision</h3><p>一种分类模型指标。精确率指模型正确预测正类别的频率。</p><p>$$<br>召回率  =   \frac { 真正例数 }  { 真正例数 + 假负例数 }<br>$$</p><h3 id="交叉熵｜Cross-Entropy"><a href="#交叉熵｜Cross-Entropy" class="headerlink" title="交叉熵｜Cross-Entropy"></a>交叉熵｜Cross-Entropy</h3><p><strong>对数损失函数</strong>向<strong>多类别分类问题</strong>进行的一种泛化。交叉熵可以量化两种概率分布之间的差异。</p><p>$$<br>H(p,q) = \sum_{i=1}^{n} p(x) \cdot log(\frac{1}{q(x)})<br>$$</p><h3 id="激活函数｜Activation-Function"><a href="#激活函数｜Activation-Function" class="headerlink" title="激活函数｜Activation Function"></a>激活函数｜Activation Function</h3><p>一种函数（例如 ReLU 或 S 型函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。</p><h3 id="结构风险最小化｜Structural-Risk-Minimization"><a href="#结构风险最小化｜Structural-Risk-Minimization" class="headerlink" title="结构风险最小化｜Structural Risk Minimization"></a>结构风险最小化｜Structural Risk Minimization</h3><p>一种算法，用于平衡以下两个目标：</p><ul><li>期望构建最具预测性的模型（例如损失最低）。</li><li>期望使模型尽可能简单（例如强大的正则化）。</li></ul><p>例如，旨在将基于训练集的损失和正则化降至最低的模型函数就是一种结构风险最小化算法。</p><h2 id="L"><a href="#L" class="headerlink" title="L"></a>L</h2><hr><h3 id="离群点｜Outlier"><a href="#离群点｜Outlier" class="headerlink" title="离群点｜Outlier"></a>离群点｜Outlier</h3><p>与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。</p><ul><li>绝对值很高的权重。</li><li>与实际值相差很大的预测值。</li><li>值比平均值高大约 3 个标准偏差的输入数据。</li></ul><p>离群值常常会导致模型训练出现问题。</p><h3 id="类别｜Class"><a href="#类别｜Class" class="headerlink" title="类别｜Class"></a>类别｜Class</h3><p>为标签枚举的一组目标值中的一个。例如，在检测垃圾邮件的二元分类模型中，两种类别分别是「垃圾邮件」和「非垃圾邮件」。在识别狗品种的多类别分类模型中，类别可以是「贵宾犬」、「小猎犬」、「哈巴犬」等。</p><h3 id="离散特征｜Discrete-Feature"><a href="#离散特征｜Discrete-Feature" class="headerlink" title="离散特征｜Discrete Feature"></a>离散特征｜Discrete Feature</h3><p>一种特征，包含有限个可能值。例如，某个值只能是「动物」、「蔬菜」或「矿物」的特征便是一个离散特征（或分类特征）。与连续特征相对。</p><h2 id="M"><a href="#M" class="headerlink" title="M"></a>M</h2><hr><h3 id="密集层｜Dense-Layer"><a href="#密集层｜Dense-Layer" class="headerlink" title="密集层｜Dense Layer"></a>密集层｜Dense Layer</h3><p>是全连接层的同义词。</p><h2 id="P"><a href="#P" class="headerlink" title="P"></a>P</h2><hr><h3 id="批次｜Batch"><a href="#批次｜Batch" class="headerlink" title="批次｜Batch"></a>批次｜Batch</h3><p>模型训练的一次迭代（即一次梯度更新）中使用样本簇。</p><h3 id="偏差｜Bias"><a href="#偏差｜Bias" class="headerlink" title="偏差｜Bias"></a>偏差｜Bias</h3><p>距离原点的截距或偏移。偏差（也称为偏差项）在机器学习模型中以 $b$ 或 $w_0$ 表示。例如，在下面的公式中，偏差为 $b$：</p><p>$$y’ = b + w_1x_1 + w_2x_2 + … w_nx_n$$</p><p>请勿与「预测偏差」混淆。</p><h3 id="批次规模｜Batch-Size"><a href="#批次规模｜Batch-Size" class="headerlink" title="批次规模｜Batch Size"></a>批次规模｜Batch Size</h3><p>模型迭代一次，使用的样本集的大小。</p><p>例如训练集有 <code>6400</code> 个样本，<code>batch_size=128</code>，那么训练完整个样本集需要 <code>50</code> 次迭代。Batch Size 的大小一般设置为 <code>16</code> 及 <code>16</code> 的倍数。</p><h2 id="R"><a href="#R" class="headerlink" title="R"></a>R</h2><hr><h3 id="ROC-曲线下面积"><a href="#ROC-曲线下面积" class="headerlink" title="ROC 曲线下面积"></a>ROC 曲线下面积</h3><p>一种会考虑所有可能分类阀值的评价指标。</p><p>ROC 曲线下面积的数值意义为：对于随机选择的<strong>正类别样本</strong>确实为正类别，以及随机选择的<strong>负类样本</strong>为正类别，<strong>分类器</strong>更确信前者的概率。</p><h2 id="S"><a href="#S" class="headerlink" title="S"></a>S</h2><hr><h3 id="输入层｜Input-Layer"><a href="#输入层｜Input-Layer" class="headerlink" title="输入层｜Input Layer"></a>输入层｜Input Layer</h3><p>神经网络中的第一层（接受输入数据的层）</p><h3 id="输出层｜Output-Layer"><a href="#输出层｜Output-Layer" class="headerlink" title="输出层｜Output Layer"></a>输出层｜Output Layer</h3><p>神经网络最后一层。</p><h3 id="损失｜Loss"><a href="#损失｜Loss" class="headerlink" title="损失｜Loss"></a>损失｜Loss</h3><p>一种衡量指标，用于衡量模型的预测偏离其标签的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将均方误差用于损失函数，而逻辑回归模型则使用对数损失函数。</p><h3 id="收敛｜Convergence"><a href="#收敛｜Convergence" class="headerlink" title="收敛｜Convergence"></a>收敛｜Convergence</h3><p>通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练损失和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。</p><h3 id="缩放｜Scaling"><a href="#缩放｜Scaling" class="headerlink" title="缩放｜Scaling"></a>缩放｜Scaling</h3><p>特征工程中的一种常用做法，是对某个特征的值区间进行调整，使之与数据集中其他特征的值区间一致。例如，假设您希望数据集中所有浮点特征的值都位于 0 到 1 区间内，如果某个特征的值位于 0 到 500 区间内，您就可以通过将每个值除以 500 来缩放该特征。</p><h3 id="随机梯度下降法｜SGD"><a href="#随机梯度下降法｜SGD" class="headerlink" title="随机梯度下降法｜SGD"></a>随机梯度下降法｜SGD</h3><p>SGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。</p><h3 id="Softmax-函数"><a href="#Softmax-函数" class="headerlink" title="Softmax 函数"></a>Softmax 函数</h3><p>一种函数，可提供多类别分类模型中每个可能类别的概率。这些概率的总和正好为 <code>1.0</code>。例如，softmax 可能会得出某个图像是狗、猫和马的概率分别是 <code>0.9</code>、<code>0.08</code> 和 <code>0.02</code>。（也称为完整 softmax。）</p><h2 id="T"><a href="#T" class="headerlink" title="T"></a>T</h2><hr><h3 id="推断｜Inference"><a href="#推断｜Inference" class="headerlink" title="推断｜Inference"></a>推断｜Inference</h3><p>在机器学习中，推断通常指以下过程：通过将训练过的模型应用于无标签样本来做出预测。在统计学中，推断是指在某些观测数据条件下拟合分布参数的过程。</p><h3 id="梯度｜Gradient"><a href="#梯度｜Gradient" class="headerlink" title="梯度｜Gradient"></a>梯度｜Gradient</h3><p>偏导数相对于所有自变量的向量。在机器学习中，梯度是模型函数偏导数的向量。</p><h3 id="梯度下降法｜Gradient-Descent"><a href="#梯度下降法｜Gradient-Descent" class="headerlink" title="梯度下降法｜Gradient Descent"></a>梯度下降法｜Gradient Descent</h3><p>一种通过计算并且减小梯度将损失降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到权重和偏差的最佳组合，从而将损失降至最低。</p><h3 id="特征｜Feature"><a href="#特征｜Feature" class="headerlink" title="特征｜Feature"></a>特征｜Feature</h3><p>在进行预测时使用的输入变量。</p><h3 id="特征组合｜Feature-Cross"><a href="#特征组合｜Feature-Cross" class="headerlink" title="特征组合｜Feature Cross"></a>特征组合｜Feature Cross</h3><p>通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的合成特征。特征组合有助于表示非线性关系。</p><h3 id="特征工程｜Feature-Engineering"><a href="#特征工程｜Feature-Engineering" class="headerlink" title="特征工程｜Feature Engineering"></a>特征工程｜Feature Engineering</h3><p>指以下过程：确定哪些特征可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。</p><h3 id="凸函数｜Convex-Function"><a href="#凸函数｜Convex-Function" class="headerlink" title="凸函数｜Convex Function"></a>凸函数｜Convex Function</h3><p>一种函数，函数图像以上的区域为凸集。典型凸函数的形状类似于字母 U。例如，以下都是凸函数：</p><p><img width='600px' src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/mlg_04.png"></img></p><p>严格凸函数只有一个局部最低点，该点也是全局最低点。经典的 U 形函数都是严格凸函数。不过，有些凸函数（例如直线）则不是这样。</p><p>很多常见的损失函数（包括下列函数）都是凸函数：</p><ul><li>L2 损失函数</li><li>对数损失函数</li><li>L1 正则化</li><li>L2 正则化</li></ul><p>梯度下降法的很多变体都一定能找到一个接近严格凸函数最小值的点。同样，随机梯度下降法的很多变体都有很高的可能性能够找到接近严格凸函数最小值的点（但并非一定能找到）。</p><p>两个凸函数的和（例如 L2 损失函数 + L1 正则化）也是凸函数。</p><p>深度模型绝不会是凸函数。值得注意的是，专门针对凸优化设计的算法往往总能在深度网络上找到非常好的解决方案，虽然这些解决方案并不一定对应于全局最小值。</p><h3 id="凸优化｜Convex-Optimization"><a href="#凸优化｜Convex-Optimization" class="headerlink" title="凸优化｜Convex Optimization"></a>凸优化｜Convex Optimization</h3><p>使用数学方法（例如梯度下降法）寻找凸函数最小值的过程。机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。</p><h2 id="X"><a href="#X" class="headerlink" title="X"></a>X</h2><hr><h3 id="学习率｜Learning-Rate"><a href="#学习率｜Learning-Rate" class="headerlink" title="学习率｜Learning Rate"></a>学习率｜Learning Rate</h3><p>在训练模型时用于梯度下降的一个变量。在每次迭代期间，梯度下降法都会将学习速率与梯度相乘。得出的乘积称为梯度步长。</p><p>学习速率是一个重要的超参数。</p><h3 id="稀疏特征｜Sparse-Feature"><a href="#稀疏特征｜Sparse-Feature" class="headerlink" title="稀疏特征｜Sparse Feature"></a>稀疏特征｜Sparse Feature</h3><p>一种特征向量，其中的大多数值都为 0 或为空。例如，某个向量包含一个为 1 的值和一百万个为 0 的值，则该向量就属于稀疏向量。再举一个例子，搜索查询中的单词也可能属于稀疏特征 - 在某种指定语言中有很多可能的单词，但在某个指定的查询中仅包含其中几个。</p><p>与密集特征相对。</p><h3 id="协同过滤｜Collabroative-Filtering"><a href="#协同过滤｜Collabroative-Filtering" class="headerlink" title="协同过滤｜Collabroative Filtering"></a>协同过滤｜Collabroative Filtering</h3><p>根据很多其他用户的兴趣来预测某位用户的兴趣。协同过滤通常用在推荐系统中。</p><h2 id="Y"><a href="#Y" class="headerlink" title="Y"></a>Y</h2><hr><h3 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h3><p>已经过训练的模型或模型组件（例如嵌套）。有时，您需要将预训练的嵌套馈送到神经网络。在其他时候，您的模型将自行训练嵌套，而不依赖于预训练的嵌套。</p><h2 id="Z"><a href="#Z" class="headerlink" title="Z"></a>Z</h2><hr><h3 id="准确率｜Accuracy"><a href="#准确率｜Accuracy" class="headerlink" title="准确率｜Accuracy"></a>准确率｜Accuracy</h3><p>分类模型的正确预测所占的比例。在多类别分类中，准确率的定义如下：</p><p>$$ 准确率  =   \frac { 正确预测数 }  { 样本总数 } $$</p><p>在二元分类中，准确率的定义如下：</p><p>$$ 准确率  =   \frac { 真正例数 + 真负例数 }  { 样本总数 } $$</p><h3 id="真负例"><a href="#真负例" class="headerlink" title="真负例"></a>真负例</h3><p>被模型正确地预测为负类别的样本。例如，模型推断出某封电子邮件不是垃圾邮件，而该电子邮件确实不是垃圾邮件。</p><h3 id="真正例"><a href="#真正例" class="headerlink" title="真正例"></a>真正例</h3><p>被模型正确地预测为正类别的样本。例如，模型推断出某封电子邮件是垃圾邮件，而该电子邮件确实是垃圾邮件。</p><h3 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h3><p>一种分类模型指标，用于回答以下问题：在所有可能的正类别标签中，模型正确地识别出了多少个？即：</p><p>$$ 召回率  =   \frac { 真正例数 }  { 真正例数  + 假正例数} $$</p><h3 id="张量｜Tensor"><a href="#张量｜Tensor" class="headerlink" title="张量｜Tensor"></a>张量｜Tensor</h3><p>TensorFlow 程序中的主要数据结构。张量是 N 维（其中 N 可能非常大）数据结构，最常见的是标量、向量或矩阵。张量的元素可以包含整数值、浮点值或字符串值。</p><p><img width='600px' src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/mlg_05.png"></img></p><h3 id="迁移学习｜Transfer-Learning"><a href="#迁移学习｜Transfer-Learning" class="headerlink" title="迁移学习｜Transfer Learning"></a>迁移学习｜Transfer Learning</h3><p>将信息从一个机器学习任务转移到另一个机器学习任务。例如，在多任务学习中，一个模型可以完成多项任务，例如针对不同任务具有不同输出节点的深度模型。迁移学习可能涉及将知识从较简单任务的解决方案转移到较复杂的任务，或者将知识从数据较多的任务转移到数据较少的任务。</p><p>大多数机器学习系统都只能完成一项任务。迁移学习是迈向人工智能的一小步；在人工智能中，单个程序可以完成多项任务。</p><h3 id="L1-正则化｜L1-Regularization"><a href="#L1-正则化｜L1-Regularization" class="headerlink" title="L1 正则化｜L1 Regularization"></a>L1 正则化｜L1 Regularization</h3><p>一种正则化，根据权重的绝对值的总和来惩罚权重。在依赖稀疏特征的模型中，L1 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 L2 正则化相对。</p><h3 id="L2-正则化｜L2-Regularization"><a href="#L2-正则化｜L2-Regularization" class="headerlink" title="L2 正则化｜L2 Regularization"></a>L2 正则化｜L2 Regularization</h3><p>一种正则化，根据权重的平方和来惩罚权重。L2 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 L1 正则化相对。）在线性模型中，L2 正则化始终可以改进泛化。</p><h3 id="周期｜Epoch"><a href="#周期｜Epoch" class="headerlink" title="周期｜Epoch"></a>周期｜Epoch</h3><p>在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。因此，一个周期表示（N/批次规模）次训练迭代，其中 N 是样本总数。</p><hr><div style="color: #999;font-size: 12px;font-style: italic;">©️ 部分内容参考自 [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/)</div>]]></content>
    
    
    <summary type="html">&lt;p&gt;本术语表中列出了一般的机器学习术语和 TensorFlow 专用术语的定义。&lt;/p&gt;
&lt;div style=&quot;color: #999;font-size: 12px;font-style: italic;&quot;&gt;提示：你可以通过中文名称拼音首字母快速检索。&lt;/div&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
  </entry>
  
</feed>
