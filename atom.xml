<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Laugh&#39;s blog</title>
  
  
  <link href="http://www.laugh12321.cn/blog/atom.xml" rel="self"/>
  
  <link href="http://www.laugh12321.cn/blog/"/>
  <updated>2020-10-23T08:46:01.006Z</updated>
  <id>http://www.laugh12321.cn/blog/</id>
  
  <author>
    <name>Laugh</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>译|Bayesian Personalized Ranking with Multi-Channel User Feedback</title>
    <link href="http://www.laugh12321.cn/blog/2019/07/15/bpr_with_multi-channel_user_feedback/"/>
    <id>http://www.laugh12321.cn/blog/2019/07/15/bpr_with_multi-channel_user_feedback/</id>
    <published>2019-07-15T00:00:00.000Z</published>
    <updated>2020-10-23T08:46:01.006Z</updated>
    
    <content type="html"><![CDATA[<p> 已经显示 $learning−to−rank$ 算法允许推荐系统利用一元用户进行反馈。我们提出了 $Multi−feedbackBayesianPersonalizedRanking$ (多反馈贝叶斯个性化排序)，即利用扩展采样与不同类型反馈的成对方法。反馈类型来自不同的 channels$channels$，其中 users$users$ 与 items$items$ 相互作用 (e.g.,clicks,likes,listens,follows,andpurchase)$e.g.,clicks,likes,listens,follows,andpurchase)$。 我们通过像 clicks$clicks$, likes$likes$ 这样不同类型的反馈，来反映不同级别的 commitment$commitment$ 或 preference$preference$。 我们的方法不同之处在于它在训练过程中同时利用多个反馈来源，MF−BPR$MF−BPR$ 的新颖之处在于它是一种扩展的采样方法, 将反馈源等同于信号预期贡献的 level$level$。 我们通过对包含多种反馈类型的三个数据集进行了一系列实验来证明我们的方法的有效性，实验结果表明：采用更好采样方法的 MF−BPR$MF−BPR$ 在准确性方面优于 BPR$BPR$，MF−BPR$MF−BPR$ 的优势在于它能够在抽样负面项目时利用水平信息。  </p><a id="more"></a><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a><a href="#INTRODUCTION" title="INTRODUCTION"></a>INTRODUCTION</h2><p> 在许多领域，用户通过许多不同的 channels$channels$ 提供一元反馈。例如，在线市场收集视图，添加到购物车和购买信号，音乐平台记录收听，收藏和添加播放列表事件。贝叶斯个性化排名，一种通过抽样对学习用户偏好的成对学习-排名方法，允许推荐系统有效地从一元反馈中学习。然而，目前 BPR$BPR$ 的实例化不能充分利用某些领域中可用的所有不同类型的反馈。</p><p> 在本文中，我们提出了一种称为 MF−BPR$MF-BPR$ 的方法。 MF−BRP$MF−BRP$ 的创新是旨在同时训练期间利用来自多个渠道的一元反馈一种采样方法，这种采样方法已经被证明可有效改善 BPR$BPR$。</p><p> 我们方法的关键是将不同的反馈渠道映射到不同的 levels$levels$，以反映每种类型的反馈在培训阶段可能产生的 contribution$contribution$。在 BPR$BPR$ 样本对中第一项优先于第二项，而 MF−BPR$MF-BPR$ 的 levels$levels$ 有助于自动引导采样，将重点放在信息量最大的样本对中。该方法的优点在于它利用可用信息的一致性和一些用户反馈信号比其他信号更可靠或更有意义的直观判断。</p><p> 本文的结构如下：在下一节中，我们将介绍相关的相关工作。然后，我们详细介绍 MF−BPR$MF-BPR$ 和我们测试的三个数据集。之后对实验结果进行说明，这些结果证明了 MF−BPR$MF-BPR$ 改善 BPR$BPR$ 的能力。最后，我们结束对结果的讨论，该结果揭示了通过对项目对的 negative$negative$ 项目的合理取样所带来的改进。</p><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a><a href="#RELATED-WORK" title="RELATED WORK"></a>RELATED WORK</h2><p> 自 BPR$BPR$ 引入以来，已经提出了许多改进，MF−BPR$MF-BPR$ 属于对抽样进行改进的方法。这方面的最初贡献是由 [3] 做出的，他们通过流行度对项目抽样进行加权。最近，[7] 提出了自适应采样，它基于当前模型对信息对进行过采样。MF−BPR$MF-BPR$ 类似于这种方法，侧重于信息对，但其采样是由反馈信道 feedbackchannels$feedbackchannels$ 所驱动的。在本节的剩余部分，我们将介绍已经提出的利用来自不同源的反馈的各种技术，为每个关键点提供示例并并标注与 MF−BPR$MF-BPR$ 的差异。</p><p> 矩阵分解方法试图同时分解多个矩阵，[11] 的作者考虑了来自社交网络（评论，重新分享或创建帖子）的不同信号，这些信号具有单独的分解因式，用于进行随后组合的预测。[4] 的工作利用了多种类型的关系，并针对冷启动问题。而 MF−BPR$MF-BPR$ 的 “多“ 指的是反映相同用户-项目关系的多种类型的反馈。</p><p> 其他方法使用了 ratings$ratings$ 表示不同层次的用户反馈的方法。在 [9] 中提出了列表优化标准，其允许 learning−to−rank$learning−to−rank$ 算法利用分级反馈。与我们的工作最密切相关的是 [5]，它使用 ratings$ratings$ 和 interactioncounts$interactioncounts$ 衍生分级隐式反馈的 BPR$BPR$ 算法，同时还探讨了时间信息的 contribution$contribution$。本文展望了混合方法的发展前景，MF−BPR$MF-BPR$ 可以被视为混合型，因为它同时使用不同的反馈源，它与 [5] 的不同之处在于它避免了手动选择的权重，更重要的是，它同时使用多个反馈通道。</p><h2 id="MULTI-FEEDBACK-BPR"><a href="#MULTI-FEEDBACK-BPR" class="headerlink" title="MULTI-FEEDBACK BPR"></a><a href="#MULTI-FEEDBACK-BPR" title="MULTI-FEEDBACK BPR"></a>MULTI-FEEDBACK BPR</h2><p> BPR−MF$BPR-MF$ 基于以下见解：通过各种渠道收集的用户反馈反映了用户偏好的不同优势，BPR$BPR$的抽样方法可以利用这些差异。简而言之，我们允许 BPR$BPR$ 从以下事实中学习：例如，点击代表不同程度的 commitment$commitment$ 或 preference$preference$ 类似于 like$like$。</p><p> 对于由给定训练集 S$S$ 组成的 user−item$user−item$ 对 (u,i)$(u,i)$，标准 BPR$BPR$ 通过从 S$S$ 中采集观察到的反馈对 (u，i)$(u，i)$ 来创建元组 (u，i，j)$(u，i，j)$，其中 negativeitem$negativeitem$j$j$ 表示 u$u$ 中未观察到的项目。对于每个采样元组，BPR$BPR$ 算法以随机梯度下降的方式更新参数，使得 i$i$ 的排名高于 j$j$，MF−BPR$MF-BPR$ 则将不同类型的反馈映射到 levels$levels$ 上，这允许我们约束抽样以反映我们与反馈类型相关联的偏好程度 (preferencestrengths)$(preferencestrengths)$。图1将标准 BPR$BPR$ 采样方法(A)$(A)$与 MF−BPR$MF-BPR$ 使用的扩展采样方法(B)$(B)$进行了比较，后者对正反馈的类型施加了一个顺序，这使得在学习阶段可用的反馈之间存在更细微的差异。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh%27s%20blog/images/translate/Figure-1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh%27s%20blog/images/translate/Figure-1.png"></a></p><p> 现在我们将正式介绍 MF−BPR$MF-BPR$ 如何利用反馈 levels$levels$。令 L=(L1,…,Lp)$L=(L1,…,Lp)$ 表示数据集中给定的有序 levels$levels$ 集合，使得与 Li+1$Li+1$ 中的反馈相比，Li$Li$中反馈的 interest$interest$ 信号更强，即 Li≻Li+1$Li≻Li+1$。 一般来说，未观察到的反馈也被认为属于 Luo$Luo$，因此对于每个正反馈级别 Li$Li$ 和负反馈级别 Lj$Lj$ ，有 Li≻Luo$Li≻Luo$ 和 Luo≻Lj$Luo≻Lj$。级别 L$L$ 的训练反馈集由 SL$SL$表示。我们将 L+$L+$ 和 L−$L−$ 定义为正反馈水平和负反馈水平。在标准 BPR$BPR$中，因为 |L|=2$|L|=2$，∣∣L+∣∣=1$|L+|=1$ 和 ∣∣L−∣∣=0$|L−|=0$，对于未观察到的反馈，只有一个级别的正反馈，而另一个级别没有明确的负反馈。我们还将 IL,u$IL,u$ 定义为用户 u$u$ 与之交互的级别 L$L$ 中的项目，将 Iu$Iu$ 定义为 u$u$ 与之交互的所有项目（在所有级别中）。MF−BPR$MF-BPR$ 的 DMF$DMF$ 中所有首选项组合的集合可以定义为：</p><p>DMF={(u,i,j)|i∈IL,u∧j∈IN,u∧L∈L+∧L≻N}(1)</p><p>$$(1)DMF={(u,i,j)|i∈IL,u∧j∈IN,u∧L∈L+∧L≻N}$$</p><p> 在标准 BPR$BPR$ 中，元组(u,i,j)$(u,i,j)$被均匀地采样，但是在 MF−BPR$MF-BPR$ 中，我们引入了非均匀采样器，其考虑了反馈信道的层次（重要性）。要使用多个级别，MF−BPR$MF-BPR$ 需要从 L$L$ 采样 L$L$ 和 N$N$，然后可以相对于给定级别对元组(u,i,j)$(u,i,j)$进行采样。通过对 S$S$ 使用采样分布 p(u,i,L)$p(u,i,L)$ 抽样观察反馈来对 positiveitem$positiveitem$ 采样，同时也对 positivelevelL$positivelevelL$ 进行采样。概率分布 p(u,i,L)$p(u,i,L)$ 可以进一步扩展为 p(u,i,L)=p(u,i|L)p(L)$p(u,i,L)=p(u,i|L)p(L)$，使得 p(u,i|L)$p(u,i|L)$ 是均匀分布在 SL$SL$ 和 p(L)$p(L)$ 上的 L$L$ 级采样分布。p(L)$p(L)$ 的一个简单选择是在所有级别上的均匀分布。然而，如果分布均匀，基数较小的 levels$levels$ 将被过采样，这可能导致采样不良。我们提出了非均匀分布的 p(L)$p(L)$，其中反馈的基数以及 levels$levels$ 的重要性与权重因子一起考虑。概率分布 p(L)$p(L)$ 定义为：</p><p>p(L)=wL|SL|∑Q∈L+wQ∣∣SQ∣∣(2)</p><p>$$(2)p(L)=wL|SL|∑Q∈L+wQ|SQ|$$</p><p>其中，wL$wL$ 是等级 L$L$ 的权重。在权重相等的情况下，从所有级别均匀地对正项进行采样，这相当于标准 BPR$BPR$ 的正采样器。可以定义非均匀分布的权重值以反映 levels$levels$ 的重要性。在实验中，我们发现正 levels$levels$ 的逆是权重的良好候选者(即w1=1,w2=12,…)$(即w1=1,w2=12,…)$。如果 levels$levels$ 的顺序不是已知的先验（例如，是否应该认为 like$like$ 比 share$share$ 更重要，反之亦然），则可以通过使用超参数搜索算法来寻找近似最佳权重。</p><p> 要对负项目 j$j$ 进行采样，应该给出正项目的级别以便采样器从其下面的一个级别中采样项目。给定正 levelL$levelL$ 和正 user−item$user−item$ 对 (u,i)$(u,i)$，我们将采样分布 p(j,N|u,L)$p(j,N|u,L)$ 表示为负样本 j$j$ 及其对应的 levelN$levelN$。与正采样器类似，负采样器可以扩展为 p(j,N|u,L)=p(j|u,L,N)p(N|u,L)$p(j,N|u,L)=p(j|u,L,N)p(N|u,L)$ 其中 p(j|u,L,N)$p(j|u,L,N)$是负项目采样器，p(N|u,L)$p(N|u,L)$ 是条件负级采样器。我们还提出了一个非均匀的负项目采样器类似于（2），它考虑了基数和基数的权重。 negativelevel$negativelevel$ 采样器 p(N|u,L)$p(N|u,L)$ 定义为：</p><p>p(N|u,L)=⎧⎨⎩(1−β)wN|SN|∑Q≻LwQ∣∣SQ∣∣N≠LuoβN=Luo(3)</p><p>$$(3)p(N|u,L)={(1−β)wN|SN|∑Q≻LwQ|SQ|N≠LuoβN=Luo$$</p><p>0≤β≤1$0≤β≤1$ 是控制采样中未观察到的反馈的比率的参数。对于标准 BPR$BPR$ β=1$β=1$ 是因为所有负项目都是从未观察到的反馈中采样的，正确 β$β$ 值可以通过实验得到。在实验中，我们发现在 β$β$ 值较高的情况下模型更准确。关于对 β$β$ 正确值的观察将在第4节中更详细地讨论。</p><p> 与标准 BPR$BPR$ 类似，如果 N≠Luo$N≠Luo$ 则负采样器 p(j|u,L,N)$p(j|u,L,N)$ 可以从 IN,u$IN,u$ 均匀地对负项目采样，如果 N=Luo$N=Luo$，则可以从 I∖Iu$I∖Iu$ 采样。我们将 puni(j|u,L,N)$puni(j|u,L,N)$ 表示为统一负项目采样器，定义为：</p><p>puni(j|u,L,N)=⎧⎪ ⎪ ⎪⎨⎪ ⎪ ⎪⎩1∣∣IN,u∣∣N≠Luo∧j∈IN,u1|I∖Iu|N=Luo∧j∈I∖Iu0 otherwise (4)</p><p>$$(4)puni(j|u,L,N)={1|IN,u|N≠Luo∧j∈IN,u1|I∖Iu|N=Luo∧j∈I∖Iu0 otherwise $$</p><p>此外，当 N$N$ 是未观察到的等级 Luo$Luo$ 时，我们还提出了提出一个带有非均匀采样器的多级项目采样器。将多级负项目采样器 pml(j|u,L,N)$pml(j|u,L,N)$ 定义为：</p><p>pml(j|u,L,N)=⎧⎪ ⎪⎨⎪ ⎪⎩1∣∣IN,u∣∣N≠Luo∧j∈IN,up(j,u′,L′|u)N=Luo∧j∈I∖Iu0 otherwise (5)</p><p>$$(5)pml(j|u,L,N)={1|IN,u|N≠Luo∧j∈IN,up(j,u′,L′|u)N=Luo∧j∈I∖Iu0 otherwise $$</p>]]></content>
    
    
    <summary type="html">&lt;p&gt; 已经显示 $learning−to−rank$ 算法允许推荐系统利用一元用户进行反馈。我们提出了 $Multi−feedbackBayesianPersonalizedRanking$ (多反馈贝叶斯个性化排序)，即利用扩展采样与不同类型反馈的成对方法。反馈类型来自不同的 channels$channels$，其中 users$users$ 与 items$items$ 相互作用 (e.g.,clicks,likes,listens,follows,andpurchase)$e.g.,clicks,likes,listens,follows,andpurchase)$。 我们通过像 clicks$clicks$, likes$likes$ 这样不同类型的反馈，来反映不同级别的 commitment$commitment$ 或 preference$preference$。 我们的方法不同之处在于它在训练过程中同时利用多个反馈来源，MF−BPR$MF−BPR$ 的新颖之处在于它是一种扩展的采样方法, 将反馈源等同于信号预期贡献的 level$level$。 我们通过对包含多种反馈类型的三个数据集进行了一系列实验来证明我们的方法的有效性，实验结果表明：采用更好采样方法的 MF−BPR$MF−BPR$ 在准确性方面优于 BPR$BPR$，MF−BPR$MF−BPR$ 的优势在于它能够在抽样负面项目时利用水平信息。  &lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="推荐系统" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="论文翻译" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
    
    <category term="Deep Learning" scheme="http://www.laugh12321.cn/blog/tags/Deep-Learning/"/>
    
    <category term="Recommender System" scheme="http://www.laugh12321.cn/blog/tags/Recommender-System/"/>
    
    <category term="RS" scheme="http://www.laugh12321.cn/blog/tags/RS/"/>
    
    <category term="BPR" scheme="http://www.laugh12321.cn/blog/tags/BPR/"/>
    
  </entry>
  
  <entry>
    <title>基于 Keras 实现图像风格转移</title>
    <link href="http://www.laugh12321.cn/blog/2019/04/19/keras_realize_neural_style_transfer/"/>
    <id>http://www.laugh12321.cn/blog/2019/04/19/keras_realize_neural_style_transfer/</id>
    <published>2019-04-19T00:00:00.000Z</published>
    <updated>2020-10-23T08:53:08.323Z</updated>
    
    <content type="html"><![CDATA[<p> Style Transfer 这个方向火起来是从2015年Gatys发表的Paper <a href="https://arxiv.org/pdf/1508.06576.pdf">A Neural Algorithm of Artistic Style</a>（神经风格迁移） ， 这里就简单提一下论文的主要思想。  </p><h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><hr><p>  Gatys这篇论文的核心思想就是 —— 图片的内容和风格是可以分离的，可以通过神经网络的方式，将图片的风格进行自由交换。</p><p> 如果内容和风格是可以分离的，那么风格的迁移即可转化成这样一个问题：让生成图片的内容与内容来源图片尽可能相似，让图片的风格与风格来源图片尽可能相似。</p><a id="more"></a><p> 那么如何才能将图片的风格提取出来呢？Gatys发现<strong>纹理能够描述一个图像的风格</strong>，也就是说只要提取出图片的纹理就可以了。那么如何又提取出图像的纹理呢，Gatys发现VGG19网络其实就相当于一堆局部特征识别器，他在VGG19的基础上套了一个<strong>格拉姆矩阵</strong>（Gram matrix）用来计算不同局部特征的相关性，把它变成里一个统计模型，这样图像纹理提取已经完成了。剩下的就比较轻松了，前文提到VGG19 相当于一堆局部特征识别器，于是Gatys直接把局部特征看作近似的图像内容，这样就得到了一个将图像内容与纹理分开的系统。而将内容与纹理合成的方法就是Google 由2015年夏天首次发布的 DeepDream 方法，找到能让合适的特征提取神经元被激活的图片即可。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural-Style-Transfer/neural-style.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural-Style-Transfer/neural-style.png"></a></p><p>至此， <a href="https://arxiv.org/pdf/1508.06576.pdf">A Neural Algorithm of Artistic Style</a> 的主要思想已经解释完了。</p><h2 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h2><hr><p>神经风格迁移的一般步骤：</p><ul><li>创建一个网络，它能够同时计算风格参考图像、目标图像和生成图像的 VGG19 层激活。</li><li>使用这三张图像上计算的层激活来定义之前所述的损失函数，为了实现风格迁移，需要将这个损失函数最小化。</li><li>设置梯度下降过程来将这个损失函数最小化。</li></ul><p>完整代码 <a href="https://github.com/laugh12321/neural-style">laugh12321/neural-style</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt; Style Transfer 这个方向火起来是从2015年Gatys发表的Paper &lt;a href=&quot;https://arxiv.org/pdf/1508.06576.pdf&quot;&gt;A Neural Algorithm of Artistic Style&lt;/a&gt;（神经风格迁移） ， 这里就简单提一下论文的主要思想。  &lt;/p&gt;
&lt;h2 id=&quot;论文概述&quot;&gt;&lt;a href=&quot;#论文概述&quot; class=&quot;headerlink&quot; title=&quot;论文概述&quot;&gt;&lt;/a&gt;论文概述&lt;/h2&gt;&lt;hr&gt;
&lt;p&gt;  Gatys这篇论文的核心思想就是 —— 图片的内容和风格是可以分离的，可以通过神经网络的方式，将图片的风格进行自由交换。&lt;/p&gt;
&lt;p&gt; 如果内容和风格是可以分离的，那么风格的迁移即可转化成这样一个问题：让生成图片的内容与内容来源图片尽可能相似，让图片的风格与风格来源图片尽可能相似。&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="迁移学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="风格迁移" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    
    <category term="Deep Learning" scheme="http://www.laugh12321.cn/blog/tags/Deep-Learning/"/>
    
    <category term="Transfer Learning" scheme="http://www.laugh12321.cn/blog/tags/Transfer-Learning/"/>
    
    <category term="Neural Style Transfer" scheme="http://www.laugh12321.cn/blog/tags/Neural-Style-Transfer/"/>
    
    <category term="Kreas" scheme="http://www.laugh12321.cn/blog/tags/Kreas/"/>
    
    <category term="VGG19" scheme="http://www.laugh12321.cn/blog/tags/VGG19/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统|矩阵分解概述</title>
    <link href="http://www.laugh12321.cn/blog/2019/04/09/mf_techniques_for_rs/"/>
    <id>http://www.laugh12321.cn/blog/2019/04/09/mf_techniques_for_rs/</id>
    <published>2019-04-09T00:00:00.000Z</published>
    <updated>2020-10-23T08:43:32.549Z</updated>
    
    <content type="html"><![CDATA[<p> 推荐系统的研究从上世纪90年代初发展至今，目前有三大主流算法作为几乎全部的推荐算法的基石，它们就是基于内容的过滤算法（content-based filtering，简称CBF）、邻域算法（neighborhood methods）、隐语义模型（latent factor models，简称LFM），其中后两者统称为协同过滤算法（collaborative filtering，简CF）。  </p><a id="more"></a><p> CBF通过给用户、物品定义显式的属性（通常会找所涉及的推荐领域的人类专家来定义）来描述他们的本质，然后为用户推荐与他们本质“门当户对”的物品；CF则是通过发动“群体的力量”，从其他用户、物品中学习到宝贵的信息，无需显式地定义属性：CF下的邻域算法着重于学习用户与用户、物品与物品之间的关系，为目标用户推荐与目标用户相似的用户所选择的物品（user-based）或者与目标用户所选择的物品相似的物品（item-based）；CF下的隐语义模型则是通过学习用户与用户、物品与物品之间的关系来自动获得用户、物品的隐属性（这里的“隐”指的是学习到的属性是不可解释的），相当于把用户-评分矩阵分解成用户隐属性矩阵和物品隐属性矩阵，然后通过用户隐属性向量u与物品隐属性向量i作点乘来获取到该用户对该物品的评分，以此为依据进行推荐。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/0.jpg"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/0.jpg" alt="三种主流方法优缺点">三种主流方法优缺点</a></p><h2 id="矩阵分解的主要思想"><a href="#矩阵分解的主要思想" class="headerlink" title="矩阵分解的主要思想"></a><a href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%9A%84%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3" title="矩阵分解的主要思想"></a>矩阵分解的主要思想</h2><p> 如上图所示，矩阵分解是构建隐语义模型的主要方法，即通过把整理、提取好的“用户—物品”评分矩阵进行分解，来得到一个用户隐向量矩阵和一个物品隐向量矩阵。假设现在有一个 M∗N$M∗N$ 的矩阵，M$M$ 代表用户数，N$N$ 代表物品数，想将用户、物品分别训练出两个隐属性，即每个用户、每个物品都对应着一个二维向量，即得到了一个 M∗2$M∗2$ 的用户隐向量矩阵和一个 N∗2$N∗2$ 的矩阵，分解示意图如下所示：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/1.png"></a></p><p> 在得到用户隐向量和物品隐向量（均是2维向量）之后，我们可以将每个用户、物品对应的二维隐向量看作是一个坐标，将其画在坐标轴上。虽然我们得到的是不可解释的隐向量，但是可以为其赋予一定的意义来帮助我们理解这个分解结果。比如我们把用户、物品的2维的隐向量赋予严肃文学（Serious）vs.消遣文学（Escapist）、针对男性（Geared towards males）vs.针对女性（Geared towards females），那么可以形成如下图的可视化图片：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/2.png"></a></p><p> 从上图我们可以看到，用户对于与其处于同一象限的物品的喜爱度/评分是会很高的，因为他们相比于其它的组合更加“门当户对”一些。而实例人物Dave对应的坐标恰好处于坐标系的中央，这说明他对图中所有物品的喜爱程度差不多，没有特别喜欢的也没有特别讨厌的。</p><h2 id="矩阵分解中的显式反馈与隐式反馈"><a href="#矩阵分解中的显式反馈与隐式反馈" class="headerlink" title="矩阵分解中的显式反馈与隐式反馈"></a><a href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B8%AD%E7%9A%84%E6%98%BE%E5%BC%8F%E5%8F%8D%E9%A6%88%E4%B8%8E%E9%9A%90%E5%BC%8F%E5%8F%8D%E9%A6%88" title="矩阵分解中的显式反馈与隐式反馈"></a>矩阵分解中的显式反馈与隐式反馈</h2><p> 推荐系统、推荐算法的设计依赖于多种输入。在上部分中，我们针对用户—物品评分矩阵来对其进行分解，得到了两个隐向量矩阵。这里我们用到的输入就是这个评分矩阵。在推荐系统中，用户的评分信息输入最重要的输入信息，也是一种显式反馈。不过，显式反馈的信息往往是很稀有的，也就是说我们要分解的评分矩阵往往是一个很稀疏的矩阵，可能里面 70% 以上的元素都是 0，只有 30% 的部分是稀稀落落的评分，所以单纯地依赖显式反馈信息在如今会得到正确率较低的推荐结果。不过矩阵分解的好处在于，它可以融入多种额外的信息。用户的购买、浏览、点击行为虽然不如评分那样有着很大的信息量，但是也是一种隐式反馈的信息，利用好它们，我们可以组成一个很稠密的矩阵，以此来改良推荐结果。</p><h2 id="矩阵分解过程"><a href="#矩阵分解过程" class="headerlink" title="矩阵分解过程"></a><a href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E8%BF%87%E7%A8%8B" title="矩阵分解过程"></a>矩阵分解过程</h2><p>完整代码地址 👉 <a href="https://github.com/DL-Metaphysics/APR/blob/master/MF/MF.ipynb">https://github.com/DL-Metaphysics/APR/blob/master/MF/MF.ipynb</a><br> 先把之后需要用到的全部的数学符号或者缩略语都统一列到下表中：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/3.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/3.png"></a></p><p> 如前面所介绍的，矩阵分解可以融入多种额外信息，不断地对待分解矩阵进行升级、改良，整体的矩阵分解框架如下图所示：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/4.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/4.png"></a></p><p>我们首先看一下没有融合额外信息的时候，是如何来预测评分的：</p><p>^rui=qTi∗pu</p><p>$$r^ui=qiT∗pu$$</p><p> 可以看出，其实很简单的。当我们要预测 u$u$ 对 i$i$ 评分的时候，就直接把对应的用户隐向量、物品隐向量直接点乘起来就好了，就得到预测的评分了。但是这两个矩阵是怎么来的呢？当然是训练出来的啦，我们要通过优化下面这个目标函数来训练出这两个矩阵，这就是标准的“机器学习版”的矩阵分解算法，也是推荐系统领域非常重要的算法：</p><p>minq∗,p∗∑(u,i)∈K(rui−qTi∗pu)2+λ∗(|qi|2+|pu|2)</p><p>$$minq∗,p∗∑(u,i)∈K(rui−qiT∗pu)2+λ∗(|qi|2+|pu|2)$$</p><p>我们采用随机梯度下降（SGD）算法来训练两个隐向量矩阵：</p><p>qi=qi+γ∗(eui∗pu−λ∗qi)</p><p>$$qi=qi+γ∗(eui∗pu−λ∗qi)$$</p><p>pu=pu+γ∗(eui∗qi−λ∗pu)</p><p>$$pu=pu+γ∗(eui∗qi−λ∗pu)$$</p><p>整个SGD的函数代码如下所示：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def sgd(data\_matrix, user, item, alpha, lam, iter\_num):  </span><br><span class="line">  </span><br><span class="line">    for j in range(iter\_num):  </span><br><span class="line">        for u in range(data\_matrix.shape\[0\]):  </span><br><span class="line">            for i in range(data\_matrix.shape\[1\]):  </span><br><span class="line">                if data\_matrix\[u\]\[i\] !&#x3D; 0:  </span><br><span class="line">                    e\_ui &#x3D; data\_matrix\[u\]\[i\] - sum(user\[u,:\] \* item\[i,:\])  </span><br><span class="line">                    user\[u,:\] +&#x3D; alpha \* (e\_ui \* item\[i,:\] - lam \* user\[u,:\])  </span><br><span class="line">                    item\[i,:\] +&#x3D; alpha \* (e\_ui \* user\[u,:\] - lam \* item\[i,:\])  </span><br><span class="line">    return user, item  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p> 这样我们就训练出了两个隐向量矩阵，将他们相乘便得到了预测版的评分矩阵，可以和真实评分矩阵对比一下，如果训练的次数足够，训练步长不大的话其实预测的评分已经比较准了。</p><p> 但是，有一个问题我们需要考虑一下。如果有的用户比较苛刻，对他来说，烂片最多打1分，好的片子也就得3-4分，有的用户比较宽容，他认为人家拍个电影挺不容易的，烂片也给了3分，好的片子一律给5分，那对于这两种用户，我们要采取同样的对待方式进行预测吗？如果有的电影拍的真心好，普遍评分都很高，有的电影烂出了新高度，基本上上3分那都是绝对的高分了，那这两种电影真的都处于0-5分这一分段吗？</p><p> 在这种情况下，我们其实需要为每个用户和每个物品加入一些偏置元素 <code>bu</code> 和 <code>bi</code>，代表了他们自带的与其他事物无关的属性，融入了这些元素，才能区别且正确地对待每一个用户和每一个物品，才能在预测中显得更加个性化。所以，预测评分的计算公式就变成了这样：</p><p>^rui=bui+qTi∗pu</p><p>$$r^ui=bui+qiT∗pu$$</p><p> 我们要优化、训练参数的目标公式也就变成了下图所示，要训练的参数除了用户、物品隐向量还要加上用户、物品偏置值，训练的方法同样是采用随机梯度下降法：</p><p>minq∗,p∗∑(u,i)∈K(rui−bui−qTi∗pu)2+λ∗(|qi|2+|pu|2+b2u+b2i)</p><p>$$minq∗,p∗∑(u,i)∈K(rui−bui−qiT∗pu)2+λ∗(|qi|2+|pu|2+bu2+bi2)$$</p><p>整个SGD_bias的函数的代码如下所示：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def sgd\_bias(data\_matrix, user, item, alpha, lam, iter\_num, miu):  </span><br><span class="line">  </span><br><span class="line">    b\_u &#x3D; \[1\] \* rating\_matrix.shape\[0\]  </span><br><span class="line">    b\_i &#x3D; \[1\] \* rating\_matrix.shape\[1\]  </span><br><span class="line">    for j in range(iter\_num):  </span><br><span class="line">        for u in range(data\_matrix.shape\[0\]):  </span><br><span class="line">            for i in range(data\_matrix.shape\[1\]):  </span><br><span class="line">                if data\_matrix\[u\]\[i\] !&#x3D; 0:  </span><br><span class="line">                    b\_ui &#x3D; b\_u\[u\] + b\_i\[i\] + miu  </span><br><span class="line">                    e\_ui &#x3D; data\_matrix\[u\]\[i\] - b\_ui - sum(user\[u,:\] \* item\[i,:\])  </span><br><span class="line">                    user\[u,:\] +&#x3D; alpha \* (e\_ui \* item\[i,:\] - lam \* user\[u,:\])  </span><br><span class="line">                    item\[i,:\] +&#x3D; alpha \* (e\_ui \* user\[u,:\] - lam \* item\[i,:\])  </span><br><span class="line">                    b\_u\[u\] +&#x3D; alpha \* (e\_ui - lam \* b\_u\[u\])  </span><br><span class="line">                    b\_i\[i\] +&#x3D; alpha \* (e\_ui - lam \* b\_i\[i\])  </span><br><span class="line">    return user, item, b\_u, b\_i  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p> 在加入偏置后，预测的就更加准确一些了。在编程实验中，笔者采用了推荐系统算法常见的评测标准——MSE来进行两种分解算法的评测，两者的结果如下所示（注：训练步长0.001，正则化系数0.1，训练次数：1000次）：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/5.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/RS_MF/5.png" alt="可以看到，效果的提升幅度还是很大的">可以看到，效果的提升幅度还是很大的</a></p><p>原文链接 - <a href="https://zhuanlan.zhihu.com/p/28577447?group_id=881547532893851649">论文篇：Matrix Factorization Techniques for RS</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt; 推荐系统的研究从上世纪90年代初发展至今，目前有三大主流算法作为几乎全部的推荐算法的基石，它们就是基于内容的过滤算法（content-based filtering，简称CBF）、邻域算法（neighborhood methods）、隐语义模型（latent factor models，简称LFM），其中后两者统称为协同过滤算法（collaborative filtering，简CF）。  &lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="推荐系统" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="矩阵分解" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    
    
    <category term="Deep Learning" scheme="http://www.laugh12321.cn/blog/tags/Deep-Learning/"/>
    
    <category term="Recommender System" scheme="http://www.laugh12321.cn/blog/tags/Recommender-System/"/>
    
    <category term="RS" scheme="http://www.laugh12321.cn/blog/tags/RS/"/>
    
    <category term="MF" scheme="http://www.laugh12321.cn/blog/tags/MF/"/>
    
  </entry>
  
  <entry>
    <title>深度学习|卷积神经网络概述</title>
    <link href="http://www.laugh12321.cn/blog/2019/03/26/cnn_overview/"/>
    <id>http://www.laugh12321.cn/blog/2019/03/26/cnn_overview/</id>
    <published>2019-03-26T00:00:00.000Z</published>
    <updated>2020-10-24T11:06:56.197Z</updated>
    
    <content type="html"><![CDATA[<p><strong>实现简单手写识别完整代码地址：</strong>👉<br><a href="https://github.com/laugh12321/Handwriting-Recognition">laugh12321/Handwriting-Recognition</a></p><p>提到人工智能，人们都会希望自己的智能机器具备「听说读写」这样像人一样的基本感知表达能力，计算机视觉就是研究如何让机器学会「看」的学科。如今，借助深度学习的推动，计算机视觉已经来到了一个飞速发展的时代。人脸识别，自动驾驶，医学图像分析，计算机视觉的成熟让这一切变得可能甚至远高于我们人类的工作效率。  </p><a id="more"></a><p>在计算机视觉领域，卷积神经网络发挥了重要价值。其实它的思想来源很简单，就像神经网络通过模拟生物神经元工作的思路创建，卷积神经网络也在模拟我们的视觉。1959 年，Hubel 和 Wiesel 的实验表明，生物的视觉处理是从简单的形状开始的，比如边缘、直线、曲线。凭借这一发现，Hubel 和 Wiesel 获得了1981 年的诺贝尔生理学或医学奖。卷积神经网络在逐层学习过程中，也模拟着这个过程，第一层学习比较低层的图片结构，第二层学习依据第一层的结果学习高一级的特征，这样最后一层依据前一层学到的高级特征就能完成我们的学习任务。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/2.png"></center><p>比如识别人脸，第一层可能学到了简单的线条，第二层学到了轮廓，倒数第二层学到了双眼皮和耳垂，最后一层识别出了她是你的高中班主任。可以说，卷积神经网络现在主宰了计算机视觉领域，通过灵活运用卷积神经网络的结构，会让我们的机器「看」得更多、更清楚。</p><p>卷积神经网络一般是由卷积层、池化层和全连接层堆叠而成的前馈神经网络结构。与前面讲到的前馈神经网络相似，卷积神经网络同样使用反向传播算法进行训练。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/3.png"></center><p>上图展示了一个基础的卷积神经网络结构。当然，我们通常使用到的网络结构更深、更复杂，一般是由许多卷积层、池化层和全连接层交叉堆叠而成。</p><p>接下来的实验内容将带你逐步了解卷积层和池化层的概念，并构建一个深度卷积神经网络。</p><h2 id="卷积层-Convolution-Layer"><a href="#卷积层-Convolution-Layer" class="headerlink" title="卷积层 Convolution Layer"></a>卷积层 Convolution Layer</h2><p>现在我们来逐步深入了解卷积神经网络。首先，你需要了解一张图片在计算机眼中的样子。</p><p>下图表述了像素矩阵的概念。具体来讲，我们可能看到的是一张图的整体视觉效果，但从计算机中读取图片，图片是通过像素矩阵标识的。每一个像素具有一个数值（0-255），代表图片在该信道像素点位置的大小。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/4.png"></center><p>信道这个概念，大家可以理解为颜色。我们常用的 RGB 信道分别代表红绿蓝三种。通过把三个信道的像素矩阵叠加起来，就可以组成我们看到的色彩斑斓的图片了。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/5.png"></center><p>所以在计算机中，图片是一个 $m×n×k$ 的三维矩阵，其中 $m$ 是矩阵的高度，$n$ 是矩阵的宽度，$k$ 是矩阵的深度。基于这样的数据结构，接下来可以进行卷积运算了。</p><h3 id="卷积核-Convolution-Kernel"><a href="#卷积核-Convolution-Kernel" class="headerlink" title="卷积核 Convolution Kernel"></a>卷积核 Convolution Kernel</h3><p>接下来，我们首先需要了解卷积核的概念。如下图所示，我们现在有一个图像矩阵，同时定义了一个卷积核（权值矩阵）。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/6.png"></center><p>其中，卷积核的作用就是从输入矩阵（图像）中提取一定的特征。动画如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/7.png"></center><p>上面的动画过程，其实就是一个卷积的过程，不知你是否能一眼就看明白？</p><p>简单来讲，我们将卷积核沿着图像矩阵的左上角开始移动，不断运算得到右边的矩阵。卷积计算的过程非常简单，就是乘法和加法的运算。例如，当我们第一次把卷积核置于图像矩阵的左上角时：</p><p>$$<br>1 \times 0 + 2 \times 1 + 2 \times 1 + 1 \times 2 = 6<br>$$</p><p>通过不断平移卷积核，同时与输入矩阵进行卷积操作，就可以得到卷积后矩阵。</p><p>如何理解卷积的过程，最简单的就是把卷积核看作是一片滤镜，原矩阵通过滤镜之后就得到了新的特征矩阵。</p><p>如果你想形象一点了解卷积核究竟在做一件什么事情，就可以把卷积后的图片与原图对比呈现。下图就是一张图片通过不同卷积核做卷积操作后的结果，我们可以明显察觉到不同的卷积核可以帮助我们分解出原图片不同层次的特征。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/8.png"></center><p>所以，如果卷积核能够帮助分解图片特征并运用到机器学习的过程中，例如识别一辆车能看到四个轮子以及车窗的细节，你可以想象这将会对提升准确率有极大帮助。</p><h3 id="卷积步长-Stride"><a href="#卷积步长-Stride" class="headerlink" title="卷积步长 Stride"></a>卷积步长 Stride</h3><p>上面的卷积过程相信你已经看明白了。这时候有一个问题，如果我们在对图片卷积时都依次移动，效率会不会比较低？</p><p>是的，对于一些很大的图片，每次只移动一步运算效率会很低下，同时分解得到的特征也很容易冗余。于是，可以引入一个超参数能对每一次移动的步数进行调节，我们把它称之为卷积步长 Stride。</p><p>同样，我们通过一组动图来查看不同卷积步长 Stride 的移动效果：</p><p>当卷积步长为 <code>1</code> 时，也就是和上面相同的移动过程：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/9.png"></center><p>此时，输出矩阵大小为：</p><p>$$<br>output = (input - kernel) + 1<br>$$</p><p>如果我们将卷积步长设为 <code>2</code>，就是下面的效果：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/10.png"></center><p>可以发现，当 <code>Stride=2</code> 横向移动以及纵向移动时，每一步都跳过了一个单元格。也就是从移动 <code>1</code> 步变成了 <code>2</code> 步。</p><p>此时，输出矩阵大小为（向下取整）：</p><p>$$<br>output = \lfloor \frac{input - kernel}{stride} \rfloor + 1<br>$$</p><h3 id="边距扩展-Padding"><a href="#边距扩展-Padding" class="headerlink" title="边距扩展 Padding"></a>边距扩展 Padding</h3><p>介绍了步长的概念，接下来再介绍一个卷积操作过程中的重要概念，也就是 Padding。Padding 也就是边距，当然我们一般都直接使用英文。步长解决了卷积效率低的问题，但又造成了新的问题。你会发现，随着步长的增大，卷积输出的矩阵将持续变小。但是，在构建网络的时候我们往往希望输出矩阵的大小为指定大小，而不完全由卷积步长的变换而左右。于是，就有了 Padding 的操作。</p><p>一旦我们确定了原矩阵的大小和步长的大小，得到的卷积矩阵大小将是确定的。假如我们希望卷积得到的矩阵大小变大，在不改变卷积核大小和步长的前提下，唯一的方法就是调整原矩阵的大小。于是，我们就通过对原矩阵进行 Padding 操作（扩大边距）达到目的。</p><p>Padding 比较常见的操作是在输入矩阵最外围补上一圈 <code>0</code> 之后，再参与卷积运算。这样，我们就可以自行控制输出矩阵的大小。下面列举几个常见的 Padding 方式：</p><p>Arbitrary Padding 在输入图像周围填充 <code>0</code>，使输出矩阵尺寸大于输入尺寸。例如，下图对 $5×5$ 的输入矩阵外围补上 2 圈 <code>0</code> 后，与 $4×4$ 的卷积核做卷积运算，最终得到 $6×6$ 的输出矩阵的过程。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/11.png"></center><p>此时，输出矩阵大小为：</p><p>$$<br>output = (input - kernel) + 2 * padding + 1<br>$$</p><p>Half Padding 也叫 Same Padding，它希望得到的输出矩阵尺寸与输入尺寸一致。下图即为我们对 $5×5$ 的输入矩阵外围补上一圈 <code>0</code> 后，与 $3×3$ 的卷积核做卷积运算，最终依然得到 $5×5$ 的输出矩阵的过程。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/12.png"></center><p>还有一种是 Full Padding，卷积核从输入矩阵左角第一个方块开始，依次移动到右下角的最后一个方块。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/13.png"></center><p>其实，Padding 并不是说只有这几种，还有其他的一些形式，包括 Padding 和 Stride 结合在一起的一些卷积形式，甚至你可以自定义 Padding 和 Stride 的规则，这里也就不再一一介绍了。</p><h3 id="高维多卷积核过程"><a href="#高维多卷积核过程" class="headerlink" title="高维多卷积核过程"></a>高维多卷积核过程</h3><p>前面，我们已经介绍了矩阵通过单个卷积核的过程，相信你已经对卷积核、步长、Padding 等概念非常熟悉了。</p><p>一张黑白图片形成了单个像素矩阵，但如果是一张用 RGB 通道的彩色图片，将会形成 3 维的像素矩阵。这该怎么办呢？</p><p>对于一个 $m×n×k$ 的三维矩阵，它是如何在这种二维结构的卷积下操作的呢？其实，在卷积神经网络设计上有一个重要原则，就是卷积核也是一个三维 $a×b×k$ 的矩阵(通常 $a=b$，且常取 <code>1，3，5，7</code> 这样的数)。更重要的是，一定要保证输入矩阵大小与卷积核大小在第三个维度上值相等。</p><p>这样，我们在第三个维度上对相同深度的输入矩阵与卷积核做如上的卷积运算时，最后叠加所有深度的值得到一个二维的输出矩阵。也就是说，对 $m×n×k$ 的输入矩阵与 $a×b×k$ 的卷积核，我们如果做一个 $stride==s$，$padding=p$ 的卷积操作，那么将会得到一个 $[(m+2p-a)/s+1]\times [(n+2p-a)/s+1]$ 的二维矩阵。</p><p>那么如何保证输出矩阵依然是一个三维矩阵呢？其实，只需要每一层都由多个卷积核来实现操作就行了，最后把得到的所有二维矩阵叠起来，就得到了一个 $[(m+2p-a)/s+1]\times [(n+2p-a)/s+1]\times c$，$c$ 为卷积核的个数的输出矩阵。</p><p>整个过程可以由下图更直观地表示，一个 $5×5×3$ 的输入矩阵被两个 $3×3×3$ 的卷积核执行 $stride=2$，$padding=1$ 卷积后，得到了 $3×3×2$ 的输出矩阵：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/14.png"></center><p>以上就是一个高维矩阵通过多卷积核的执行过程。</p><h2 id="池化层-Pooling-Layer"><a href="#池化层-Pooling-Layer" class="headerlink" title="池化层 Pooling Layer"></a>池化层 Pooling Layer</h2><p>上面介绍了卷积层，接着介绍卷积神经网络中另一个非常重要的层，也就是池化层。池化操作其实就是降采样操作过程。我们都知道，图片是一个非常大的训练数据，所以想达到很好的训练性能，只有卷积层是不够的。池化层通过降采样的方式，在不影响图像质量的情况下，压缩图片，达到减少训练参数的目的。</p><p>需要注意的是，往往在几个卷积层之后我们就会引入池化层，池化层没有需要学习的参数，且池化层在每一个深度上独立完成，就是说池化后图像的纵深保持不变。下面介绍两种常用的池化方式。</p><h3 id="最大值池化"><a href="#最大值池化" class="headerlink" title="最大值池化"></a>最大值池化</h3><p>对于池化操作，我们需要界定一个过滤器和步长。最大值池化，取输入矩阵在过滤器大小上矩阵块的最大值作为输出矩阵对应位置输出。如下图：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/18.png"></center><h3 id="平均值池化"><a href="#平均值池化" class="headerlink" title="平均值池化"></a>平均值池化</h3><p>除了最大值池化，另一个常用的池化方法是平均值池化。顾名思义，这是取输入矩阵在过滤器大小上矩阵块的平均值作为输出矩阵对应位置输出。如下图：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/19.png"></center><p>相比于卷积，池化的过程就更加简单了。相信你通过上面的示意图能很清楚的看出最大池化和平均池化的过程。</p><p><strong>CNN 实现数字手写识别的完整代码可以在我的 GitHub 上查看：</strong> 👉 <a href="https://github.com/laugh12321/Handwriting-Recognition/blob/master/CNN_TF.ipynb">laugh12321/Handwriting-Recognition</a></p><h2 id="经典的卷积神经网络"><a href="#经典的卷积神经网络" class="headerlink" title="经典的卷积神经网络"></a>经典的卷积神经网络</h2><p>深度学习领域有很多的技巧需要大量的探索与实践。比如对于一个人脸识别任务，我们搭建的网络具有很强的多样性。大量数据集和多次实验结果下，有些经典的神经网络孕育而生。这些网络从诞生时到现在都有着独特的优越性能和革命性的创新思想。所以，建议大家在深度学习的领域学习中，熟悉这些经典神经网络的架构，这将进一步深化我们对深度神经网络的理解。</p><p>实际上，在对神经网络的理论探索中，尚不具有十分充分的理论证明为什么一些经典网络有如此强大的性能。当然，深度学习还有很多未得到解释的地方，包括神经网络每一步究竟在学习什么，每个参数是如何影响学习性能或者迭代收敛过程，这些问题都有待进一步探寻。所以说，深度神经网络的调参过程充满了「玄学」的感觉，这需要大量的实践积累。</p><h3 id="LeNet-神经网络"><a href="#LeNet-神经网络" class="headerlink" title="LeNet 神经网络"></a>LeNet 神经网络</h3><p>LeNet-5 是一个很早的经典卷积神经网络。Yann LeCun 大牛（被称作卷积神经网络之父）在 1998 年搭建了 LeNet，并且在手写识别网络(MNIST)上广泛使用。LeNet 神经网络的架构如下图：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/21.png"></center><p>其中，网络中的下采样采用了平均值池化。前面各层都由 tanh 激活，最后一层由 RBF 激活。经过测试，LeNet 在 6 万张原始图片的数据集中，错误率能降低到 0.95%。</p><p>LeNet 一经发布就带来了一波卷积神经网络潮流。很可惜，由于那个年代计算能力不强，使得卷积神经网络的多样性受到严重限制，研究者都不愿意将网络深化，卷积神经网络只掀起了一波热潮后又几乎无人问津。</p><h3 id="AlexNet-神经网络"><a href="#AlexNet-神经网络" class="headerlink" title="AlexNet 神经网络"></a>AlexNet 神经网络</h3><p>2012 年 ImageNet 比赛上，一个卷积神经网络的表现力大放异彩，这就是 AlexNet。经过了十几年的硬件发展，计算机运算能力大幅增强，我们可运用的数据也大幅增加，AlexNet 由此孕育而生。</p><p>AlexNet 的结构很像 LeNet-5，但是网络变得更大更深。同时，AlexNet 是第一个将卷积层叠层后再叠上池化层的网络。不仅如此，AlexNet 运用上 Dropout（训练时，按照一定的概率将神经元暂时从网络中丢弃）和数据增强两种方法降低过拟合，以及 LRN(local responce normalization) 做为规划层来促使神经元学到更加广泛的特征。可以说，AlexNet 的出色表现，让计算机视觉领域开始逐渐由卷积神经网络主宰。</p><p>下面就是 AlexNet 的网络结构图：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/22.png"></center><p>原作者在训练 AlexNet 时，一个 GPU 负责图中顶层部分，一个 GPU 运行图中底层部分，GPU 之间仅在某些层互相通信。5 个卷积层，5 个池化层，3 个全连接层，大约 5000 万个可调参数组成了这个经典的卷积神经网络。最后的全连接层输出到 1000 维的 softmax 层，产生一个覆盖 1000 类标记的分布。最终完成了对 ImageNet 的分类任务。</p><p><strong>模仿 AlexNet 搭建一个卷积神经网络用于手写识别网络的完整代码可以在我的 GitHub 上查看：</strong> 👉<br><a href="https://github.com/laugh12321/Handwriting-Recognition/blob/master/AlexNet_TF.ipynb">laugh12321/Handwriting-Recognition</a></p><h3 id="VGG-神经网络"><a href="#VGG-神经网络" class="headerlink" title="VGG 神经网络"></a>VGG 神经网络</h3><p>VGG Net 可以看成是 Alex Net 的加深版本，同样运用 Alex Net 所带来的思路。VGG Net 拥有 5 个卷积组，2 层全图像连接特征和 1 层全连接分类特征。另外，Alex Net 只有 8 层，但是 VGG Net 通常有 16 到 19 层。</p><p>VGG Net 的革命性在于，它在论文中给出了不同的卷积层配置方法，而且从论文实验结果可以看出，随着卷积层从 8 到 16 一步步加深，通过加深卷积层数准确率已经达到了瓶颈数值。这项研究表明了单纯添加卷积层往往并不能起到更好的效果，这将促使以后的卷积神经网络朝向增强卷积层功能的思路发展。</p><p>下图描述了一种配置的 VGG 网络结构：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/24.png"></center><h3 id="Google-Net-神经网络"><a href="#Google-Net-神经网络" class="headerlink" title="Google Net 神经网络"></a>Google Net 神经网络</h3><p>自 VGG Net 之后，卷积神经网络朝向另一个方向演化，那就是增强卷积模块的功能。</p><p>谷歌公司提出的 Google Net 可以说带来了革命性的成果。虽然乍一看好像是因为它将网络变深了，实际上是其中的一个小模块的设计技巧加强了所有卷积层的学习性能。谷歌公司把它称作 Inception，灵感来源于当时同时期的电影 Inception（盗梦空间）。Inception 模块如下图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/25.png"></center><p>这个多层感知卷积层的引入，来源于一个很直观的思想，就是一个卷积网络里面的局部稀疏最优结构往往可以由简单可复用的密集组合来近似或者替代。就像上图里面 <code>1x1, 3x3, 5x5</code> 的卷积层，与 <code>3x3</code> 的池化层的组合一个 <code>Inception</code>。这样做是出于以下几点考虑：</p><ol><li>不同尺寸的卷积核可以提取不同尺度的信息。</li><li>采用 <code>1x1,3x3, 5x5</code> 可以方便对齐，Padding 分别为 <code>0,1,2</code> 就可以对齐。</li><li>由于池化层在 CNN 网络里面的成功运用，也把池化层当做组合的一部分，在 GoogleNet 论文里也说明了它的有效性。</li></ol><p>由于 Google Net 是好几个 Inception 模块的堆叠，而且往往越后面的 Inception 模块能够提取到更加高级抽象的特征。</p><p>前一层经过 $1×1$ 或者池化层降维后，在全连接层过滤器将 $1×1$, $3×3$, $5×5$ 的卷积结果连接起来，使网络的深度和宽度均可以扩大。论文显示，Inception 模块使整个网络的训练过程有 2 到 3 倍的加速，且更容易捕捉到关键特征用以学习。同时，由于网络很深，为了避免梯度消失的问题，Google Net 还巧妙地在不同深度增加了两个损失函数来训练。</p><p>整个 Google Net 结构如下图：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/26.png"></center><p>可以看出 Google Net 已经是非常深的网络了，你可以通过新标签页打开图片查看细节。Google Net 所依赖的训练时间与运算成本都上了一个台阶，Inception 这个创意性的设计，极大地优化了卷积神经网络的学习过程。</p><h3 id="ResNet-神经网络"><a href="#ResNet-神经网络" class="headerlink" title="ResNet 神经网络"></a>ResNet 神经网络</h3><p>如果把网络加深和增强卷积模块功能两个演化方向相结合，就形成了 ResNet。2015 年何明凯团队发布的 ResNet 可以说影响了整个深度学习界。首先让我们看看这个网络有多么震撼：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/27.png"></center><p>ResNet 的训练深度达到了 152 层，上图（最右）仅是一个迷你型的 34 层 ResNet 结构。左图是 VGG-19，中间是不具备 Residual 结构的深度神经网络。</p><p>可以很明显的看出，ResNet 将部分网络进行了一个跳层传递的操作。网络在深化的过程中，可能会出现一种叫网络退化的问题，ResNet 通过引入这个跳层传递，将前面某层的结果与这一层的卷积结果相加，去优化一个残差来规避这些问题。</p><p>ResNet 将我们的网络深度倍数化增加，从最多 20 层一下子拉到了 100 层以上，而且网络也不再是平常的层数堆叠，最终模型预测准确率已经达到了一个巨大的量变提高。</p><p>这些经典的神经网络都有它的开创性和典型性，我们可以像上面实现 AlexNet 一样，用 Tensorflow 实现其他网络。但后面的网络结构都太复杂，实验就不再动手搭建。如果你有兴趣，可以依据网络结构，仔细研读该网络的论文，试着动手搭建这些网络。</p><ul><li>Deep Residual Learning for Image Recognition: <a href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a></li></ul><h3 id="卷积神经网络的发展史"><a href="#卷积神经网络的发展史" class="headerlink" title="卷积神经网络的发展史"></a>卷积神经网络的发展史</h3><p>最后，我们通过一张图来总结卷积神经网络的发展史：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/28.png"></center><h2 id="为什么要使用卷积神经网络？"><a href="#为什么要使用卷积神经网络？" class="headerlink" title="为什么要使用卷积神经网络？"></a>为什么要使用卷积神经网络？</h2><p>这是一个很关键的问题，大家肯定也有这样的疑问。之前我们的神经网络像全连接网络这样，让每个神经元互相连接然后激活，已经可以达到不错的学习效果，也可以拟合去学习各种复杂的过程。</p><p>那么，经过以上对卷积神经网络的深入了解后，你能否简要回答这个问题呢？</p><p>形象地来讲，卷积神经网络保留了图像的空间信息。</p><p>19 世纪 60 年代，科学家通过对猫的视觉皮层细胞研究发现，每一个视觉神经元只会处理一小块区域的视觉图像，即感受野。我们也看到在卷积神经网络中，一个卷积层可以有多个不同的卷积核（权值），而每个卷积核在输入图像上滑动且每次只处理一小块图像。</p><p>这样，输入端的卷积层可以提取到图像中最基础的特征，比如不同方向的直线或者拐角；接着再组合成高阶特征，比如三角形、正方形等；再继续抽象组合，得到眼睛、鼻子和嘴等五官；最后再将五官组合成一张脸，完成匹配识别。过程中，每个卷积层提取的特征，在后面的层中都会抽象组合成更高阶的特征，最终习得足够多足够关键的特征，完成学习任务。</p><p>从运算上讲，卷积层具有两个关键优势，分别是局部连接和权值共享。</p><p>每个神经元只与上一层的一个局部区域连接，该连接的空间大小可视为神经网络神经元的感受野。权值共享的意义在于，当前层在深度方向上每层神经元都使用同样的权重和偏差。</p><p>这里作了一个合理的假设：如果一个特征在计算某个空间位置 $(x,y)$ 的时候有用，那么它在计算另一个不同位置 $(x2,y2)$ 的时候也有用。我们将局部连接和权值共享结合，就降低了参数量，使训练复杂度大大下降。这样的操作还能减轻过拟合，权值共享同时还赋予了卷积网络对平移的容忍性。</p><p>试想，如果我们用全连接神经网络来做图像识别任务，图片是由像素点组成的，用矩阵表示的，$28×28$ 的矩阵，我们得把它「拍平」后，变成一个 $784$ 的一列向量。如果该列向量和隐含层的 $15$ 个神经元连接，就有 $784×15=11760$ 个权重 $w$。如果，隐含层和最后的输出层的 $10$ 个神经元连接，就有 $11760×10=117600$ 个权重 $w$ 。最后，再加上隐含层的偏置项 $15$ 个和输出层的偏置项 $10$ 个，就得到了 $117625$ 个参数。这是多么恐怖。</p><p>下图表示一个三层全连接网络的结构，输入为图片即 $784$ 个神经元，隐藏层 $15$ 个神经元，输出层 $10$ 个神经元：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/CNN/29.png"></center><p>那么，试想我们如果用卷积神经网络来解决。卷积层里，我们只需要学习卷积核（权值），就像前面搭建的卷积神经网络中，第一个卷积层只需要学习 $3×3×1×8=72$ 个参数，第二层学习 $3×3×8×10=720$ 个参数，池化层不需要学习任何参数，全连接层参数也大幅下降。这样算下来，所需要学习的参数相比上面的三层全连接层已经大幅减少。这就是卷积神经网络的好处。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;实现简单手写识别完整代码地址：&lt;/strong&gt;👉&lt;br&gt;&lt;a href=&quot;https://github.com/laugh12321/Handwriting-Recognition&quot;&gt;laugh12321/Handwriting-Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;提到人工智能，人们都会希望自己的智能机器具备「听说读写」这样像人一样的基本感知表达能力，计算机视觉就是研究如何让机器学会「看」的学科。如今，借助深度学习的推动，计算机视觉已经来到了一个飞速发展的时代。人脸识别，自动驾驶，医学图像分析，计算机视觉的成熟让这一切变得可能甚至远高于我们人类的工作效率。  &lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="卷积神经网络" scheme="http://www.laugh12321.cn/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="Deep Learning" scheme="http://www.laugh12321.cn/blog/tags/Deep-Learning/"/>
    
    <category term="CNN" scheme="http://www.laugh12321.cn/blog/tags/CNN/"/>
    
    <category term="CV" scheme="http://www.laugh12321.cn/blog/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|梯度下降详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/03/12/gradient_descent_plus/"/>
    <id>http://www.laugh12321.cn/blog/2019/03/12/gradient_descent_plus/</id>
    <published>2019-03-12T00:00:00.000Z</published>
    <updated>2020-10-24T10:37:38.116Z</updated>
    
    <content type="html"><![CDATA[<p>在了解<strong>梯度下降（Gradient Descent）</strong>之前，我们先要知道有关<strong>线性回归</strong>的基本知识，这样可以进一步的加深对<strong>梯度下降</strong>的理解，当然<strong>梯度下降（Gradient Descent）</strong>并不单单只能进行回归预测，它还可以进行诸如分类等操作。</p><a id="more"></a><p>关于<strong>线性回归</strong>的具体讲解本文不详细涉及，只简单列出几个相关公式。(关于线性回归可以看这篇 👉<a href="https://www.laugh12321.cn/2019/01/01/Linear_Regression/#more">传送门</a>)</p><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p><strong>公式 4-1：线性回归模型预测</strong><br>$$<br>\hat{y} = \theta_0 + \theta_1x_{1} + \theta_2x_{2} + … + \theta_nx_{n}<br>$$</p><ul><li>$\hat{y}$ 是预测值</li><li>$n$ 是特征的数量</li><li>$x_i$ 是 $i$ 个特征值</li><li>$θ_j$ 是第 $j$ 个模型参数 (包括偏置项 $θ_0$ 以及特征权重 $θ_1,θ_2,…,θ_n$</li></ul><p>也可以用更为简洁的向量化形式表达</p><p><strong>公式 4-2：线性回归模型预测 (向量化)</strong></p><p>$$<br>\hat{y} = h_{\theta}(X) = \theta^T \cdot X<br>$$</p><ul><li>$θ$ 是模型的参数向量，包括偏置项 $θ_0$ 以及特征权重 $θ_1$ 到 $θ_n$</li><li>$θ^T$ 是 $θ$ 的转置向量 (为行向量，而不再是列向量)</li><li>$X$ 是实例的特征向量，包括从 $θ_0$ 到 $θ_n,θ_0$ 永远为 $1$</li><li>$θ^T⋅X$ 是 $θ^T$ 和 $X$ 的点积</li><li>$h_θ$ 是模型参数 $θ$ 的假设函数</li></ul><p><strong>公式 4-3：线性回归模型的 $MSE$ 成本函数</strong></p><p>$$<br>MSE(X, h_{\theta}) = \frac{1}{m}\sum_{i=1}^{m} (\theta^T \cdot X^{(i)} - y^{(i)})^2<br>$$</p><h3 id="标准方程"><a href="#标准方程" class="headerlink" title="标准方程"></a>标准方程</h3><p>为了得到使成本函数最小的 $θ$ 值，有一个闭式解方法——也就是一个直接得出结果的数学方程，即标准方程。</p><p><strong>公式 4-4：标准方程</strong></p><p>$$<br>MSE(X, h_{\theta}) = \frac{1}{m}\sum_{i=1}^{m} (\theta^T \cdot X^{(i)} - y^{(i)})^2<br>$$</p><ul><li>$\hat{\theta}$ 是使成本函数最小的 $θ$ 值</li><li>$y$ 是包含 $y^{(1)}$ 到 $y^{(m)}$ 的目标值量</li></ul><p>我们生成一些线性数据来测试这个公式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.plot(X, y, <span class="string">&quot;b.&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_6_0.png"></center><p>现在我们使用标准方程来计算 $\hat{y}$。使用 Numpy 的线性代数模块 <code>np.linalg</code> 中的 <code>inv()</code> 函数来对矩阵求逆，并用 <code>dot()</code> 方法计算矩阵的内积：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_b = np.c_[np.ones((<span class="number">100</span>, <span class="number">1</span>)), X] <span class="comment"># add xo = 1 to each instance</span></span><br><span class="line">theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br></pre></td></tr></table></figure><p>我们实际用来生成数据的函数是 $y = 4 + 3x_0 + 高斯噪声$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta_best</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[4.0939709 ],</span><br><span class="line">       [3.08934507]]) </span><br></pre></td></tr></table></figure><p>我们期待的是 $θ_0=4,θ_1=3$ 得到的是 $\theta_0 = 4.0939709, \theta_1 = 3.08934507$。非常接近了，因为噪声的存在使其不可能完全还原为原本的函数。</p><p>现在可以用 $\hat{\theta}$ 做出预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_new = np.array([[<span class="number">0</span>], [<span class="number">2</span>]])</span><br><span class="line">X_new_b = np.c_[np.ones((<span class="number">2</span>, <span class="number">1</span>)), X_new] <span class="comment"># add x0 = 1 to each instance</span></span><br><span class="line">y_predict = X_new_b.dot(theta_best)</span><br><span class="line">y_predict</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[ 4.0939709 ],</span><br><span class="line">       [10.27266104]]) </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制模型的预测结果</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.plot(X_new, y_predict, <span class="string">&quot;r-&quot;</span>)</span><br><span class="line">ax.plot(X, y, <span class="string">&quot;b.&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_14_0.png"></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scikit-Learn 的等效代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LinearRegression(copy_X&#x3D;True, fit_intercept&#x3D;True, n_jobs&#x3D;None,</span><br><span class="line">         normalize&#x3D;False) </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lin_reg.intercept_, lin_reg.coef_</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([4.0939709]), array([[3.08934507]])) </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lin_reg.predict(X_new)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[ 4.0939709 ],</span><br><span class="line">       [10.27266104]]) </span><br></pre></td></tr></table></figure><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降是一种非常通用的优化算法，能够为大范围的问题找到最优解。梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。</p><p>假设你迷失在山上的浓雾之中，你能感觉到的只有你脚下路面的坡度。快速到达山脚的一个策略就是沿着最陡的方向下坡。这就是梯度下降的做法：通过测量参数向量 $θ$ 相关的误差函数的局部梯度，并不断沿着降低梯度的方向调整，直到梯度降为0，到达最小值！</p><p>具体来说，首先使用一个随机的 $θ$ 值（这被称为随机初始化），然后逐步改进，每次踏出一步，每一步都尝试降低一点成本函数（如 $MSE$ ），直到算法收敛出一个最小值（参见图4-3）</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_00.jpg"></center><p>梯度下降中一个重要参数是每一步的步长，这取决于超参数学习率。如果学习率太低，算法需要经过大量迭代才能收敛，这将耗费很长时间（参见图4-4）。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_01.jpg"></center><p>反过来说，如果学习率太高，那你可能会越过山谷直接到达山的另一边，甚至有可能比之前的起点还要高。这会导致算法发散，值越来越大，最后无法找到好的解决方案（参见图4-5）。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_02.jpg"></center><p>最后，并不是所有的成本函数看起来都像一个漂亮的碗。有的可能看着像洞、像山脉、像高原或者是各种不规则的地形，导致很难收敛到最小值。图4-6显示了梯度下降的两个主要挑战：如果随机初始化，算法从左侧起步，那么会收敛到一个局部最小值，而不是全局最小值。如果算法从右侧起步，那么需要经过很长时间才能越过整片高原，如果你停下得太早，将永远达不到全局最小值。</p><p>幸好，线性回归模型的 $MSE$ 成本函数恰好是个凸函数，这意味着连接曲线上任意两个点的线段永远不会跟曲线相交。也就是说不存在局部最小，只有一个全局最小值。它同时也是一个连续函数，所以斜率不会产生陡峭的变化。这两件事保证的结论是：即便是乱走，梯度下降都可以趋近到全局最小值（只要等待时间足够长，学习率也不是太高）。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_03.jpg"></center><p>成本函数虽然是碗状的，但如果不同特征的尺寸差别巨大，那它可能是一个非常细长的碗。如图4-7所示的梯度下降，左边的训练集上特征1和特征2具有相同的数值规模，而右边的训练集上，特征1的值则比特征2要小得多。因为特征1的值较小，所以 $θ_1$ 需要更大的变化来影响成本函数，这就是为什么碗形会沿着 $θ_1$ 轴拉长。）</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_04.jpg"></center><p>正如你所见，左图的梯度下降算法直接走向最小值，可以快速到达。而在右图中，先是沿着与全局最小值方向近乎垂直的方向前进，接下来是一段几乎平坦的长长的山谷。最终还是会抵达最小值，但是这需要花费大量的时间。</p><blockquote><p><strong>注意：</strong> 应用梯度下降时，需要保证所有特征值的大小比例都差不多 （比如使用<code>Scikit-Learn</code>的<code>StandardScaler</code>类），否则收敛的时间会长很多。</p></blockquote><p>这张图也说明，训练模型也就是搜寻使成本函数（在训练集上）最小化的参数组合。这是模型参数空间层面上的搜索：模型的参数越多，这个空间的维度就越多，搜索就越难。同样是在干草堆里寻找一根针，在一个三百维的空间里就比在一个三维空间里要棘手得多。幸运的是，线性回归模型的成本函数是凸函数，针就躺在碗底。</p><h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p>要实现梯度下降，你需要计算每个模型关于参数 $θ_j$ 的成本函数的梯度。换言之，你需要计算的是如果改变 $θ_j$ ，成本函数会改变多少。这被称为偏导数。这就好比是在问 “如果我面向东，我脚下的坡度斜率是多少？” 然后面向北问同样的问题（如果你想象超过三个维度的宇宙，对于其他的维度以此类推）。公式4-5计算了关于参数 $θ_j$ 的成本函数的偏导数，计作$\frac{\partial}{\partial \theta_j}MSE(\theta)$</p><p><strong>公式 4-5 ：成本函数的偏导数</strong></p><p>$$<br>\frac{\partial}{\partial \theta_j}MSE(\theta) = \frac{2}{m}\sum_{i=1}^{m} (\theta^T \cdot X^{(i)} - y^{(i)})x_j^{(i)}<br>$$</p><p>如果不想单独计算这些梯度，可以使用公式4-6对其进行一次性计算。梯度向量，记作 $\nabla_\theta MSE(\theta)$ ，包含所有成本函数（每个模型参数一个）的偏导数。</p><p><strong>公式 4-6 ：成本函数的梯度向量</strong></p><p>$$<br>\nabla_\theta MSE(\theta) =  \left[  \begin{matrix}    \frac{\partial}{\partial \theta_0}MSE(\theta) \    \frac{\partial}{\partial \theta_1}MSE(\theta) \    … \    \frac{\partial}{\partial \theta_n}MSE(\theta)   \end{matrix}   \right] = \frac{2}{m}X^T \cdot (X \cdot \theta - y)<br>$$</p><blockquote><p><strong>注意：</strong> 公式4-6在计算梯度下降的每一步时，都是基于完整的训练集 $X$ 的。这就是为什么该算法会被称为批量梯度下降：每一步都使用整批训练数据。因此，面对非常庞大的训练集时，算法会变得极慢（不过我们即将看到快得多的梯度下降算法）。但是，梯度下降算法随特征数量扩展的表现比较好：如果要训练的线性模型拥有几十万个特征，使用梯度下降比标准方程要快得多。</p></blockquote><p>一旦有了梯度向量，哪个点向上，就朝反方向下坡。也就是从 $θ$ 中减去 $\nabla_\theta MSE(\theta)$ 。这时学习率 $η$ 就发挥作用了：用梯度向量乘以 $η$ 确定下坡步长的大小（公式4-7）。</p><p><strong>公式 4-6 ：梯度下降步长</strong></p><p>$$<br>\theta^{(next  step)} = \theta - \eta\nabla_\theta MSE(\theta)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.1</span>  <span class="comment"># learning rate </span></span><br><span class="line">n_iterations = <span class="number">1000</span> </span><br><span class="line">m = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(n_iterations):    </span><br><span class="line">    gradients = <span class="number">2</span>/m * X_b.T.dot(X_b.dot(theta) - y)    </span><br><span class="line">    theta = theta - eta * gradients</span><br><span class="line">    </span><br><span class="line">theta</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[4.0939709 ],</span><br><span class="line">       [3.08934507]]) </span><br></pre></td></tr></table></figure><p>这不正是标准方程的发现么！梯度下降表现完美。如果使用了其他的学习率 $η$ 呢？图4-8展现了分别使用三种不同的学习率时，梯度下降的前十步（虚线表示起点）。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_05.jpg"></center><p>左图的学习率太低：算法最终还是能找到解决方法，就是需要太长时间。中间的学习率看起来非常棒：几次迭代就收敛出了最终解。而右边的学习率太高：算法发散，直接跳过了数据区域，并且每一步都离实际解决方案越来越远。</p><p>要找到合适的学习率，可以使用网格搜索。但是你可能需要限制迭代次数，这样网格搜索可以淘汰掉那些收敛耗时太长的模型。</p><p>你可能会问，要怎么限制迭代次数呢？如果设置太低，算法可能在离最优解还很远时就停了；但是如果设置得太高，模型达到最优解后，继续迭代参数不再变化，又会浪费时间。一个简单的办法是，在开始时设置一个非常大的迭代次数，但是当梯度向量的值变得很微小时中断算法——也就是当它的范数变得低于 $ε$（称为容差）时，因为这时梯度下降已经（几乎）到达了最小值。</p><blockquote><p><strong>收敛率</strong></p><p>成本函数为凸函数，并且斜率没有陡峭的变化时（如 $MSE$ 成本函数），通过批量梯度下降可以看出一个固定的学习率有一个收敛率，为 $0(\dfrac{1}{迭代次数})$。换句话说，如果将容差 $ε$ 缩小为原来的 $\dfrac{1}{10}$（以得到更精确的解），算法将不得不运行10倍的迭代次数</p></blockquote><h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>批量梯度下降的主要问题是它要用整个训练集来计算每一步的梯度，所以训练集很大时，算法会特别慢。与之相反的极端是随机梯度下降，每一步在训练集中随机选择一个实例，并且仅基于该单个实例来计算梯度。显然，这让算法变得快多了，因为每个迭代都只需要操作少量的数据。它也可以被用来训练海量的数据集，因为每次迭代只需要在内存中运行一个实例即可（$SGD$ 可以作为核外算法实现）。</p><p>另一方面，由于算法的随机性质，它比批量梯度下降要不规则得多。成本函数将不再是缓缓降低直到抵达最小值，而是不断上上下下，但是从整体来看，还是在慢慢下降。随着时间推移，最终会非常接近最小值，但是即使它到达了最小值，依旧还会持续反弹，永远不会停止（见图4-9）。所以算法停下来的参数值肯定是足够好的，但不是最优的。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_06.jpg"></center><p>当成本函数非常不规则时（见图4-6），随机梯度下降其实可以帮助算法跳出局部最小值，所以相比批量梯度下降，它对找到全局最小值更有优势。</p><p>因此，随机性的好处在于可以逃离局部最优，但缺点是永远定位不出最小值。要解决这个困境，有一个办法是逐步降低学习率。开始的步长比较大（这有助于快速进展和逃离局部最小值），然后越来越小，让算法尽量靠近全局最小值。这个过程叫作<strong>模拟退火</strong>，因为它类似于冶金时熔化的金属慢慢冷却的退火过程。确定每个迭代学习率的函数叫作<strong>学习计划</strong>。如果学习率降得太快，可能会陷入局部最小值，甚至是停留在走向最小值的半途中。如果学习率降得太慢，你需要太长时间才能跳到差不多最小值附近，如果提早结束训练，可能只得到一个次优的解决方案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">50</span> </span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">50</span>  <span class="comment"># learning schedule hyperparameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_schedule</span>(<span class="params">t</span>):</span>    </span><br><span class="line">    <span class="keyword">return</span> t0 / (t + t1)</span><br><span class="line"></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):        </span><br><span class="line">        random_index = np.random.randint(m)        </span><br><span class="line">        xi = X_b[random_index:random_index+<span class="number">1</span>]        </span><br><span class="line">        yi = y[random_index:random_index+<span class="number">1</span>]        </span><br><span class="line">        gradients = <span class="number">2</span> * xi.T.dot(xi.dot(theta) - yi)        </span><br><span class="line">        eta = learning_schedule(epoch * m + i)        </span><br><span class="line">        theta = theta - eta * gradients</span><br></pre></td></tr></table></figure><p>按照惯例，我们用 $m$ 来表示迭代次数，每一次迭代称为一轮。前面的批量梯度下降需要在整个训练集上迭代 $1000$次，而这段代码只迭代了 $50$ 次就得到了一个相当不错的解：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta  </span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[4.11135275],</span><br><span class="line">       [3.06756448]]) </span><br></pre></td></tr></table></figure><p>图 4-10 显示了训练过程的前 10 步 (注意不规则的步子)</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_07.jpg"></center><p>因为实例是随机挑选，所以在同一轮里某些实例可能被挑选多次，而有些实例则完全没被选到。如果你希望每一轮算法都能遍历每个实例，有一种办法是将训练集洗牌打乱，然后一个接一个的使用实例，用完再重新洗牌，以此继续。不过这种方法通常收敛得更慢。</p><p>在 <code>Scikit-Learn</code> 里，用 $SGD$ 执行线性回归可以使用 <code>SGDRegressor</code> 类，其默认优化的成本函数是平方误差。下面这段代码从学习率 0.1 开始（<code>eta0=0.1</code>），使用默认的学习计划（跟前面的学习计划不同） 运行了50 轮，而且没有使用任何正则化（<code>penalty=None</code>）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor </span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegressor(n_iter=<span class="number">50</span>, penalty=<span class="literal">None</span>, eta0=<span class="number">0.1</span>) </span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SGDRegressor(alpha&#x3D;0.0001, average&#x3D;False, early_stopping&#x3D;False, epsilon&#x3D;0.1,</span><br><span class="line">       eta0&#x3D;0.1, fit_intercept&#x3D;True, l1_ratio&#x3D;0.15,</span><br><span class="line">       learning_rate&#x3D;&#39;invscaling&#39;, loss&#x3D;&#39;squared_loss&#39;, max_iter&#x3D;None,</span><br><span class="line">       n_iter&#x3D;50, n_iter_no_change&#x3D;5, penalty&#x3D;None, power_t&#x3D;0.25,</span><br><span class="line">       random_state&#x3D;None, shuffle&#x3D;True, tol&#x3D;None, validation_fraction&#x3D;0.1,</span><br><span class="line">       verbose&#x3D;0, warm_start&#x3D;False) </span><br></pre></td></tr></table></figure><p>你再次得到了一个跟标准方程的解非常相近的解决方案：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sgd_reg.intercept_, sgd_reg.coef_</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([4.08805401]), array([3.08242337])) </span><br></pre></td></tr></table></figure><h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>我们要了解的最后一个梯度下降算法叫作<strong>小批量梯度下降</strong>。一旦理解了批量梯度下降和随机梯度下降，这个算法就非常容易理解了：每一步的梯度计算，既不是基于整个训练集（如批量梯度下降）也不是基于单个实例（如随机梯度下降），而是基于一小部分随机的实例集也就是小批量。相比随机梯度下降，小批量梯度下降的主要优势在于可以从矩阵运算的硬件优化中获得显著的性能提升，特别是需要用到图形处理器时。</p><p>这个算法在参数空间层面的前进过程也不像 $SGD$ 那样不稳定，特别是批量较大时。所以小批量梯度下降最终会比 $SGD$ 更接近最小值一些。但是另一方面，它可能更难从局部最小值中逃脱（不是我们前面看到的线性回归问题，而是对于那些深受局部最小值陷阱困扰的问题）。图 4-11 显示了三种梯度下降算法在训练过程中参数空间里的行进路线。它们最终都汇聚在最小值附近，批量梯度下降最终停在了最小值上，而随机梯度下降和小批量梯度下降还在继续游走。但是，别忘了批量梯度可是花费了大量时间来计算每一步的，如果用好了学习计划，随机梯度下降和小批量梯度下降也同样能到达最小值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/GD_08.jpg"></center><p>最后，我们来比较一下到目前为止所讨论过的线性回归算法 (m$m$ 是训练实例的数量，n$n$ 是特征数量)。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/excel.jpg"></center><p>_参考_：</p><ul><li><a href="http://shop.oreilly.com/product/0636920052289.do">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在了解&lt;strong&gt;梯度下降（Gradient Descent）&lt;/strong&gt;之前，我们先要知道有关&lt;strong&gt;线性回归&lt;/strong&gt;的基本知识，这样可以进一步的加深对&lt;strong&gt;梯度下降&lt;/strong&gt;的理解，当然&lt;strong&gt;梯度下降（Gradient Descent）&lt;/strong&gt;并不单单只能进行回归预测，它还可以进行诸如分类等操作。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="梯度下降" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="GD" scheme="http://www.laugh12321.cn/blog/tags/GD/"/>
    
    <category term="SGD" scheme="http://www.laugh12321.cn/blog/tags/SGD/"/>
    
    <category term="MBGD" scheme="http://www.laugh12321.cn/blog/tags/MBGD/"/>
    
    <category term="BGD" scheme="http://www.laugh12321.cn/blog/tags/BGD/"/>
    
  </entry>
  
  <entry>
    <title>译|Gradient Descent in Python</title>
    <link href="http://www.laugh12321.cn/blog/2019/03/09/gradient_descent/"/>
    <id>http://www.laugh12321.cn/blog/2019/03/09/gradient_descent/</id>
    <published>2019-03-09T00:00:00.000Z</published>
    <updated>2020-10-23T08:47:45.933Z</updated>
    
    <content type="html"><![CDATA[<p>当你初次涉足机器学习时，你学习的第一个基本算法就是 <strong>梯度下降 (Gradient Descent)</strong>, 可以说<strong>梯度下降法</strong>是机器学习算法的支柱。 在这篇文章中，我尝试使用 python$python$ 解释<strong>梯度下降法</strong>的基本原理。一旦掌握了<strong>梯度下降法</strong>，很多问题就会变得容易理解，并且利于理解不同的算法。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use([<span class="string">&#x27;ggplot&#x27;</span>])</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p>如果你想尝试自己实现<strong>梯度下降法</strong>， 你需要加载基本的 p$python$ $packages$ —— $numpy$ and $matplotlib$  </p><p>首先， 我们将创建包含着噪声的线性数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机创建一些噪声</span></span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>接下来通过 <code>matplotlib</code> 可视化数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.plot(X, y, <span class="string">&#x27;b.&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;$x$&quot;</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;$y$&quot;</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])</span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_7_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_7_1.png"></a></p><p>显然， y$y$ 与 x$x$ 具有良好的线性关系，这个数据非常简单，只有一个自变量 x$x$.</p><p>我们可以将其表示为简单的线性关系：</p><p>y=b+mx</p><p>$$y=b+mx$$</p><p>并求出 b$b$ , m$m$。</p><p>这种被称为解方程的分析方法并没有什么不妥，但机器学习是涉及矩阵计算的，因此我们使用矩阵法（向量法）进行分析。</p><p>我们将 y$y$ 替换成 J(θ)$J(θ)$， b$b$ 替换成 θ0$θ0$， m$m$ 替换成 θ1$θ1$。<br>得到如下表达式：</p><p>J(θ)=θ0+θ1x</p><p>$$J(θ)=θ0+θ1x$$</p><p><strong>注意：</strong> 本例中 θ0=4$θ0=4$， θ1=3$θ1=3$</p><p>求解 θ0$θ0$ 和 θ1$θ1$ 的分析方法，代码如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X\_b &#x3D; np.c\_\[np.ones((100, 1)), X\] # 为X添加了一个偏置单位，对于X中的每个向量都是1  </span><br><span class="line">theta\_best &#x3D; np.linalg.inv(X\_b.T.dot(X\_b)).dot(X\_b.T).dot(y)  </span><br><span class="line">theta\_best  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[3.86687149],</span><br><span class="line">       [3.12408839]]) </span><br></pre></td></tr></table></figure><p>不难发现这个值接近真实的 θ0$θ0$，θ1$θ1$，由于我在数据中引入了噪声，所以存在误差。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X\_new &#x3D; np.array(\[\[0\], \[2\]\])  </span><br><span class="line">X\_new\_b &#x3D; np.c\_\[np.ones((2, 1)), X\_new\]  </span><br><span class="line">y\_predict &#x3D; X\_new\_b.dot(theta\_best)  </span><br><span class="line">y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[ 3.86687149],</span><br><span class="line">       [10.11504826]]) </span><br></pre></td></tr></table></figure><h2 id="梯度下降法-（Gradient-Descent）"><a href="#梯度下降法-（Gradient-Descent）" class="headerlink" title="梯度下降法 （Gradient Descent）"></a><a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%EF%BC%88Gradient-Descent%EF%BC%89" title="梯度下降法 （Gradient Descent）"></a>梯度下降法 （Gradient Descent）</h2><h3 id="Cost-Function-amp-Gradients"><a href="#Cost-Function-amp-Gradients" class="headerlink" title="Cost Function &amp; Gradients"></a><a href="#Cost-Function-amp-Gradients" title="Cost Function &amp; Gradients"></a>Cost Function &amp; Gradients</h3><p>计算代价函数和梯度的公式如下所示。</p><p><strong>注意：</strong>代价函数用于线性回归，对于其他算法，代价函数是不同的，梯度必须从代价函数中推导出来。</p><p><strong>Cost</strong></p><p>J(θ)=1/2mm∑i=1(h(θ)(i)−y(i))2</p><p>$$J(θ)=1/2m∑i=1m(h(θ)(i)−y(i))2$$</p><p><strong>Gradient</strong></p><p>∂J(θ)∂θj=1/mm∑i=1(h(θ(i)−y(i)).X(i)j</p><p>$$∂J(θ)∂θj=1/m∑i=1m(h(θ(i)−y(i)).Xj(i)$$</p><p><strong>Gradients</strong></p><p>θ0:=θ0−α.(1/m.m∑i=1(h(θ(i)−y(i)).X(i)0)</p><p>$$θ0:=θ0−α.(1/m.∑i=1m(h(θ(i)−y(i)).X0(i))$$</p><p>θ1:=θ1−α.(1/m.m∑i=1(h(θ(i)−y(i)).X(i)1)</p><p>$$θ1:=θ1−α.(1/m.∑i=1m(h(θ(i)−y(i)).X1(i))$$</p><p>θ2:=θ2−α.(1/m.m∑i=1(h(θ(i)−y(i)).X(i)2)</p><p>$$θ2:=θ2−α.(1/m.∑i=1m(h(θ(i)−y(i)).X2(i))$$</p><p>θj:=θj−α.(1/m.m∑i=1(h(θ(i)−y(i)).X(i)0)</p><p>$$θj:=θj−α.(1/m.∑i=1m(h(θ(i)−y(i)).X0(i))$$</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def cal\_cost(theta, X, y):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    Calculates the cost for given X and Y. The following shows and example of a single dimensional X  </span><br><span class="line">    theta &#x3D; Vector of thetas   </span><br><span class="line">    X     &#x3D; Row of X&#39;s np.zeros((2,j))  </span><br><span class="line">    y     &#x3D; Actual y&#39;s np.zeros((2,1))  </span><br><span class="line">      </span><br><span class="line">    where:  </span><br><span class="line">        j is the no of features  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    m &#x3D; len(y)  </span><br><span class="line">      </span><br><span class="line">    predictions &#x3D; X.dot(theta)  </span><br><span class="line">    cost &#x3D; (1&#x2F;2\*m) \* np.sum(np.square(predictions - y))  </span><br><span class="line">      </span><br><span class="line">    return cost  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def gradient\_descent(X, y, theta, learning\_rate &#x3D; 0.01, iterations &#x3D; 100):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    X    &#x3D; Matrix of X with added bias units  </span><br><span class="line">    y    &#x3D; Vector of Y  </span><br><span class="line">    theta&#x3D;Vector of thetas np.random.randn(j,1)  </span><br><span class="line">    learning\_rate   </span><br><span class="line">    iterations &#x3D; no of iterations  </span><br><span class="line">      </span><br><span class="line">    Returns the final theta vector and array of cost history over no of iterations      </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    m &#x3D; len(y)  </span><br><span class="line">    # learning\_rate &#x3D; 0.01  </span><br><span class="line">    # iterations &#x3D; 100  </span><br><span class="line">      </span><br><span class="line">    cost\_history &#x3D; np.zeros(iterations)  </span><br><span class="line">    theta\_history &#x3D; np.zeros((iterations, 2))  </span><br><span class="line">    for i in range(iterations):  </span><br><span class="line">        prediction &#x3D; np.dot(X, theta)  </span><br><span class="line">          </span><br><span class="line">        theta &#x3D; theta - (1&#x2F;m) \* learning\_rate \* (X.T.dot((prediction - y)))  </span><br><span class="line">        theta\_history\[i, :\] &#x3D; theta.T  </span><br><span class="line">        cost\_history\[i\] &#x3D; cal\_cost(theta, X, y)  </span><br><span class="line">          </span><br><span class="line">    return theta, cost\_history, theta\_history  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">\# 从1000次迭代开始，学习率为0.01。从高斯分布的θ开始  </span><br><span class="line">lr &#x3D;0.01  </span><br><span class="line">n\_iter &#x3D; 1000  </span><br><span class="line">theta &#x3D; np.random.randn(2, 1)  </span><br><span class="line">X\_b &#x3D; np.c\_\[np.ones((len(X), 1)), X\]  </span><br><span class="line">theta, cost\_history, theta\_history &#x3D; gradient\_descent(X\_b, y, theta, lr, n\_iter)  </span><br><span class="line">  </span><br><span class="line">print(&#39;Theta0:          &#123;:0.3f&#125;,\\nTheta1:          &#123;:0.3f&#125;&#39;.format(theta\[0\]\[0\],theta\[1\]\[0\]))  </span><br><span class="line">print(&#39;Final cost&#x2F;MSE:  &#123;:0.3f&#125;&#39;.format(cost\_history\[-1\]))  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta0:          3.867,</span><br><span class="line">Theta1:          3.124</span><br><span class="line">Final cost&#x2F;MSE:  5457.747 </span><br></pre></td></tr></table></figure><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\# 绘制迭代的成本图  </span><br><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(12,8))  </span><br><span class="line">  </span><br><span class="line">ax.set\_ylabel(&#39;J(Theta)&#39;)  </span><br><span class="line">ax.set\_xlabel(&#39;Iterations&#39;)  </span><br><span class="line">ax.plot(range(1000), cost\_history, &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_27_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_27_1.png"></a></p><p>在大约 <code>150</code> 次迭代之后代价函数趋于稳定，因此放大到迭代200，看看曲线</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(10,8))  </span><br><span class="line">ax.plot(range(200), cost\_history\[:200\], &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_29_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_29_1.png"></a></p><p>值得注意的是，最初成本下降得更快，然后成本降低的收益就不那么多了。 我们可以尝试使用不同的学习速率和迭代组合，并得到不同学习率和迭代的效果会如何。</p><p>让我们建立一个函数，它可以显示效果，也可以显示梯度下降实际上是如何工作的。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def plot\_GD(n\_iter, lr, ax, ax1&#x3D;None):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    n\_iter &#x3D; no of iterations  </span><br><span class="line">    lr &#x3D; Learning Rate  </span><br><span class="line">    ax &#x3D; Axis to plot the Gradient Descent  </span><br><span class="line">    ax1 &#x3D; Axis to plot cost\_history vs Iterations plot  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    ax.plot(X, y, &#39;b.&#39;)  </span><br><span class="line">    theta &#x3D; np.random.randn(2, 1)  </span><br><span class="line">      </span><br><span class="line">    tr &#x3D; 0.1  </span><br><span class="line">    cost\_history &#x3D; np.zeros(n\_iter)  </span><br><span class="line">    for i in range(n\_iter):  </span><br><span class="line">        pred\_prev &#x3D; X\_b.dot(theta)  </span><br><span class="line">        theta, h, \_ &#x3D; gradient\_descent(X\_b, y, theta, lr, 1)  </span><br><span class="line">        pred &#x3D; X\_b.dot(theta)  </span><br><span class="line">          </span><br><span class="line">        cost\_history\[i\] &#x3D; h\[0\]  </span><br><span class="line">          </span><br><span class="line">        if ((i % 25 &#x3D;&#x3D; 0)):  </span><br><span class="line">            ax.plot(X, pred, &#39;r-&#39;, alpha&#x3D;tr)  </span><br><span class="line">            if tr &lt; 0.8:  </span><br><span class="line">                tr +&#x3D; 0.2  </span><br><span class="line">      </span><br><span class="line">    if not ax1 &#x3D;&#x3D; None:  </span><br><span class="line">        ax1.plot(range(n\_iter), cost\_history, &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">\# 绘制不同迭代和学习率组合的图  </span><br><span class="line">fig &#x3D; plt.figure(figsize&#x3D;(30,25), dpi&#x3D;200)  </span><br><span class="line">fig.subplots\_adjust(hspace&#x3D;0.4, wspace&#x3D;0.4)  </span><br><span class="line">  </span><br><span class="line">it\_lr &#x3D; \[(2000, 0.001), (500, 0.01), (200, 0.05), (100, 0.1)\]  </span><br><span class="line">count &#x3D; 0  </span><br><span class="line">for n\_iter, lr in it\_lr:  </span><br><span class="line">    count +&#x3D; 1  </span><br><span class="line">      </span><br><span class="line">    ax &#x3D; fig.add\_subplot(4, 2, count)  </span><br><span class="line">    count +&#x3D; 1  </span><br><span class="line">     </span><br><span class="line">    ax1 &#x3D; fig.add\_subplot(4, 2, count)  </span><br><span class="line">      </span><br><span class="line">    ax.set\_title(&quot;lr:&#123;&#125;&quot; .format(lr))  </span><br><span class="line">    ax1.set\_title(&quot;Iterations:&#123;&#125;&quot; .format(n\_iter))  </span><br><span class="line">    plot\_GD(n\_iter, lr, ax, ax1)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_32_0.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_32_0.png"></a></p><p>通过观察发现，以较小的学习速率收集解决方案需要很长时间，而学习速度越大，学习速度越快。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\_, ax &#x3D; plt.subplots(figsize&#x3D;(14, 10))  </span><br><span class="line">plot\_GD(100, 0.1, ax)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_34_0.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_34_0.png"></a></p><h2 id="随机梯度下降法（Stochastic-Gradient-Descent）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent）"></a><a href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Stochastic-Gradient-Descent%EF%BC%89" title="随机梯度下降法（Stochastic Gradient Descent）"></a>随机梯度下降法（Stochastic Gradient Descent）</h2><p>随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的 m$m$ 个样本的数据，而是仅仅选取一个样本 j$j$ 来求梯度。对应的更新公式是：</p><p>θi=θi−α(hθ(x(j)0,x(j)1,…x(j)n)−yj)x(j)i</p><p>$$θi=θi−α(hθ(x0(j),x1(j),…xn(j))−yj)xi(j)$$</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def stocashtic\_gradient\_descent(X, y, theta, learning\_rate&#x3D;0.01, iterations&#x3D;10):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    X    &#x3D; Matrix of X with added bias units  </span><br><span class="line">    y    &#x3D; Vector of Y  </span><br><span class="line">    theta&#x3D;Vector of thetas np.random.randn(j,1)  </span><br><span class="line">    learning\_rate   </span><br><span class="line">    iterations &#x3D; no of iterations  </span><br><span class="line">      </span><br><span class="line">    Returns the final theta vector and array of cost history over no of iterations  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    m &#x3D; len(y)  </span><br><span class="line">    cost\_history &#x3D; np.zeros(iterations)  </span><br><span class="line">      </span><br><span class="line">    for it in range(iterations):  </span><br><span class="line">        cost &#x3D; 0.0  </span><br><span class="line">        for i in range(m):  </span><br><span class="line">            rand\_ind &#x3D; np.random.randint(0, m)  </span><br><span class="line">            X\_i &#x3D; X\[rand\_ind, :\].reshape(1, X.shape\[1\])  </span><br><span class="line">            y\_i &#x3D; y\[rand\_ind, :\].reshape(1, 1)  </span><br><span class="line">            prediction &#x3D; np.dot(X\_i, theta)  </span><br><span class="line">              </span><br><span class="line">            theta -&#x3D; (1&#x2F;m) \* learning\_rate \* (X\_i.T.dot((prediction - y\_i)))  </span><br><span class="line">            cost +&#x3D; cal\_cost(theta, X\_i, y\_i)  </span><br><span class="line">        cost\_history\[it\] &#x3D; cost  </span><br><span class="line">          </span><br><span class="line">    return theta, cost\_history  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">lr &#x3D; 0.5  </span><br><span class="line">n\_iter &#x3D; 50  </span><br><span class="line">theta &#x3D; np.random.randn(2,1)  </span><br><span class="line">X\_b &#x3D; np.c\_\[np.ones((len(X),1)), X\]  </span><br><span class="line">theta, cost\_history &#x3D; stocashtic\_gradient\_descent(X\_b, y, theta, lr, n\_iter)  </span><br><span class="line">  </span><br><span class="line">print(&#39;Theta0:          &#123;:0.3f&#125;,\\nTheta1:          &#123;:0.3f&#125;&#39; .format(theta\[0\]\[0\],theta\[1\]\[0\]))  </span><br><span class="line">print(&#39;Final cost&#x2F;MSE:  &#123;:0.3f&#125;&#39; .format(cost\_history\[-1\]))  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta0:          3.762,</span><br><span class="line">Theta1:          3.159</span><br><span class="line">Final cost&#x2F;MSE:  46.964 </span><br></pre></td></tr></table></figure><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(10,8))  </span><br><span class="line">  </span><br><span class="line">ax.set\_ylabel(&#39;$J(\\Theta)$&#39; ,rotation&#x3D;0)  </span><br><span class="line">ax.set\_xlabel(&#39;$Iterations$&#39;)  </span><br><span class="line">theta &#x3D; np.random.randn(2,1)  </span><br><span class="line">  </span><br><span class="line">ax.plot(range(n\_iter), cost\_history, &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_39_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_39_1.png"></a></p><h2 id="小批量梯度下降法（Mini-batch-Gradient-Descent）"><a href="#小批量梯度下降法（Mini-batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a><a href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Mini-batch-Gradient-Descent%EF%BC%89" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a>小批量梯度下降法（Mini-batch Gradient Descent）</h2><p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于 m$m$ 个样本，我们采用 x$x$ 个样子来迭代，1&lt;x&lt;m$1&lt;x&lt;m$。一般可以取 x=10$x=10$，当然根据样本的数据，可以调整这个 x$x$ 的值。对应的更新公式是：</p><p>θi=θi−αt+x−1∑j=t(hθ(x(j)0,x(j)1,…x(j)n)−yj)x(j)i</p><p>$$θi=θi−α∑j=tt+x−1(hθ(x0(j),x1(j),…xn(j))−yj)xi(j)$$</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def minibatch\_gradient\_descent(X, y, theta, learning\_rate&#x3D;0.01, iterations&#x3D;10, batch\_size&#x3D;20):  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">    X    &#x3D; Matrix of X without added bias units  </span><br><span class="line">    y    &#x3D; Vector of Y  </span><br><span class="line">    theta&#x3D;Vector of thetas np.random.randn(j,1)  </span><br><span class="line">    learning\_rate   </span><br><span class="line">    iterations &#x3D; no of iterations  </span><br><span class="line">      </span><br><span class="line">    Returns the final theta vector and array of cost history over no of iterations  </span><br><span class="line">    &#39;&#39;&#39;  </span><br><span class="line">      </span><br><span class="line">    m &#x3D; len(y)  </span><br><span class="line">    cost\_history &#x3D; np.zeros(iterations)  </span><br><span class="line">    n\_batches &#x3D; int(m &#x2F; batch\_size)  </span><br><span class="line">      </span><br><span class="line">    for it in range(iterations):  </span><br><span class="line">        cost &#x3D; 0.0  </span><br><span class="line">        indices &#x3D; np.random.permutation(m)  </span><br><span class="line">        X &#x3D; X\[indices\]  </span><br><span class="line">        y &#x3D; y\[indices\]  </span><br><span class="line">        for i in range(0, m, batch\_size):  </span><br><span class="line">            X\_i &#x3D; X\[i: i+batch\_size\]  </span><br><span class="line">            y\_i &#x3D; y\[i: i+batch\_size\]  </span><br><span class="line">              </span><br><span class="line">            X\_i &#x3D; np.c\_\[np.ones(len(X\_i)), X\_i\]  </span><br><span class="line">            prediction &#x3D; np.dot(X\_i, theta)  </span><br><span class="line">              </span><br><span class="line">            theta -&#x3D; (1&#x2F;m) \* learning\_rate \* (X\_i.T.dot((prediction - y\_i)))  </span><br><span class="line">            cost +&#x3D; cal\_cost(theta, X\_i, y\_i)  </span><br><span class="line">        cost\_history\[it\] &#x3D; cost  </span><br><span class="line">      </span><br><span class="line">    return theta, cost\_history  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lr &#x3D; 0.1  </span><br><span class="line">n\_iter &#x3D; 200  </span><br><span class="line">theta &#x3D; np.random.randn(2, 1)  </span><br><span class="line">theta, cost\_history &#x3D; minibatch\_gradient\_descent(X, y, theta, lr, n\_iter)  </span><br><span class="line">  </span><br><span class="line">print(&#39;Theta0:          &#123;:0.3f&#125;,\\nTheta1:          &#123;:0.3f&#125;&#39; .format(theta\[0\]\[0\], theta\[1\]\[0\]))  </span><br><span class="line">print(&#39;Final cost&#x2F;MSE:  &#123;:0.3f&#125;&#39; .format(cost\_history\[-1\]))  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta0:          3.842,</span><br><span class="line">Theta1:          3.146</span><br><span class="line">Final cost&#x2F;MSE:  1090.518 </span><br></pre></td></tr></table></figure><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(10,8))  </span><br><span class="line">  </span><br><span class="line">ax.set\_ylabel(&#39;$J(\\Theta)$&#39;, rotation&#x3D;0)  </span><br><span class="line">ax.set\_xlabel(&#39;$Iterations$&#39;)  </span><br><span class="line">theta &#x3D; np.random.randn(2, 1)  </span><br><span class="line">  </span><br><span class="line">ax.plot(range(n\_iter), cost\_history, &#39;b.&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_44_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Gradient%20Descent/output_44_1.png"></a></p><p>_参考_：</p><ul><li><a href="https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f">Gradient Descent in Python</a></li><li><a href="https://www.cnblogs.com/pinard/p/5970503.html">梯度下降（Gradient Descent）小结</a></li></ul><p># <a href="/tags/Gradient-Descent/">Gradient Descent</a>, <a href="/tags/machine-learning/">machine learning</a>, <a href="/tags/python/">python</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;当你初次涉足机器学习时，你学习的第一个基本算法就是 &lt;strong&gt;梯度下降 (Gradient Descent)&lt;/strong&gt;, 可以说&lt;strong&gt;梯度下降法&lt;/strong&gt;是机器学习算法的支柱。 在这篇文章中，我尝试使用 python$python$ 解释&lt;strong&gt;梯度下降法&lt;/strong&gt;的基本原理。一旦掌握了&lt;strong&gt;梯度下降法&lt;/strong&gt;，很多问题就会变得容易理解，并且利于理解不同的算法。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="梯度下降" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="GD" scheme="http://www.laugh12321.cn/blog/tags/GD/"/>
    
    <category term="SGD" scheme="http://www.laugh12321.cn/blog/tags/SGD/"/>
    
    <category term="MBGD" scheme="http://www.laugh12321.cn/blog/tags/MBGD/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|层次聚类方法</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/11/hierarchical_clustering_method/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/11/hierarchical_clustering_method/</id>
    <published>2019-02-11T00:00:00.000Z</published>
    <updated>2020-10-23T08:50:34.684Z</updated>
    
    <content type="html"><![CDATA[<p>在之前的文章中，我们学习了划分聚类方法，并着重介绍了其中的 K-Means 算法。K-Means 算法可以说是用处非常广泛的聚类算法之一，它非常好用。但是，当你使用过这种算法之后，你就会发现一个比较让人「头疼」的问题，那就是我们需要手动指定 K 值，也就是聚类的类别数量。</p><a id="more"></a><p>预先确定聚类的类别数量看起来是个小事情，但是在很多时候是比较麻烦的，因为我们可能在聚类前并不知道数据集到底要被聚成几类。例如，下面的示意图中，感觉聚成 2 类或者 4 类都是比较合理的。  </p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/1.png"></a></p><p>今天要学习的层次聚类方法与划分聚类最大的区别之一，就是我们<strong>无需提前指定需要聚类类别的数量</strong>。这听起来是非常诱人的，到底是怎样做到的呢？</p><p>简单来讲，层次聚类方法的特点在于通过计算数据集元素间的相似度来生成一颗具备层次结构的树。首先在这里强调一点，这里的「层次树」和之前学习的「决策树」是完全不同，不要混淆。</p><p>与此同时，当我们使用层次聚类方法时，可以根据创建树的方式分为两种情况：</p><ul><li><p><strong>自底向上层次聚类法</strong>：该方法的过程被称为「凝聚」(Agglomerative)，也就是把数据集中的每个元素看作是一个类别，然后进行迭代合并成为更大的类别，直到满足某个终止条件。</p></li><li><p><strong>自顶向下层次聚类法</strong>：该方法的过程被称为「分裂」(Divisive)，也就是凝聚的反向过程。首先，把数据集看作是一个类别，然后递归地划分为多个较小的子类，直到满足某个终止条件。</p></li></ul><h2 id="自底向上层次聚类法"><a href="#自底向上层次聚类法" class="headerlink" title="自底向上层次聚类法"></a><a href="#%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E6%B3%95" title="自底向上层次聚类法"></a>自底向上层次聚类法</h2><p>自底向上层次聚类法也就是 Agglomerative Clustering 算法。这种算法的主要特点在于，我们使用「自底向上」进行聚类的思路来帮助距离相近的样本被放在同一类别中。</p><h3 id="自底向上层次聚类流程"><a href="#自底向上层次聚类流程" class="headerlink" title="自底向上层次聚类流程"></a><a href="#%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E6%B5%81%E7%A8%8B" title="自底向上层次聚类流程"></a>自底向上层次聚类流程</h3><p>具体来讲，这种方法的主要步骤如下：</p><p>对于数据集 D$D$，D=(x1,x2,⋯,xn)$D=(x1,x2,⋯,xn)$：</p><ol><li>将数据集中每个样本标记为 <code>1</code> 类，即 $D$ 初始时包含的类别（Class）为 C$C$，C=(c1,c2,⋯,cn)$C=(c1,c2,⋯,cn)$。</li><li>计算并找出 C$C$ 中距离最近的 <code>2</code> 个类别，合并为 <code>1</code> 类。</li><li>依次合并直到最后仅剩下一个列表，即建立起一颗完整的层次树。</li></ol><p>我们通过下图来演示自底向上层次聚类法的过程，首先平面上有 5 个样本点，我们将每个样本点都单独划为 1 类。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/2.png"></a></p><p>接下来，我们可以计算元素间的距离，并将距离最近的合并为 1 类。于是，总类别变为 3 类。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/3.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/3.png"></a></p><p>重复上面的步骤，总类别变为 2 类。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/4.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/4.png"></a></p><p>最后，合并为 1 类，聚类终止。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/5.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/5.png"></a></p><p>我们将上面的聚类过程变为层次树就为：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/6.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/6.png"></a></p><p>看完上面的演示过程，你会发现「自底向上层次聚类法」原来这么简单呀！</p><h3 id="距离计算方法"><a href="#距离计算方法" class="headerlink" title="距离计算方法"></a><a href="#%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95" title="距离计算方法"></a>距离计算方法</h3><p>虽然聚类过程看似简单，但不知道你是否意识到一个问题：<strong>当一个类别中包含多个元素时，类别与类别之间的距离是怎样确定的呢？</strong></p><p>也就是说，上面的演示过程中，为什么要把 5$5$ 归为 &lt;3,4&gt;$&lt;3,4&gt;$ 组成的那一类，而不是 &lt;1,2&gt;$&lt;1,2&gt;$ 组成的类呢？或者说，为什么不先把 &lt;1,2&gt;$&lt;1,2&gt;$ 与 &lt;3,4&gt;$&lt;3,4&gt;$ 合并，最后才合并 5$5$ 呢？</p><p>这就涉及到 Agglomerative 聚类过程中的距离计算方式。简单来讲，我们一般有 3 种不同的距离计算方式：</p><h4 id="单连接（Single-linkage）"><a href="#单连接（Single-linkage）" class="headerlink" title="单连接（Single-linkage）"></a><a href="#%E5%8D%95%E8%BF%9E%E6%8E%A5%EF%BC%88Single-linkage%EF%BC%89" title="单连接（Single-linkage）"></a>单连接（Single-linkage）</h4><p>单连接的计算方式是根据两种类别之间<strong>最近</strong>的元素间距离作为两类别之间的距离。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/7.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/7.png"></a></p><h4 id="全连接（Complete-linkage）"><a href="#全连接（Complete-linkage）" class="headerlink" title="全连接（Complete-linkage）"></a><a href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%EF%BC%88Complete-linkage%EF%BC%89" title="全连接（Complete-linkage）"></a>全连接（Complete-linkage）</h4><p>全连接的计算方式是根据两种类别之间<strong>最远</strong>的元素间距离作为两类别之间的距离。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/8.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/8.png"></a></p><h4 id="平均连接（Average-linkage）"><a href="#平均连接（Average-linkage）" class="headerlink" title="平均连接（Average-linkage）"></a><a href="#%E5%B9%B3%E5%9D%87%E8%BF%9E%E6%8E%A5%EF%BC%88Average-linkage%EF%BC%89" title="平均连接（Average-linkage）"></a>平均连接（Average-linkage）</h4><p>平均连接的计算方式是依次计算两种类别之间两两元素间距离，并最终求得平均值作为两类别之间的距离。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/9.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/9.png"></a></p><h4 id="中心连接（Center-linkage）"><a href="#中心连接（Center-linkage）" class="headerlink" title="中心连接（Center-linkage）"></a><a href="#%E4%B8%AD%E5%BF%83%E8%BF%9E%E6%8E%A5%EF%BC%88Center-linkage%EF%BC%89" title="中心连接（Center-linkage）"></a>中心连接（Center-linkage）</h4><p>平均连接虽然看起来更加合理，但是两两元素间的距离计算量往往非常庞大。有时候，也可以使用中心连接计算方法。即先计算类别中心，再以中心连线作为两类别之间的距离。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/10.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/10.png"></a></p><p>总之，上面 4 种距离计算方法中，<strong>一般常用「平均连接」和「中心连接」</strong>方法，因为「单连接」和「全连接」都相对极端，容易受到噪声点和分布不均匀数据造成的干扰。</p><h3 id="Agglomerative-聚类-Python-实现"><a href="#Agglomerative-聚类-Python-实现" class="headerlink" title="Agglomerative 聚类 Python 实现"></a><a href="#Agglomerative-%E8%81%9A%E7%B1%BB-Python-%E5%AE%9E%E7%8E%B0" title="Agglomerative 聚类 Python 实现"></a>Agglomerative 聚类 Python 实现</h3><p>下面，我们尝试通过 Python 实现自底向上层次聚类算法。首先导入实验必要的模块：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line">from sklearn import datasets  </span><br><span class="line">from matplotlib import pyplot as plt  </span><br><span class="line">%matplotlib inline  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>然后，通过 <code>make_blobs</code> 方法随机生成一组示例数据，且使得示例数据呈现出 2 类数据的趋势。这里，我们设定随机数种子 <code>random_state=10</code> 以保证你的结果和实验结果一致。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; datasets.make\_blobs(10, n\_features&#x3D;2, centers&#x3D;2, random\_state&#x3D;10)  </span><br><span class="line">data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(array([[  6.04774884, -10.30504657],</span><br><span class="line">        [  2.90159483,   5.42121526],</span><br><span class="line">        [  4.1575017 ,   3.89627276],</span><br><span class="line">        [  1.53636249,   5.11121453],</span><br><span class="line">        [  3.88101257,  -9.59334486],</span><br><span class="line">        [  1.70789903,   6.00435173],</span><br><span class="line">        [  5.69192445,  -9.47641249],</span><br><span class="line">        [  5.4307043 ,  -9.75956122],</span><br><span class="line">        [  5.85943906,  -8.38192364],</span><br><span class="line">        [  0.69523642,   3.23270535]]), array([0, 1, 1, 1, 0, 1, 0, 0, 0, 1])) </span><br></pre></td></tr></table></figure><p>使用 Matplotlib 绘制示例数据结果，可以看到数据的确呈现出 <code>2</code> 种类别的趋势。其中 <code>data[1]</code> 的结果即为生成数据时预设的类别，当然接下来的聚类过程，我们是不知道数据的预设类别。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(data\[0\]\[:,0\], data\[0\]\[:,1\], c&#x3D;data\[1\], s&#x3D;60)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_46_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_46_1.png"></a></p><p>首先，我们实现欧式距离的计算函数：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;欧式距离  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def euclidean\_distance(a, b):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    a -- 数组 a  </span><br><span class="line">    b -- 数组 b  </span><br><span class="line">      </span><br><span class="line">    返回:  </span><br><span class="line">    dist -- a, b 间欧式距离  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    x &#x3D; float(a\[0\]) - float(b\[0\])  </span><br><span class="line">    x &#x3D; x \* x  </span><br><span class="line">    y &#x3D; float(a\[1\]) - float(b\[1\])  </span><br><span class="line">    y &#x3D; y \* y  </span><br><span class="line">    dist &#x3D; round(np.sqrt(x + y), 2)  </span><br><span class="line">    return dist  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>然后，实现 Agglomerative 聚类函数，这里使用中心连接的方法。为了更加具体的展示层次聚类的过程，这里在函数中添加一些多余的 <code>print()</code> 函数。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;Agglomerative 聚类计算过程  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def agglomerative\_clustering(data):  </span><br><span class="line">      </span><br><span class="line">    while len(data) &gt; 1:  </span><br><span class="line">        print(&quot;☞ 第 &#123;&#125; 次迭代\\n&quot;.format(10 - len(data) + 1))  </span><br><span class="line">        min\_distance &#x3D; float(&#39;inf&#39;) # 设定初始距离为无穷大  </span><br><span class="line">        for i in range(len(data)):  </span><br><span class="line">            print(&quot;---&quot;)  </span><br><span class="line">            for j in range(i + 1, len(data)):  </span><br><span class="line">                distance &#x3D; euclidean\_distance(data\[i\], data\[j\])  </span><br><span class="line">                print(&quot;计算 &#123;&#125; 与 &#123;&#125; 距离为 &#123;&#125;&quot;.format(data\[i\], data\[j\],distance))  </span><br><span class="line">                if distance &lt; min\_distance:  </span><br><span class="line">                    min\_distance &#x3D; distance  </span><br><span class="line">                    min\_ij &#x3D; (i, j)  </span><br><span class="line">        i, j &#x3D; min\_ij # 最近数据点序号  </span><br><span class="line">        data1 &#x3D; data\[i\]  </span><br><span class="line">        data2 &#x3D; data\[j\]  </span><br><span class="line">        data &#x3D; np.delete(data, j, 0) # 删除原数据  </span><br><span class="line">        data &#x3D; np.delete(data, i, 0) # 删除原数据  </span><br><span class="line">        b &#x3D; np.atleast\_2d(\[(data1\[0\] + data2\[0\]) &#x2F; 2, (data1\[1\] + data2\[1\]) &#x2F; 2\]) # 计算两点新中心  </span><br><span class="line">        data &#x3D; np.concatenate((data, b), axis&#x3D;0) # 将新数据点添加到迭代过程  </span><br><span class="line">        print(&quot;\\n最近距离:&#123;&#125; &amp; &#123;&#125; &#x3D; &#123;&#125;, 合并后中心:&#123;&#125;\\n&quot;.format(data1, data2, min\_distance, b))  </span><br><span class="line">          </span><br><span class="line">    return data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">agglomerative\_clustering(data\[0\])  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br></pre></td><td class="code"><pre><span class="line">☞ 第 1 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [2.90159483 5.42121526] 距离为 16.04</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [4.1575017  3.89627276] 距离为 14.33</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [1.53636249 5.11121453] 距离为 16.06</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 3.88101257 -9.59334486] 距离为 2.28</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [1.70789903 6.00435173] 距离为 16.88</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.69192445 -9.47641249] 距离为 0.9</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.4307043  -9.75956122] 距离为 0.82</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.85943906 -8.38192364] 距离为 1.93</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [0.69523642 3.23270535] 距离为 14.56</span><br><span class="line">---</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [4.1575017  3.89627276] 距离为 1.98</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.69192445 -9.47641249] 距离为 15.16</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.4307043  -9.75956122] 距离为 15.39</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11</span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.69192445 -9.47641249] 距离为 13.46</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.4307043  -9.75956122] 距离为 13.72</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">---</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.69192445 -9.47641249] 距离为 15.17</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.4307043  -9.75956122] 距离为 15.37</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.69192445 -9.47641249] 距离为 1.81</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.4307043  -9.75956122] 距离为 1.56</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">---</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.69192445 -9.47641249] 距离为 15.99</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.4307043  -9.75956122] 距离为 16.2</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95</span><br><span class="line">---</span><br><span class="line">计算 [ 5.69192445 -9.47641249] 与 [ 5.4307043  -9.75956122] 距离为 0.39</span><br><span class="line">计算 [ 5.69192445 -9.47641249] 与 [ 5.85943906 -8.38192364] 距离为 1.11</span><br><span class="line">计算 [ 5.69192445 -9.47641249] 与 [0.69523642 3.23270535] 距离为 13.66</span><br><span class="line">---</span><br><span class="line">计算 [ 5.4307043  -9.75956122] 与 [ 5.85943906 -8.38192364] 距离为 1.44</span><br><span class="line">计算 [ 5.4307043  -9.75956122] 与 [0.69523642 3.23270535] 距离为 13.83</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[ 5.69192445 -9.47641249] &amp; [ 5.4307043  -9.75956122] &#x3D; 0.39, 合并后中心:[[ 5.56131437 -9.61798686]]</span><br><span class="line"></span><br><span class="line">☞ 第 2 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [2.90159483 5.42121526] 距离为 16.04</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [4.1575017  3.89627276] 距离为 14.33</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [1.53636249 5.11121453] 距离为 16.06</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 3.88101257 -9.59334486] 距离为 2.28</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [1.70789903 6.00435173] 距离为 16.88</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.85943906 -8.38192364] 距离为 1.93</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [0.69523642 3.23270535] 距离为 14.56</span><br><span class="line">计算 [  6.04774884 -10.30504657] 与 [ 5.56131437 -9.61798686] 距离为 0.84</span><br><span class="line">---</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [4.1575017  3.89627276] 距离为 1.98</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.56131437 -9.61798686] 距离为 15.27</span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.56131437 -9.61798686] 距离为 13.59</span><br><span class="line">---</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.56131437 -9.61798686] 距离为 15.27</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.56131437 -9.61798686] 距离为 1.68</span><br><span class="line">---</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.56131437 -9.61798686] 距离为 16.09</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [ 5.56131437 -9.61798686] 距离为 1.27</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.56131437 -9.61798686] 距离为 13.74</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[  6.04774884 -10.30504657] &amp; [ 5.56131437 -9.61798686] &#x3D; 0.84, 合并后中心:[[ 5.80453161 -9.96151671]]</span><br><span class="line"></span><br><span class="line">☞ 第 3 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [4.1575017  3.89627276] 距离为 1.98</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.80453161 -9.96151671] 距离为 15.65</span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96</span><br><span class="line">---</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06</span><br><span class="line">计算 [1.53636249 5.11121453] 与 [ 5.80453161 -9.96151671] 距离为 15.67</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96</span><br><span class="line">---</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95</span><br><span class="line">计算 [1.70789903 6.00435173] 与 [ 5.80453161 -9.96151671] 距离为 16.48</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[1.53636249 5.11121453] &amp; [1.70789903 6.00435173] &#x3D; 0.91, 合并后中心:[[1.62213076 5.55778313]]</span><br><span class="line"></span><br><span class="line">☞ 第 4 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [4.1575017  3.89627276] 距离为 1.98</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [ 5.80453161 -9.96151671] 距离为 15.65</span><br><span class="line">计算 [2.90159483 5.42121526] 与 [1.62213076 5.55778313] 距离为 1.29</span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [1.62213076 5.55778313] 距离为 3.03</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [1.62213076 5.55778313] 距离为 15.32</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [1.62213076 5.55778313] 距离为 14.57</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [1.62213076 5.55778313] 距离为 2.5</span><br><span class="line">---</span><br><span class="line">计算 [ 5.80453161 -9.96151671] 与 [1.62213076 5.55778313] 距离为 16.07</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[2.90159483 5.42121526] &amp; [1.62213076 5.55778313] &#x3D; 1.29, 合并后中心:[[2.26186279 5.4894992 ]]</span><br><span class="line"></span><br><span class="line">☞ 第 5 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [2.26186279 5.4894992 ] 距离为 15.17</span><br><span class="line">---</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58</span><br><span class="line">计算 [ 5.85943906 -8.38192364] 与 [2.26186279 5.4894992 ] 距离为 14.33</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75</span><br><span class="line">---</span><br><span class="line">计算 [ 5.80453161 -9.96151671] 与 [2.26186279 5.4894992 ] 距离为 15.85</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[ 5.85943906 -8.38192364] &amp; [ 5.80453161 -9.96151671] &#x3D; 1.58, 合并后中心:[[ 5.83198533 -9.17172018]]</span><br><span class="line"></span><br><span class="line">☞ 第 6 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 5.83198533 -9.17172018] 距离为 13.17</span><br><span class="line">---</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [2.26186279 5.4894992 ] 距离为 15.17</span><br><span class="line">计算 [ 3.88101257 -9.59334486] 与 [ 5.83198533 -9.17172018] 距离为 2.0</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 5.83198533 -9.17172018] 距离为 13.43</span><br><span class="line">---</span><br><span class="line">计算 [2.26186279 5.4894992 ] 与 [ 5.83198533 -9.17172018] 距离为 15.09</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[ 3.88101257 -9.59334486] &amp; [ 5.83198533 -9.17172018] &#x3D; 2.0, 合并后中心:[[ 4.85649895 -9.38253252]]</span><br><span class="line"></span><br><span class="line">☞ 第 7 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48</span><br><span class="line">计算 [4.1575017  3.89627276] 与 [ 4.85649895 -9.38253252] 距离为 13.3</span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 4.85649895 -9.38253252] 距离为 13.28</span><br><span class="line">---</span><br><span class="line">计算 [2.26186279 5.4894992 ] 与 [ 4.85649895 -9.38253252] 距离为 15.1</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[4.1575017  3.89627276] &amp; [2.26186279 5.4894992 ] &#x3D; 2.48, 合并后中心:[[3.20968225 4.69288598]]</span><br><span class="line"></span><br><span class="line">☞ 第 8 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [ 4.85649895 -9.38253252] 距离为 13.28</span><br><span class="line">计算 [0.69523642 3.23270535] 与 [3.20968225 4.69288598] 距离为 2.91</span><br><span class="line">---</span><br><span class="line">计算 [ 4.85649895 -9.38253252] 与 [3.20968225 4.69288598] 距离为 14.17</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[0.69523642 3.23270535] &amp; [3.20968225 4.69288598] &#x3D; 2.91, 合并后中心:[[1.95245933 3.96279567]]</span><br><span class="line"></span><br><span class="line">☞ 第 9 次迭代</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">计算 [ 4.85649895 -9.38253252] 与 [1.95245933 3.96279567] 距离为 13.66</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">最近距离:[ 4.85649895 -9.38253252] &amp; [1.95245933 3.96279567] &#x3D; 13.66, 合并后中心:[[ 3.40447914 -2.70986843]]</span><br><span class="line"></span><br><span class="line">array([[ 3.40447914, -2.70986843]]) </span><br></pre></td></tr></table></figure><p>通过上面的计算过程，你应该能很清晰地看出 Agglomerative 聚类的完整过程了。我们将 <code>data</code> 数组的每行依次按 <code>0-9</code> 编号，并将计算过程绘制成层次聚类的二叉树结构如下：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/11.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/11.png"></a></p><p>建立好树形结构后，如果我们想取类别为 <code>2</code>，就从顶部画一条横线就可以了。然后，沿着网络延伸到叶节点就能找到各自对应的类别。如下图所示：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/12.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/12.png"></a></p><p>如果你对着 <code>data</code> 预设的类别，你会发现最终的聚类结果和 <code>data[1] = [0, 1, 1, 1, 0, 1, 0, 0, 0, 1]</code>完全一致。</p><p>至此，我们就完整实现了自底向上层次聚类法。</p><h3 id="使用-scikit-learn-完成-Agglomerative-聚类"><a href="#使用-scikit-learn-完成-Agglomerative-聚类" class="headerlink" title="使用 scikit-learn 完成 Agglomerative 聚类"></a><a href="#%E4%BD%BF%E7%94%A8-scikit-learn-%E5%AE%8C%E6%88%90-Agglomerative-%E8%81%9A%E7%B1%BB" title="使用 scikit-learn 完成 Agglomerative 聚类"></a>使用 scikit-learn 完成 Agglomerative 聚类</h3><p>scikit-learn 中也提供了 Agglomerative 聚类的类，相应的参数解释如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cluster.AgglomerativeClustering(n\_clusters&#x3D;2, affinity&#x3D;&#39;euclidean&#39;, memory&#x3D;None, connectivity&#x3D;None, compute\_full\_tree&#x3D;&#39;auto&#39;, linkage&#x3D;&#39;ward&#39;, pooling\_func&#x3D;&lt;function mean&gt;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>n_clusters</code>: 表示最终要查找类别的数量，例如上面的 2 类。</li><li><code>affinity</code>: 亲和力度量，有 <code>euclidean</code>（欧式距离）, <code>l1</code>（L1 范数）, <code>l2</code>（L2 范数）, <code>manhattan</code>（曼哈顿距离）等可选。</li><li><code>linkage</code>: 连接方法：<code>ward</code>（单连接）, <code>complete</code>（全连接）, <code>average</code>（平均连接）可选。</li></ul><p>实验同样使用上面的数据集完成模型构建并聚类：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import AgglomerativeClustering  </span><br><span class="line">  </span><br><span class="line">model &#x3D; AgglomerativeClustering(n\_clusters&#x3D;2, affinity&#x3D;&#39;euclidean&#39;, linkage&#x3D;&#39;average&#39;)  </span><br><span class="line">model.fit\_predict(data\[0\])  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 0, 0, 0, 1, 0, 1, 1, 1, 0]) </span><br></pre></td></tr></table></figure><p>可以看到，最终的聚类结构和我们上面是一致的。（表示类别的 <code>1, 0</code> 相反没有影响）</p><h2 id="自顶向下层次聚类法"><a href="#自顶向下层次聚类法" class="headerlink" title="自顶向下层次聚类法"></a><a href="#%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E6%B3%95" title="自顶向下层次聚类法"></a>自顶向下层次聚类法</h2><p>除了上面讲到的自底向上的层次聚类法，还有一类是自顶向下层次聚类法，这种方法的计算流程与前一种正好相反，但过程要复杂很多。</p><p>大致说来，我们首先将全部数据归为一类，然后逐步分割成小类，而这里的分割方法又有常见的 2 种形式：</p><h3 id="利用-K-Means-算法进行分割"><a href="#利用-K-Means-算法进行分割" class="headerlink" title="利用 K-Means 算法进行分割"></a><a href="#%E5%88%A9%E7%94%A8-K-Means-%E7%AE%97%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%88%86%E5%89%B2" title="利用 K-Means 算法进行分割"></a>利用 K-Means 算法进行分割</h3><p>首先，我们说一说利用 K-Means 算法分割方法：</p><ol><li>把数据集 D$D$ 归为单个类别 C$C$ 作为顶层。</li><li>使用 K-Means 算法把 C$C$ 划分成 2 个子类别，构成子层；</li><li>可递归使用 K-Means 算法继续划分子层到终止条件。</li></ol><p>同样，我们可以通过示意图来演示该聚类算法的流程。</p><p>首先，全部数据在一个类别中：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/13.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/13.png"></a></p><p>然后，通过 K-Means 算法把其聚成 <code>2</code> 类。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/14.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/14.png"></a></p><p>紧接着，将子类再分别使用 K-Means 算法聚成 <code>2</code> 类。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/15.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/15.png"></a></p><p>最终，直到所以的数据都各自为 1 个类别，即分割完成。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/16.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/16.png"></a></p><p>利用 K-Means 算法进行自顶向下层次聚类过程同样属于看起来简单，但计算量庞大的过程。</p><h3 id="利用平均距离进行分割"><a href="#利用平均距离进行分割" class="headerlink" title="利用平均距离进行分割"></a><a href="#%E5%88%A9%E7%94%A8%E5%B9%B3%E5%9D%87%E8%B7%9D%E7%A6%BB%E8%BF%9B%E8%A1%8C%E5%88%86%E5%89%B2" title="利用平均距离进行分割"></a>利用平均距离进行分割</h3><ol><li>把数据集 D$D$ 归为单个类别 C$C$ 作为顶层。</li><li>从类别 C$C$ 中取出点 d$d$，使得 d$d$ 满足到 C$C$ 中其他点的平均距离最远，构成类别 N$N$。</li><li>继续从类别 C$C$ 中取出点 d’$d’$, 使得 d’$d’$ 满足到 C$C$ 中其他点的平均距离与到 N$N$ 中点的平均距离之间的差值最大，并将点放入 N$N$。</li><li>重复步骤 <code>3</code>，直到差值为负数。</li><li>再从子类中重复步骤 <code>2</code>,<code>3</code>,<code>4</code> 直到全部点单独成类，即完成分割。</li></ol><p>同样，我们可以通过示意图来演示该聚类算法的流程。</p><p>首先，全部数据在一个类别中：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/17.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/17.png"></a></p><p>然后，我们依次抽取 1 个数据点，并计算它与其他点的平均距离，且最终取平均距离最大的点单独成类。例如这里计算出结果为 5。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/18.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/18.png"></a></p><p>同样，从剩下的 4 个点再取出一个点，使该点到剩下点的平均距离与该点到点 5 的距离差值最大且不为负数。这里没有点满足条件，终止。</p><p>接下来，从剩下的 4 个点中再取出一个点，并计算它与其他 3 点的距离，取最大单独成类。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/19.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/19.png"></a></p><p>同样，从剩下的 3 个点中再取出一个点，使该点到剩下点的平均距离与该点到点 4 的距离差值最大且不为负数，合并为 1 类。点 3 明显满足：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/20.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/20.png"></a></p><p>重复步骤，继续计算形成子层。然后对子层中包含有对应元素的类重复上面的步骤聚类，直到全部点单独成类，即完成分割。</p><p>自顶向下层次聚类法在实施过程中常常遇到一个问题，那就是如果两个样本在上一步聚类中被划分成不同的类别，那么即使这两个点距离非常近，后面也不会被放到一类中。</p><p>所以在实际应用中，自顶向下层次聚类法没有自底而上的层次聚类法常用，这里也就不再进行实现了，了解其运行原理即可。</p><h2 id="BIRCH-聚类算法"><a href="#BIRCH-聚类算法" class="headerlink" title="BIRCH 聚类算法"></a><a href="#BIRCH-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95" title="BIRCH 聚类算法"></a>BIRCH 聚类算法</h2><p>除了上文提到的两种层次聚类方法，还有一种非常常用且高效的层次聚类法，叫做 BIRCH。</p><p>BIRCH 的全称为 Balanced Iterative Reducing and Clustering using Hierarchies，直译过来就是「使用层次方法的平衡迭代规约和聚类」。该算法由时任 IBM 工程师 Tian Zhang 于 1996 年发明，详见 <a href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">论文</a>。</p><p>BIRCH 最大的特点就是高效，可用于大型数据集的快速聚类。</p><h3 id="CF-CF-聚类特征"><a href="#CF-CF-聚类特征" class="headerlink" title="CF$CF$ 聚类特征"></a><a href="#CF-%E8%81%9A%E7%B1%BB%E7%89%B9%E5%BE%81" title="`CF`  聚类特征"></a>CF$CF$ 聚类特征</h3><p>BIRCH 的聚类过程主要是涉及到 CF$CF$ 聚类特征和 CF$CF$ Tree 聚类特征树的概念。所以，我们需要先了解什么是聚类特征。</p><p>一组样本的 CF$CF$ 聚类特征定义为如下所示的三元组：</p><p>CF=⟨(N,LS,SS)⟩</p><p>$$CF=⟨(N,LS,SS)⟩$$</p><p>其中，N$N$ 表示该 CF$CF$ 中拥有的样本点的数量； LS$LS$ 表示该 CF$CF$ 中拥有的样本点各特征维度的和向量；SS$SS$ 表示该 CF$CF$ 中拥有的样本点各特征维度的平方和。</p><p>例如，我们有 5 个样本，分别为：(1,3),(2,5),(1,3),(7,9),(8,8)$(1,3),(2,5),(1,3),(7,9),(8,8)$，那么：</p><ul><li>N=5$N=5$</li><li>LS=(1+2+1+7+8,3+5+3+9+8)=(19,28)$LS=(1+2+1+7+8,3+5+3+9+8)=(19,28)$</li><li>SS=(12+22+12+72+82+32+52+32+92+82)=(307)$SS=(12+22+12+72+82+32+52+32+92+82)=(307)$</li></ul><p>于是，对应的 CF$CF$ 值就为：</p><p>CF=⟨5,(19,28),(307)⟩</p><p>$$CF=⟨5,(19,28),(307)⟩$$</p><p>CF$CF$ 拥有可加性，例如当 CF′=⟨3,(35,36),857⟩$CF′=⟨3,(35,36),857⟩$ 时：</p><p>CF′+CF=⟨5,(19,28),(307)⟩+⟨3,(12,26),87⟩=⟨8,(31,54),(394)⟩</p><p>$$CF′+CF=⟨5,(19,28),(307)⟩+⟨3,(12,26),87⟩=⟨8,(31,54),(394)⟩$$</p><p>CF$CF$ 聚类特征本质上是定义类别（簇）的信息，并有效地对数据进行压缩。</p><h3 id="CF-CF-聚类特征树"><a href="#CF-CF-聚类特征树" class="headerlink" title="CF$CF$ 聚类特征树"></a><a href="#CF-%E8%81%9A%E7%B1%BB%E7%89%B9%E5%BE%81%E6%A0%91" title="`CF`  聚类特征树"></a>CF$CF$ 聚类特征树</h3><p>接下来，我们介绍第二个概念 CF$CF$ 聚类特征树。</p><p>CF$CF$ 树由根节点（root node）、枝节点（branch node）和叶节点（leaf node）构成。另包含有三个参数，分别为：枝平衡因子 β$β$、叶平衡因子 λ$λ$ 和空间阈值 τ$τ$。而非叶节点（nonleaf node）中包含不多于 β$β$ 个 [CF,childi]$[CF,childi]$ 的元项。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/21.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/21.png"></a></p><p>BIRCH 算法的核心就是基于训练样本建立了 CF$CF$ 聚类特征树。CF$CF$ 聚类特征树对应的输出就是若干个 CF$CF$ 节点，每个节点里的样本点就是一个聚类的类别。</p><p>其实，关于 CF$CF$ 聚类特征树的特点以及树的生成过程还有很多内容可以深入学习，不过这里面涉及到大量的数学理论和推导过程，不太好理解，这里就不再展开了。有兴趣的同学可以阅读 <a href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">原论文</a>。</p><p>最后，我们简单说一下 BIRCH 算法相比 Agglomerative 算法的优势，也就是总结学习 BIRCH 算法的必要性：</p><ol><li>BIRCH 算法在建立 CF 特征树时只存储原始数据的特征信息，并不需要存储原始数据信息，内存开销上更优，计算高效。</li><li>BIRCH 算法只需要遍历一遍原始数据，而 Agglomerative 算法在每次迭代都需要遍历一遍数据，再次突出 BIRCH 的高效性。</li><li>BIRCH 属于在线学习算法，并支持对流数据的聚类，开始聚类时并不需要知道所有的数据。</li></ol><h3 id="BIRCH-聚类实现"><a href="#BIRCH-聚类实现" class="headerlink" title="BIRCH 聚类实现"></a><a href="#BIRCH-%E8%81%9A%E7%B1%BB%E5%AE%9E%E7%8E%B0" title="BIRCH 聚类实现"></a>BIRCH 聚类实现</h3><p>上面说了这么多，总结就是 BIRCH 属于层次聚类算法中非常高效的那一种方法。下面，就来看一看如何调用 scikit-learn 提供的 BIRCH 类完成聚类任务。</p><p>我们先导入 DIGITS 数据集，并查看前 5 个手写字符。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line">from sklearn import datasets  </span><br><span class="line">  </span><br><span class="line">digits &#x3D; datasets.load\_digits()  </span><br><span class="line">  </span><br><span class="line">\# 查看前 5 个字符  </span><br><span class="line">fig, axes &#x3D; plt.subplots(1, 5, figsize&#x3D;(12,4))  </span><br><span class="line">for i, image in enumerate(digits.images\[:5\]):  </span><br><span class="line">    axes\[i\].imshow(image, cmap&#x3D;plt.cm.gray\_r)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="output_117_0.png"><img src="output_117_0.png" alt="png">png</a></p><p>我们都知道，一个手写字符的数据是由 8x8 的矩阵表示。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">digits.images\[0\]  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],</span><br><span class="line">       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],</span><br><span class="line">       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],</span><br><span class="line">       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]]) </span><br></pre></td></tr></table></figure><p>如果我们针对该矩阵进行扁平化处理，就能变为 1x64 的向量。对于这样一个高维向量，虽然可以在聚类时直接计算距离，但却无法很好地在二维平面中表示相应的数据点。因为，二维平面中的点只由横坐标和纵坐标组成。</p><p>所以，为了尽可能还原聚类的过程，我们需要将 1x64 的行向量（64 维），处理成 1x2 的行向量（2 维），也就是降维的过程。</p><p>既然是降低维度，那么应该怎样做呢？是直接取前面两位数，或者随机取出两位？当然不是。这里学习一种新方法，叫 PCA 主成分分析。</p><h3 id="PCA-主成分分析（降维）"><a href="#PCA-主成分分析（降维）" class="headerlink" title="PCA 主成分分析（降维）"></a><a href="#PCA-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88%E9%99%8D%E7%BB%B4%EF%BC%89" title="PCA 主成分分析（降维）"></a>PCA 主成分分析（降维）</h3><p>主成分分析是多元线性统计里面的概念，它的英文是：Principal Components Analysis，简称 PCA。主成分分析旨在降低数据的维数，通过保留数据集中的主要成分来简化数据集。</p><p>主成分分析的数学原理非常简单，通过对协方差矩阵进行特征分解，从而得出主成分（特征向量）与对应的权值（特征值）。然后剔除那些较小特征值（较小权值）对应的特征，从而达到降低数据维数的目的。</p><p>主成分分析通常有两个作用：</p><ol><li><p>参考本文的目的，方便将数据用于低维空间可视化。聚类过程中的可视化是很有必要的。</p></li><li><p>高维度数据集往往就意味着计算资源的大量消耗。通过对数据进行降维，我们就能在不较大影响结果的同时，减少模型学习时间。</p></li></ol><p>现在，假定我们需要将特征维度从 n$n$ 维降到 m$m$ 维，PCA 的计算流程如下：</p><p><strong>1.对各维度特征进行标准化处理：</strong></p><p>x(i)j=x(i)j−μjsj</p><p>$$xj(i)=xj(i)−μjsj$$</p><p>其中，μj$μj$ 为特征 j$j$ 的均值，sj$sj$ 为特征 j$j$ 的标准差。</p><p><strong>2.计算对应的协方差矩阵：</strong><br>Σ=1mm∑i=1(x(i))(x(i))T=1m⋅XTX</p><p>$$Σ=1m∑i=1m(x(i))(x(i))T=1m⋅XTX$$</p><p><strong>3.对协方差矩阵进行奇异值分解（SVD），得到特征向量：</strong><br>(U,S,VT)=SVD(Σ)</p><p>$$(U,S,VT)=SVD(Σ)$$</p><p><strong>4.从 U$U$ 中取出前 m$m$ 个左奇异向量，构成一个约减矩阵 Ureduce$Ureduce$ ：</strong><br>Ureduce=(u(1),u(2),⋯,u(k))</p><p>$$Ureduce=(u(1),u(2),⋯,u(k))$$</p><p><strong>5.计算新的特征向量 z(i)$z(i)$：</strong><br>z(i)=UTreduce⋅x(i)</p><p>$$z(i)=UreduceT⋅x(i)$$</p><p><strong>6.最后根据新的特征向量执行特征还原：</strong><br>xnew=Ureducez(i)</p><p>$$xnew=Ureducez(i)$$</p><p>PCA 的过程听起来简单，执行起来还是比较麻烦的。所以，我们这里直接使用 scikit-learn 中 <code>PCA</code> 方法完成：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.decomposition.PCA(n\_components&#x3D;None, copy&#x3D;True, whiten&#x3D;False, svd\_solver&#x3D;&#39;auto&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>n_components=</code> 表示需要保留主成分（特征）的数量。</li><li><code>copy=</code> 表示针对原始数据降维还是针对原始数据副本降维。当参数为 False 时，降维后的原始数据会发生改变，这里默认为 True。</li><li><code>whiten=</code> 白化表示将特征之间的相关性降低，并使得每个特征具有相同的方差。</li><li><code>svd_solver=</code> 表示奇异值分解 SVD 的方法。有 4 参数，分别是：<code>auto</code>, <code>full</code>, <code>arpack</code>, <code>randomized</code>。</li></ul><p>在使用 PCA 降维时，我们也会使用到 <code>PCA.fit()</code> 方法。<code>.fit()</code> 是 scikit-learn 训练模型的通用方法，但是该方法本身返回的是模型的参数。所以，通常我们会使用 <code>PCA.fit_transform()</code> 方法直接返回降维后的数据结果。</p><p>下面，我们就针对 DIGITS 数据集进行特征降维。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA  </span><br><span class="line">  </span><br><span class="line">\# PCA 将数据降为 2 维  </span><br><span class="line">pca &#x3D; PCA(n\_components&#x3D;2)  </span><br><span class="line">pca\_data &#x3D; pca.fit\_transform(digits.data)  </span><br><span class="line">pca\_data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[ -1.25946556,  21.27488973],</span><br><span class="line">       [  7.95761389, -20.7686883 ],</span><br><span class="line">       [  6.99192015,  -9.95599952],</span><br><span class="line">       ...,</span><br><span class="line">       [ 10.80128328,  -6.96025693],</span><br><span class="line">       [ -4.87209994,  12.42397329],</span><br><span class="line">       [ -0.34439245,   6.36553344]]) </span><br></pre></td></tr></table></figure><p>可以看到，每一行的特征已经由先前的 64 个缩减为 2 个了。</p><p>接下来将降维后的数据绘制到二维平面中。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.scatter(pca\_data\[:,0\], pca\_data\[:,1\])  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_147_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_147_1.png"></a></p><p>上图就是 DIGITS 数据集中 1797 个样本通过 PCA 降维后对应在二维平面的数据点。</p><p>现在，我们可以直接使用 BIRCH 对降维后的数据进行聚类。由于我们提前知道这是手写数字字符，所以选择聚为 <code>10</code> 类。当然，在聚类时，我们只是知道大致要聚集的类别数量，而并不知道数据对应的标签值。</p><p>BIRCH 在 scikit-learn 对应的主要类及参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cluster.Birch(threshold&#x3D;0.5, branching\_factor&#x3D;50, n\_clusters&#x3D;3, compute\_labels&#x3D;True, copy&#x3D;True)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>threshold</code>: 每个 CF 的空间阈值 τ$τ$。参数值越小，则 CF 特征树的规模会越大，学习时花费的时间和内存会越多。默认值是 0.5，但如果样本的方差较大，则一般需要增大这个默认值。</li><li><code>branching_factor</code>: CF 树中所有节点的最大 CF 数。该参数默认为 50，如果样本量非常大，一般需要增大这个默认值。</li><li><code>n_clusters</code>: 虽然层次聚类无需预先设定类别数量，但可以设定期望查询的类别数。</li></ul><p>接下来，使用 BIRCH 算法得到 PCA 降维后数据的聚类结果：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import Birch  </span><br><span class="line">  </span><br><span class="line">birch &#x3D; Birch(n\_clusters&#x3D;10)  </span><br><span class="line">cluster\_pca &#x3D; birch.fit\_predict(pca\_data)  </span><br><span class="line">cluster\_pca  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([3, 0, 0, ..., 0, 5, 9]) </span><br></pre></td></tr></table></figure><p>利用得到的聚类结果对散点图进行着色。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.scatter(pca\_data\[:,0\], pca\_data\[:,1\],c&#x3D;cluster\_pca)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_156_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_156_1.png"></a></p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">\# 计算聚类过程中的决策边界  </span><br><span class="line">x\_min, x\_max &#x3D; pca\_data\[:, 0\].min() - 1, pca\_data\[:, 0\].max() + 1  </span><br><span class="line">y\_min, y\_max &#x3D; pca\_data\[:, 1\].min() - 1, pca\_data\[:, 1\].max() + 1  </span><br><span class="line">  </span><br><span class="line">xx, yy &#x3D; np.meshgrid(np.arange(x\_min, x\_max, .4), np.arange(y\_min, y\_max, .4))  </span><br><span class="line">temp\_cluster &#x3D; birch.predict(np.c\_\[xx.ravel(), yy.ravel()\])  </span><br><span class="line">  </span><br><span class="line">\# 将决策边界绘制出来  </span><br><span class="line">temp\_cluster &#x3D; temp\_cluster.reshape(xx.shape)  </span><br><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.contourf(xx, yy, temp\_cluster, cmap&#x3D;plt.cm.bwr, alpha&#x3D;.3)  </span><br><span class="line">plt.scatter(pca\_data\[:, 0\], pca\_data\[:, 1\], c&#x3D;cluster\_pca, s&#x3D;15)  </span><br><span class="line">  </span><br><span class="line">\# 图像参数设置  </span><br><span class="line">plt.xlim(x\_min, x\_max)  </span><br><span class="line">plt.ylim(y\_min, y\_max)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_157_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_157_1.png"></a></p><p>其实，我们可以利用预先知道的各字符对应的标签对散点图进行着色，对比上面的聚类结果。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.scatter(pca\_data\[:,0\], pca\_data\[:,1\],c&#x3D;digits.target)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_159_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_159_1.png"></a></p><p>对照两幅图片，你会发现对 PCA 降维数据的聚类结果大致符合原数据的分布趋势。这里色块的颜色不对应没有关系，因为原标签和聚类标签的顺序不对应，只需要关注数据块的分布规律即可。</p><p>不过，使用真实标签绘制出来的散点图明显凌乱很多，这其实是由于 PCA 降维造成的。</p><p>一般情况下，我们输入到聚类模型中的数据不一定要是降维后的数据。下面输入原数据重新聚类试一试。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cluster\_ori &#x3D; birch.fit\_predict(digits.data)  </span><br><span class="line">cluster\_ori  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([7, 9, 4, ..., 4, 1, 4]) </span><br></pre></td></tr></table></figure><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(10, 8))  </span><br><span class="line">plt.scatter(pca\_data\[:,0\], pca\_data\[:,1\],c&#x3D;cluster\_ori)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_163_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/output_163_1.png"></a></p><p>现在你会发现，实验得到的聚类结果更加符合原数据集的分布规律了。再次强调，这里颜色不分离其实是由于 PCA 降维后在二维平面可视化的效果，不代表真实的聚类效果。</p><p>不过，最后我们再强调一下 PCA 的使用情形。一般情况下，我们不会拿到数据就进行 PCA 处理，只有当算法不尽如人意、训练时间太长、需要可视化等情形才考虑使用 PCA。其主要原因是，PCA 被看作是对数据的有损压缩，会造成数据集原始特征丢失。</p><p>本次我们了解了层次聚类方法，特别地学习了向上、向下以及 BIRCH 算法。其中，比较常用的是自底向上或 BIRCH 方法，且 BIRCH 拥有计算高效的特点。不过，BIRCH 也有一些弊端，例如对高维数据的聚类效果往往不太好，有时候我们也会使用 Mini Batch K-Means 进行替代。最后，通过表格对比本次实验的 3 种层次聚类法的优缺点：</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/22.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Hierarchical%20clustering%20method/22.png"></a></p><p><strong>拓展阅读：</strong></p><ul><li><a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">Hierarchical clustering - Wikipedia</a></li><li><a href="https://en.wikipedia.org/wiki/BIRCH">BIRCH - Wikipedia</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在之前的文章中，我们学习了划分聚类方法，并着重介绍了其中的 K-Means 算法。K-Means 算法可以说是用处非常广泛的聚类算法之一，它非常好用。但是，当你使用过这种算法之后，你就会发现一个比较让人「头疼」的问题，那就是我们需要手动指定 K 值，也就是聚类的类别数量。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="层次聚类" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Cluster" scheme="http://www.laugh12321.cn/blog/tags/Cluster/"/>
    
    <category term="Hierarchical Clustering" scheme="http://www.laugh12321.cn/blog/tags/Hierarchical-Clustering/"/>
    
  </entry>
  
  <entry>
    <title>使用 Mini Batch K-Means 进行图像压缩</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/09/image_compression_using_mini_batch_k-means/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/09/image_compression_using_mini_batch_k-means/</id>
    <published>2019-02-09T00:00:00.000Z</published>
    <updated>2020-10-23T08:56:47.484Z</updated>
    
    <content type="html"><![CDATA[<p>针对一张成都著名景点：锦里的图片，通过 Mini Batch K-Means 的方法将相近的像素点聚合后用同一像素点代替，以达到图像压缩的效果。  </p><a id="more"></a><h3 id="图像导入"><a href="#图像导入" class="headerlink" title="图像导入"></a>图像导入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\# 使用 Matplotlib 可视化示例图片  </span><br><span class="line">%matplotlib inline  </span><br><span class="line">import matplotlib.pyplot as plt   </span><br><span class="line">import matplotlib.image as mpimg   </span><br><span class="line">  </span><br><span class="line">chengdu &#x3D; mpimg.imread(&#39;chengdu.png&#39;) # 将图片加载为 ndarray 数组  </span><br><span class="line">plt.imshow(chengdu) # 将数组还原成图像  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_2_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_2_1.png" alt="&lt;matplotlib.image.AxesImage at 0x7f347f1b0320&gt;"></a></p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chengdu.shape  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(516, 819, 3) </span><br></pre></td></tr></table></figure><p>在使用 <code>mpimg.imread</code> 函数读取图片后，实际上返回的是一个 <code>numpy.array</code> 类型的数组，该数组表示的是一个像素点的矩阵，包含长，宽，高三个要素。如成都锦里这张图片，总共包含了 516$516$ 行，819$819$ 列共 516⋅819=422604$516⋅819=422604$ 个像素点，每一个像素点的高度对应着计算机颜色中的三原色 R,G,B$R,G,B$（红，绿，蓝），共 3 个要素构成。</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86" title="数据预处理"></a>数据预处理</h3><p>为方便后期的数据处理，需要对数据进行降维。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\# 将形状为 (516, 819, 3) 的数据转换为 (422604, 3) 形状的数据。  </span><br><span class="line">data &#x3D; chengdu.reshape(516\*819, 3)  </span><br><span class="line">data.shape, data\[10\]  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((422604, 3), array([0.12941177, 0.13333334, 0.14901961], dtype&#x3D;float32)) </span><br></pre></td></tr></table></figure><h3 id="像素点种类个数计算"><a href="#像素点种类个数计算" class="headerlink" title="像素点种类个数计算"></a><a href="#%E5%83%8F%E7%B4%A0%E7%82%B9%E7%A7%8D%E7%B1%BB%E4%B8%AA%E6%95%B0%E8%AE%A1%E7%AE%97" title="像素点种类个数计算"></a>像素点种类个数计算</h3><p>尽管有 <code>422604</code> 个像素点，但其中仍然有许多相同的像素点。在此我们定义：R,G,B$R,G,B$ 值相同的点为一个种类，其中任意值不同的点为不同种类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算像素点种类个数  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def get\_variety(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    预处理后像素点集合  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    num\_variety -- 像素点种类个数  </span><br><span class="line">      </span><br><span class="line">    思路：将数据转化为 list 类型，然后将每一个元素转换为 tuple 类型，最后利用 set() 和 len() 函数进行计算。  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">    temp &#x3D; data.tolist()  </span><br><span class="line">    num\_variety&#x3D;len(set(\[tuple(t) for t in temp\]))  </span><br><span class="line">      </span><br><span class="line">    return num\_variety  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_variety(data), data\[20\]  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(100109, array([0.24705882, 0.23529412, 0.2627451 ], dtype&#x3D;float32)) </span><br></pre></td></tr></table></figure><h3 id="Mini-Batch-K-Means-聚类"><a href="#Mini-Batch-K-Means-聚类" class="headerlink" title="Mini Batch K-Means 聚类"></a><a href="#Mini-Batch-K-Means-%E8%81%9A%E7%B1%BB" title="Mini Batch K-Means 聚类"></a>Mini Batch K-Means 聚类</h3><p>像素点种类的数量是决定图片大小的主要因素之一，在此使用 Mini Batch K-Means 的方式将图片的像素点进行聚类，将相似的像素点用同一像素点值来代替，从而降低像素点种类的数量，以达到压缩图片的效果。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import MiniBatchKMeans  </span><br><span class="line">  </span><br><span class="line">model &#x3D; MiniBatchKMeans(10) # 聚类簇数量设置为 10 类  </span><br><span class="line">  </span><br><span class="line">model.fit(data)  </span><br><span class="line">predict&#x3D;model.predict(data)  </span><br><span class="line">  </span><br><span class="line">new\_colors &#x3D; model.cluster\_centers\_\[predict\]  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |<br>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\# 调用前面实现计算像素点种类的函数，计算像素点更新后种类的个数  </span><br><span class="line">get\_variety(new\_colors)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10 </span><br></pre></td></tr></table></figure><h3 id="图像压缩前后展示"><a href="#图像压缩前后展示" class="headerlink" title="图像压缩前后展示"></a><a href="#%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9%E5%89%8D%E5%90%8E%E5%B1%95%E7%A4%BA" title="图像压缩前后展示"></a>图像压缩前后展示</h3><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\# 将聚类后并替换为类别中心点值的像素点，变换为数据处理前的格式，并绘制出图片进行对比展示  </span><br><span class="line">fig, ax &#x3D; plt.subplots(1, 2, figsize&#x3D;(16, 6))  </span><br><span class="line">  </span><br><span class="line">new\_chengdu &#x3D; new\_colors.reshape(chengdu.shape)  </span><br><span class="line">ax\[0\].imshow(chengdu)  </span><br><span class="line">ax\[1\].imshow(new\_chengdu)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_17_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_17_1.png" alt="&lt;matplotlib.image.AxesImage at 0x7f347651cac8&gt;"></a></p><p>通过图片对比，可以十分容易发现画质被压缩了。其实，因为使用了聚类，压缩后的图片颜色就变为了 10 种。</p><p>接下来，使用 <code>mpimg.imsave()</code> 函数将压缩好的文件进行存储，并对比压缩前后图像的体积变化。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\# 运行对比  </span><br><span class="line">mpimg.imsave(&quot;new\_chengdu.png&quot;,new\_chengdu)  </span><br><span class="line">!du -h new\_chengdu.png  </span><br><span class="line">!du -h chengdu.png  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">220K    new_chengdu.png</span><br><span class="line">1.1M    chengdu.png </span><br></pre></td></tr></table></figure><p>可以看到，使用 Mini Batch K-Means 聚类方法对图像压缩之后，体积明显缩小。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;针对一张成都著名景点：锦里的图片，通过 Mini Batch K-Means 的方法将相近的像素点聚合后用同一像素点代替，以达到图像压缩的效果。  &lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="划分聚类" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB/"/>
    
    <category term="K-Means" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB/K-Means/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="K-Means" scheme="http://www.laugh12321.cn/blog/tags/K-Means/"/>
    
    <category term="图像压缩" scheme="http://www.laugh12321.cn/blog/tags/%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|划分聚类之 K-Means 详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/09/cluster_of_k-means/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/09/cluster_of_k-means/</id>
    <published>2019-02-09T00:00:00.000Z</published>
    <updated>2020-10-23T08:50:36.955Z</updated>
    
    <content type="html"><![CDATA[<p>划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。</p><p>在划分的过程中，首先由用户确定划分子集的个数 k$k$，然后随机选定 k$k$ 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 k$k$ 个子集，即将数据划分为 k$k$ 类。</p><p>而评估划分的好坏标准就是：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。</p><a id="more"></a><h2 id="K-Means-聚类方法"><a href="#K-Means-聚类方法" class="headerlink" title="K-Means 聚类方法"></a>K-Means 聚类方法</h2><p>在划分聚类中，K-Means 是最具有代表性的算法，下面用图片的方式演示 K-Means 的基本算法流程。希望大家能通过简单的图文演示，对 K-Means 方法的原理过程产生大致的印象。</p><p><strong>[1] 对于未聚类数据集，首先随机初始化 K 个（代表拟聚类簇个数）中心点，如图红色五角星所示。</strong></p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/1.png"></a></p><p><strong>[2] 每一个样本按照距离自身最近的中心点进行聚类，等效于通过两中心点连线的中垂线划分区域。</strong></p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/2.png"></a></p><p><strong>[3] 依据上次聚类结果，移动中心点到个簇的质心位置，并将此质心作为新的中心点</strong></p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/3.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/3.png"></a></p><p><strong>[4] 反复迭代，直至中心点的变化满足收敛条件（变化很小或几乎不变化），最终得到聚类结果。</strong></p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/4.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/4.png"></a></p><p>在对 K-Means 有了一个直观了解后，下面我们用 Python 来进行实现。</p><h3 id="生成示例数据"><a href="#生成示例数据" class="headerlink" title="生成示例数据"></a><a href="#%E7%94%9F%E6%88%90%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE" title="生成示例数据"></a>生成示例数据</h3><p>首先通过 <code>scikit-learn</code> 模块的 <code>make_blobs()</code> 函数生成本次实验所需的示例数据。该方法可以按照我们的要求，生成特定的团状数据。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data,label &#x3D; sklearn.datasets.make\_blobs(n\_samples&#x3D;100,n\_features&#x3D;2,centers&#x3D;3,center\_box&#x3D;(-10.0,10.0),random\_state&#x3D;None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中参数为：</p><ul><li><code>n_samples</code>：表示生成数据总个数,默认为 100 个。</li><li><code>n_features</code>：表示每一个样本的特征个数，默认为 2 个。</li><li><code>centers</code>：表示中心点的个数，默认为 3 个。</li><li><code>center_box</code>：表示每一个中心的边界,默认为 -10.0到10.0。</li><li><code>random_state</code>：表示生成数据的随机数种子。</li></ul><p>返回值为：</p><ul><li><code>data</code>：表示数据信息。</li><li><code>label</code>：表示数据类别。</li></ul><p>根据上面函数，在 0.0 到 10.0 上生成 200 条数据，大致包含 3 个中心。由于是用于演示聚类效果，数据标签就不是必须的了，在生成数据时赋值给 <code>_</code>，后面也不会使用到。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;构造数据  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">from sklearn.datasets import make\_blobs  </span><br><span class="line">  </span><br><span class="line">blobs, \_ &#x3D; make\_blobs(n\_samples&#x3D;200, centers&#x3D;3, random\_state&#x3D;18)  </span><br><span class="line">blobs\[:10\] # 打印出前 10 条数据的信息  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[ 8.28390539,  4.98011149],</span><br><span class="line">       [ 7.05638504,  7.00948082],</span><br><span class="line">       [ 7.43101466, -6.56941148],</span><br><span class="line">       [ 8.20192526, -6.4442691 ],</span><br><span class="line">       [ 3.15614247,  0.46193832],</span><br><span class="line">       [ 7.7037692 ,  6.14317389],</span><br><span class="line">       [ 5.62705611, -0.35067953],</span><br><span class="line">       [ 7.53828533, -4.86595492],</span><br><span class="line">       [ 8.649291  ,  3.98488194],</span><br><span class="line">       [ 7.91651636,  4.54935348]]) </span><br></pre></td></tr></table></figure><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a><a href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96" title="数据可视化"></a>数据可视化</h3><p>为了更加直观的查看数据分布情况，使用 <code>matplotlib</code> 将生成数据绘画出来。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;数据展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">%matplotlib inline  </span><br><span class="line">import matplotlib.pyplot as plt  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20);  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_21_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_21_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd80877b8&gt;"></a></p><h3 id="随机初始化中心点"><a href="#随机初始化中心点" class="headerlink" title="随机初始化中心点"></a><a href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%AD%E5%BF%83%E7%82%B9" title="随机初始化中心点"></a>随机初始化中心点</h3><p>当我们得到数据时，依照划分聚类方法的思想，首先需要随机选取 k$k$ 个点作为每一个子集的中心点。从图像中，通过肉眼很容易的发现该数据集有 <code>3</code> 个子集。接下来，用 <code>numpy</code> 模块随机生成 <code>3</code> 个中心点，为了更方便展示，这里我们加入了随机数种子以便每一次运行结果相同。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;初始化中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import numpy as np  </span><br><span class="line">  </span><br><span class="line">def random\_k(k, data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    init\_centers -- 初始化中心点  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    prng &#x3D; np.random.RandomState(27) # 定义随机种子  </span><br><span class="line">    num\_feature&#x3D;np.shape(data)\[1\]  </span><br><span class="line">    init\_centers &#x3D; prng.randn(k, num\_feature)\*5 # 由于初始化的随机数是从-1到1，为了更加贴近数据集这里乘了一个 5  </span><br><span class="line">    return init\_centers  </span><br><span class="line">  </span><br><span class="line">init\_centers&#x3D;random\_k(3, blobs)  </span><br><span class="line">init\_centers  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 6.42802708, -1.51776689],</span><br><span class="line">       [ 3.09537831,  1.97999275],</span><br><span class="line">       [ 1.11702824, -0.27169709]]) </span><br></pre></td></tr></table></figure><p>在随机生成好中心点之后，将其在图像中表示出来，这里同样使用红色五角星表示。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;初始中心点展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20);  </span><br><span class="line">plt.scatter(init\_centers\[:,0\], init\_centers\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_26_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_26_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd849b128&gt;"></a></p><h3 id="计算样本与中心点的距离"><a href="#计算样本与中心点的距离" class="headerlink" title="计算样本与中心点的距离"></a><a href="#%E8%AE%A1%E7%AE%97%E6%A0%B7%E6%9C%AC%E4%B8%8E%E4%B8%AD%E5%BF%83%E7%82%B9%E7%9A%84%E8%B7%9D%E7%A6%BB" title="计算样本与中心点的距离"></a>计算样本与中心点的距离</h3><p>为了找到最合适的中心点位置，需要计算每一个样本和中心点的距离，从而根据距离更新中心点位置。常见的距离计算方法有欧几里得距离和余弦相似度，本实验采用更常见且更易于理解的欧几里得距离（欧式距离）。</p><p>欧式距离源自 N$N$ 维欧氏空间中两点之间的距离公式。表达式如下:</p><p>deuc= ⎷N∑i=1(Xi−Yi)2(1)</p><p>$$(1)deuc=∑i=1N(Xi−Yi)2$$</p><p>其中：</p><ul><li>X$X$, Y$Y$ ：两个数据点</li><li>N$N$：每个数据中有 N$N$ 个特征值，</li><li>Xi$Xi$ ：数据 X$X$ 的第 i$i$ 个特征值</li></ul><p>将两个数据 X$X$ 和 Y$Y$ 中的每一个对应的特征值之间差值的平方，再求和，最后开平方，便是欧式距离。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算欧氏距离  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def d\_euc(x, y):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    x -- 数据 a  </span><br><span class="line">    y -- 数据 b  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    d -- 数据 a 和 b 的欧氏距离  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    d &#x3D; np.sqrt(np.sum(np.square(x - y)))  </span><br><span class="line">    return d  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="最小化-SSE，更新聚类中心"><a href="#最小化-SSE，更新聚类中心" class="headerlink" title="最小化 SSE，更新聚类中心"></a><a href="#%E6%9C%80%E5%B0%8F%E5%8C%96-SSE%EF%BC%8C%E6%9B%B4%E6%96%B0%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83" title="最小化 SSE，更新聚类中心"></a>最小化 SSE，更新聚类中心</h3><p>和第一章的回归算法通过减小目标函数（如：损失函数）的值拟合数据集一样，聚类算法通常也是优化一个目标函数，从而提高聚类的质量。在聚类算法中，常常使用误差的平方和 SSE（Sum of squared errors）作为度量聚类效果的标准，当 SSE 越小表示聚类效果越好。其中 SSE 表示为：</p><p>SSE(C)=K∑k=1∑xi∈Ck|xi−ck|2(2)</p><p>$$(2)SSE(C)=∑k=1K∑xi∈Ck|xi−ck|2$$</p><p>其中数据集 D={x1,x2,…,xn}$D={x1,x2,…,xn}$，xi$xi$表示每一个样本值，C$C$ 表示通过 K-Means 聚类分析后的产生类别集合 C={C1,C2,…,CK}$C={C1,C2,…,CK}$ ，ck$ck$ 是类别 Ck$Ck$ 的中心点，其中 ck$ck$ 计算方式为：</p><p>ck=∑xi∈CkxiI(Ck)(3)</p><p>$$(3)ck=∑xi∈CkxiI(Ck)$$</p><p>I(Ck)$I(Ck)$ 表示在第 k$k$ 个集合 Ck$Ck$ 中数据的个数。</p><p>当然，我们希望同最小化损失函数一样，最小化 SSE 函数，从而找出最优化的聚类模型，但是求其最小值并不容易，是一个 NP 难（非确定性多项式）的问题，其中 NP 难问题是一个经典图论问题，至今也没有找到一个完美且有效的算法。</p><p>下面我们对中心点的更新用代码的方式进行实现：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;中心点的更新  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def update\_center(clusters, data, centers):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    clusters -- 每一点分好的类别  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    centers -- 中心点集合  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    new\_centers.reshape(num\_centers,num\_features) -- 新中心点集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">    num\_centers &#x3D; np.shape(centers)\[0\]  # 中心点的个数  </span><br><span class="line">    num\_features &#x3D; np.shape(centers)\[1\]  # 每一个中心点的特征数  </span><br><span class="line">    container &#x3D; \[\]  </span><br><span class="line">    for x in range(num\_centers):  </span><br><span class="line">        each\_container &#x3D; \[\]  </span><br><span class="line">        container.append(each\_container)  # 首先创建一个容器,将相同类别数据存放到一起  </span><br><span class="line">  </span><br><span class="line">    for i, cluster in enumerate(clusters):  </span><br><span class="line">        container\[cluster\].append(data\[i\])  </span><br><span class="line">  </span><br><span class="line">    # 为方便计算，将 list 类型转换为 np.array 类型  </span><br><span class="line">    container &#x3D; np.array(list(map(lambda x: np.array(x), container)))  </span><br><span class="line">  </span><br><span class="line">    new\_centers &#x3D; np.array(\[\])  # 创建一个容器，存放中心点的坐标  </span><br><span class="line">    for i in range(len(container)):  </span><br><span class="line">        each\_center &#x3D; np.mean(container\[i\], axis&#x3D;0)  # 计算每一子集中数据均值作为中心点  </span><br><span class="line">        new\_centers &#x3D; np.append(new\_centers, each\_center)  </span><br><span class="line">  </span><br><span class="line">    return new\_centers.reshape(num\_centers, num\_features)  # 以矩阵的方式返回中心点坐标  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="K-Means-聚类算法实现"><a href="#K-Means-聚类算法实现" class="headerlink" title="K-Means 聚类算法实现"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="K-Means 聚类算法实现"></a>K-Means 聚类算法实现</h3><p>K-Means 算法则采用的是迭代算法，避开优化 SSE 函数，通过不断移动中心点的距离，最终达到聚类的效果。</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><a href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B" title="算法流程"></a>算法流程</h4><ol><li>初始化中心点：判断数据集可能被分为 k$k$ 个子集，随机生成 k$k$ 个随机点作为每一个子集的中心点。</li><li>距离计算，类别标记：样本和每一个中心点进行距离计算，将距离最近的中心点所代表的类别标记为该样本的类别。</li><li>中心点位置更新：计算每一个类别中的所有样本的均值，作为新的中心点位置。</li><li>重复 2，3 步骤，直到中心点位置不再变化。</li></ol><h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a><a href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="算法实现"></a>算法实现</h4><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line">33  </span><br><span class="line">34  </span><br><span class="line">35  </span><br><span class="line">36  </span><br><span class="line">37  </span><br><span class="line">38  </span><br><span class="line">39  </span><br><span class="line">40  </span><br><span class="line">41  </span><br><span class="line">42  </span><br><span class="line">43  </span><br><span class="line">44  </span><br><span class="line">45  </span><br><span class="line">46  </span><br><span class="line">47  </span><br><span class="line">48  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;K-Means 聚类  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def kmeans\_cluster(data, init\_centers, k):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    init\_centers -- 初始化中心点集合  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    centers\_container -- 每一次更新中心点的集合  </span><br><span class="line">    cluster\_container -- 每一次更新类别的集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    max\_step &#x3D; 50  # 定义最大迭代次数，中心点最多移动的次数。  </span><br><span class="line">    epsilon &#x3D; 0.001  # 定义一个足够小的数，通过中心点变化的距离是否小于该数，判断中心点是否变化。  </span><br><span class="line">  </span><br><span class="line">    old\_centers &#x3D; init\_centers  </span><br><span class="line">  </span><br><span class="line">    centers\_container &#x3D; \[\]  # 建立一个中心点容器，存放每一次变化后的中心点，以便后面的绘图。  </span><br><span class="line">    cluster\_container &#x3D; \[\]  # 建立一个分类容器，存放每一次中心点变化后数据的类别  </span><br><span class="line">    centers\_container.append(old\_centers)  </span><br><span class="line">  </span><br><span class="line">    for step in range(max\_step):  </span><br><span class="line">        cluster &#x3D; np.array(\[\], dtype&#x3D;int)  </span><br><span class="line">        for each\_data in data:  </span><br><span class="line">            distances &#x3D; np.array(\[\])  </span><br><span class="line">            for each\_center in old\_centers:  </span><br><span class="line">                temp\_distance &#x3D; d\_euc(each\_data, each\_center)  # 计算样本和中心点的欧式距离  </span><br><span class="line">                distances &#x3D; np.append(distances, temp\_distance)  </span><br><span class="line">            lab &#x3D; np.argmin(distances)  # 返回距离最近中心点的索引，即按照最近中心点分类  </span><br><span class="line">            cluster &#x3D; np.append(cluster, lab)  </span><br><span class="line">        cluster\_container.append(cluster)  </span><br><span class="line">  </span><br><span class="line">        new\_centers &#x3D; update\_center(cluster, data, old\_centers)  # 根据子集分类更新中心点  </span><br><span class="line">  </span><br><span class="line">        # 计算每个中心点更新前后之间的欧式距离  </span><br><span class="line">        difference &#x3D; \[\]  </span><br><span class="line">        for each\_old\_center, each\_new\_center in zip(old\_centers, new\_centers):  </span><br><span class="line">            difference.append(d\_euc(each\_old\_center, each\_new\_center))  </span><br><span class="line">          </span><br><span class="line">        if (np.array(difference) &lt; epsilon).all():  # 判断每个中心点移动是否均小于 epsilon  </span><br><span class="line">            return centers\_container, cluster\_container  </span><br><span class="line">  </span><br><span class="line">        centers\_container.append(new\_centers)  </span><br><span class="line">        old\_centers &#x3D; new\_centers  </span><br><span class="line">  </span><br><span class="line">    return centers\_container, cluster\_container  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>完成 K-Means 聚类函数后，接下来用函数得到最终中心点的位置。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算最终中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">centers\_container, cluster\_container &#x3D; kmeans\_cluster(blobs, init\_centers, 3)  </span><br><span class="line">final\_center &#x3D; centers\_container\[-1\]  </span><br><span class="line">final\_cluster &#x3D; cluster\_container\[-1\]  </span><br><span class="line">final\_center  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 7.67007252, -6.44697348],</span><br><span class="line">       [ 6.83832746,  4.98604668],</span><br><span class="line">       [ 3.28477676,  0.15456871]]) </span><br></pre></td></tr></table></figure><p>最后，我们把聚类得到的中心绘制到原图中看一看聚类效果。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;可视化展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;final\_cluster);  </span><br><span class="line">plt.scatter(final\_center\[:,0\], final\_center\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_50_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_50_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbd89e8438&gt;"></a></p><h3 id="中心点移动过程可视化"><a href="#中心点移动过程可视化" class="headerlink" title="中心点移动过程可视化"></a><a href="#%E4%B8%AD%E5%BF%83%E7%82%B9%E7%A7%BB%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96" title="中心点移动过程可视化"></a>中心点移动过程可视化</h3><p>截止上小节，已经完成了 K-Means 聚类的流程。为了帮助大家理解，我们尝试将 K-Means 聚类过程中，中心点移动变化的过程绘制出来。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num\_axes &#x3D; len(centers\_container)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(1, num\_axes, figsize&#x3D;(20, 4))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[0\])  </span><br><span class="line">axes\[0\].scatter(init\_centers\[:, 0\], init\_centers\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[0\].set\_title(&quot;initial center&quot;)  </span><br><span class="line">  </span><br><span class="line">for i in range(1, num\_axes-1):  </span><br><span class="line">    axes\[i\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[i\])  </span><br><span class="line">    axes\[i\].scatter(centers\_container\[i\]\[:, 0\],  </span><br><span class="line">                    centers\_container\[i\]\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">    axes\[i\].set\_title(&quot;step &#123;&#125;&quot;.format(i))  </span><br><span class="line">  </span><br><span class="line">axes\[-1\].scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;cluster\_container\[-1\])  </span><br><span class="line">axes\[-1\].scatter(final\_center\[:, 0\], final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[-1\].set\_title(&quot;final center&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_53_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_53_1.png" alt="Text(0.5, 1.0, &#39;final center&#39;)">Text(0.5, 1.0, ‘final center’)</a></p><p>你会惊讶的发现，对于示例数据集，虽然我们先前将最大迭代次数 <code>max_step</code> 设为了 <code>50</code>，但实际上 K-Means 迭代 3 次即收敛。原因主要有 2 点：</p><ul><li>初始化中心点的位置很好，比较均匀分布在了数据范围中。如果初始化中心点集中分布在某一角落，迭代次数肯定会增加。</li><li>示例数据分布规整和简单，使得无需迭代多次就能收敛。</li></ul><h3 id="K-Means-算法聚类中的-K-值选择"><a href="#K-Means-算法聚类中的-K-值选择" class="headerlink" title="K-Means 算法聚类中的 K 值选择"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E8%81%9A%E7%B1%BB%E4%B8%AD%E7%9A%84-K-%E5%80%BC%E9%80%89%E6%8B%A9" title="K-Means 算法聚类中的 K 值选择"></a>K-Means 算法聚类中的 K 值选择</h3><p>不知道你是否还记得，前面在学习分类算法 K-近邻的时候，我们讲到了 K 值的选择。而在使用 K-Means 算法聚类时，由于要提前确定随机初始化中心点的数量，同样面临着 K 值选择问题。</p><p>在前面寻找 K 值时，我们通过肉眼观察认为应该聚为 3 类。那么，如果我们设定聚类为 5 类呢？</p><p>这一次，我们尝试通过 <code>scikit-learn</code> 模块中的 K-Means 算法完成聚类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import k\_means  </span><br><span class="line">  </span><br><span class="line">k\_means(X, n\_clusters)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中参数为：</p><ul><li><code>X</code>：表示需要聚类的数据。</li><li><code>n_clusters</code>：表示聚类的个数，也就是 K 值。</li></ul><p>返回值包含：</p><ul><li><code>centroid</code>：表示中心点坐标。</li><li><code>label</code>：表示聚类后每一个样本的类别。</li><li><code>inertia</code>：每一个样本与最近中心点距离的平方和，即 SSE。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;用 scikit-learn 聚类并绘图  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">from sklearn.cluster import k\_means  </span><br><span class="line">model &#x3D; k\_means(blobs, n\_clusters&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">centers &#x3D; model\[0\]  </span><br><span class="line">clusters\_info &#x3D; model\[1\]  </span><br><span class="line">plt.scatter(blobs\[:, 0\], blobs\[:, 1\], s&#x3D;20, c&#x3D;clusters\_info)  </span><br><span class="line">plt.scatter(centers\[:, 0\], centers\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_60_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_60_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdab66e10&gt;"></a></p><p>从图片上来看，聚为 5 类效果明显不如聚为 3 类的好。当然，我们提前用肉眼就能看出数据大致为 3 团。</p><p>实际的应用过程中，如果通过肉眼无法判断数据应该聚为几类？或者是高维数据无法可视化展示。面对这样的情况，我们就要从数值计算的角度去判断 K 值的大小。</p><p><strong>接下来，将介绍一种启发式学习算法，被称之为 肘部法则，可以帮助我们选取 K 值。</strong></p><p>使用 K-Means 算法聚类时，我们可以计算出按不同 K 值聚类后，每一个样本距离最近中心点距离的平方和 SSE。</p><p>随着 K 值增加时，也就是类别增加时，每个类别中的类内相似性也随之增加，由此造成的 SSE 的变化是单调减小的。可以想象一下，聚类类别的数量和样本的总数相同时，也就是说一个样本就代表一个类别时，这个数值会变成 0。</p><p>下面我们通过代码将不同的数量的聚类下，样本和最近中心点的距离和绘制出来。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">index &#x3D; \[\] # 横坐标数组  </span><br><span class="line">inertia &#x3D; \[\] # 纵坐标数组  </span><br><span class="line">  </span><br><span class="line">\# K 从 1~ 6 聚类  </span><br><span class="line">for i in range(6):  </span><br><span class="line">    model &#x3D; k\_means(blobs, n\_clusters&#x3D;i + 1)  </span><br><span class="line">    index.append(i + 1)  </span><br><span class="line">    inertia.append(model\[2\])  </span><br><span class="line">  </span><br><span class="line">\# 绘制折线图  </span><br><span class="line">plt.plot(index, inertia, &quot;-o&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_63_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_63_1.png" alt="&lt;matplotlib.lines.Line2D at 0x2dbdac2fdd8&gt;"></a></p><p>通过上图可以看到，和预想的一样，样本距离最近中心点距离的总和会随着 K 值的增大而降低。</p><p>现在，回想本实验划分聚类中所讲评估划分的好坏标准：「保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大」。</p><p>当 K 值越大时，越满足「同一划分的样本之间的差异尽可能的小」。而当 K 值越小时，越满足「不同划分中的样本差异尽可能的大畸变程度最大」。那么如何做到两端的平衡呢？</p><p>于是，<strong>我们通过 SSE 所绘制出来的图，将畸变程度最大的点称之为「肘部」</strong>。从图中可以看到，这里的「肘部」是 K = 3（内角最小，弯曲度最大）。这也说明，将样本聚为 3 类是最佳选择（K = 2 比较接近）。这就是所谓的「肘部法则」，你明白了吗？</p><h2 id="K-Means-聚类算法"><a href="#K-Means-聚类算法" class="headerlink" title="K-Means++ 聚类算法"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95" title="K-Means++ 聚类算法"></a>K-Means++ 聚类算法</h2><h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a><a href="#%E9%97%AE%E9%A2%98%E5%BC%95%E5%85%A5" title="问题引入"></a>问题引入</h3><p>随着数据量的增长，分类数目增多时，由于 K-Means 中初始化中心点是随机的，常常会出现：一个较大子集有多个中心点，而其他多个较小子集公用一个中心点的问题。即算法陷入局部最优解而不是达到全局最优解的问题。</p><p>造成这种问题主要原因就是：一部分中心点在初始化时离的太近。下面我们通过例子来进一步了解。</p><h3 id="生成示例数据-1"><a href="#生成示例数据-1" class="headerlink" title="生成示例数据"></a><a href="#%E7%94%9F%E6%88%90%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE-1" title="生成示例数据"></a>生成示例数据</h3><p>同样，我们先使用 <code>scikit-learn</code> 模块的 <code>make_blobs</code> 函数生成本次实验所需数据，本次生成 <code>800</code> 条数据，共 5 堆。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;生成数据并展示  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">blobs\_plus, \_ &#x3D; make\_blobs(n\_samples&#x3D;800, centers&#x3D;5, random\_state&#x3D;18)  # 生成数据  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20)  # 将数据可视化展示  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_71_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_71_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdae88278&gt;"></a></p><h3 id="随机初始化中心点-1"><a href="#随机初始化中心点-1" class="headerlink" title="随机初始化中心点"></a><a href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%AD%E5%BF%83%E7%82%B9-1" title="随机初始化中心点"></a>随机初始化中心点</h3><p>从数据点分布中可以很容易的观测出聚类数量应该为 5 类，我们先用 K-Means 中随机初始中心点的方法完成聚类：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">km\_init\_center&#x3D;random\_k(5, blobs\_plus)  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20);  </span><br><span class="line">plt.scatter(km\_init\_center\[:,0\], km\_init\_center\[:,1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_74_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_74_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdabfea90&gt;"></a></p><h3 id="K-Means-聚类"><a href="#K-Means-聚类" class="headerlink" title="K-Means 聚类"></a><a href="#K-Means-%E8%81%9A%E7%B1%BB" title="K-Means 聚类"></a>K-Means 聚类</h3><p>用传统的 K-Means 算法，将数据集进行聚类，聚类数量为 5。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">km\_centers, km\_clusters &#x3D; kmeans\_cluster(blobs\_plus, km\_init\_center, 5)  </span><br><span class="line">km\_final\_center &#x3D; km\_centers\[-1\]  </span><br><span class="line">km\_final\_cluster &#x3D; km\_clusters\[-1\]  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;km\_final\_cluster)  </span><br><span class="line">plt.scatter(km\_final\_center\[:, 0\], km\_final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_77_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_77_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb2a92b0&gt;"></a></p><p>通过传统 K-Means 算法聚类后，你会发现聚类效果和我们预想不同，我们预想的结果应该是下面这样的：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;\_)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_79_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_79_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb2e6978&gt;"></a></p><p>对比 K-Means 聚类和预想聚类的两张图，可以直观的看出 K-Means 算法显然没有达到最优的聚类效果，出现了本章开头所提到的局部最优解的问题。</p><p>对于局部最优问题是可以通过 SSE 来解决的，即在同一数据集上运行多次 K-Means 算法聚类，之后选取 SSE 最小的那次作为最终的聚类结果。虽然通过 SSE 找到最优解十分困难，但通过 SSE 判断最优解是十分容易的。</p><p>但当遇到更大的数据集，每一次 K-Means 算法会花费大量时间时，如果使用多次运行通过 SSE 来判断最优解，显然不是好的选择。是否有一种方法在初始化中心点时，就能有效避免局部最优问题的出现呢？</p><p>在 K-Means 的基础上，D.Arthur 等人在 2007 年提出了 K-Means++ 算法。其中 K-Means++ 算法主要针对初始化中心点问题进行改进，这样就可以从源头上解决局部最优解的问题。</p><h3 id="K-Means-算法流程"><a href="#K-Means-算法流程" class="headerlink" title="K-Means++ 算法流程"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B" title="K-Means++ 算法流程"></a>K-Means++ 算法流程</h3><p>K-Means++ 相较于 K-Means 在初始化中心点上做了改进，在其他方面和 K-Means 相同。</p><ol><li>在数据集中随机选择一个样本点作为第一个初始化的聚类中心。</li><li>计算样本中的非中心点与最近中心点之间的距离 D(x)$D(x)$ 并保存于一个数组里，将数组中的这些距离加起来得到 Sum(D(x))$Sum(D(x))$。</li><li>取一个落在 Sum(D(x))$Sum(D(x))$范围中的随机值 R$R$ ，重复计算 R=R−D(x)$R=R−D(x)$ 直至得到 R≤0$R≤0$ ，选取此时的点作为下一个中心点。</li><li>重复 2,3 步骤，直到 K$K$ 个聚类中心都被确定。</li><li>对 K$K$ 个初始化的聚类中心，利用 K-Means 算法计算最终的聚类中心。</li></ol><p>看完整个算法流程，可能会出现一个疑问：为避免初始点距离太近，直接选取距离最远的点不就好了，为什么要引入一个随机值 R$R$ 呢？</p><p>其实当采用直接选取距离最远的点作为初始点的方法，会容易受到数据集中离群点的干扰。采用引入随机值 R$R$ 的方法避免数据集中所包含的离群点对算法思想中要选择相距最远的中心点的目标干扰。</p><p>相对于正常的数据点，离群点所计算得出的 D(x)$D(x)$ 距离一定比较大，这样在选取的过程中，它被选中的概率也就相对较大，但是离群点在整个数据集中只占一小部分，大部分依然是正常的点，这样离群点由于距离大而造成的概率大，就被正常点的数量大给平衡掉。从而保证了整个算法的平衡性。</p><h3 id="K-Means-算法实现"><a href="#K-Means-算法实现" class="headerlink" title="K-Means++ 算法实现"></a><a href="#K-Means-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="K-Means++ 算法实现"></a>K-Means++ 算法实现</h3><p>K-Means++ 在初始化样本点之后，计算其他样本与其最近的中心点距离之和，以备下一个中心点的选择，下面用 Python 来进行实现：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def get\_sum\_dis(centers, data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    centers -- 中心点集合  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    np.sum(dis\_container) -- 样本距离最近中心点的距离之和  </span><br><span class="line">    dis\_container -- 样本距离最近中心点的距离集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    dis\_container &#x3D; np.array(\[\])  </span><br><span class="line">    for each\_data in data:  </span><br><span class="line">        distances &#x3D; np.array(\[\])  </span><br><span class="line">        for each\_center in centers:  </span><br><span class="line">            temp\_distance &#x3D; d\_euc(each\_data, each\_center)  # 计算样本和中心点的欧式距离  </span><br><span class="line">            distances &#x3D; np.append(distances, temp\_distance)  </span><br><span class="line">        lab &#x3D; np.min(distances)  </span><br><span class="line">        dis\_container &#x3D; np.append(dis\_container, lab)  </span><br><span class="line">    return np.sum(dis\_container), dis\_container  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>接下来，我们初始化中心点：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;K-Means++ 初始化中心点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">def get\_init\_center(data, k):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    k -- 中心点个数  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    np.array(center\_container) -- 初始化中心点集合  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">      </span><br><span class="line">    seed &#x3D; np.random.RandomState(20)  </span><br><span class="line">    p &#x3D; seed.randint(0, len(data))  </span><br><span class="line">    first\_center &#x3D; data\[p\]  </span><br><span class="line">  </span><br><span class="line">    center\_container &#x3D; \[\]  </span><br><span class="line">    center\_container.append(first\_center)  </span><br><span class="line">  </span><br><span class="line">    for i in range(k-1):  </span><br><span class="line">        sum\_dis, dis\_con &#x3D; get\_sum\_dis(center\_container, data)  </span><br><span class="line">        r &#x3D; np.random.randint(0, sum\_dis)  </span><br><span class="line">        for j in range(len(dis\_con)):  </span><br><span class="line">            r &#x3D; r - dis\_con\[j\]  </span><br><span class="line">            if r &lt;&#x3D; 0:  </span><br><span class="line">                center\_container.append(data\[j\])  </span><br><span class="line">                break  </span><br><span class="line">            else:  </span><br><span class="line">                pass  </span><br><span class="line">  </span><br><span class="line">    return np.array(center\_container)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>实现 K-Means++ 初始化中心点函数之后，根据生成数据，得到初始化的中心点坐标。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plus\_init\_center &#x3D; get\_init\_center(blobs\_plus, 5)  </span><br><span class="line">plus\_init\_center  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[ 4.1661903 ,  0.81807492],</span><br><span class="line">       [ 8.9161603 ,  5.58757202],</span><br><span class="line">       [ 7.62699601,  2.3492678 ],</span><br><span class="line">       [-3.42049424, -9.57117787],</span><br><span class="line">       [ 3.35681598, -0.54000802]]) </span><br></pre></td></tr></table></figure><p>为了让你更清晰的看到 K-Means++ 初始化中心点的过程，我们用 <code>matplotlib</code> 进行展示。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num &#x3D; len(plus\_init\_center)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(1, num, figsize&#x3D;(25, 4))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">axes\[0\].scatter(plus\_init\_center\[0, 0\], plus\_init\_center\[0, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[0\].set\_title(&quot;first center&quot;)  </span><br><span class="line">  </span><br><span class="line">for i in range(1, num):  </span><br><span class="line">    axes\[i\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">    axes\[i\].scatter(plus\_init\_center\[:i+1, 0\],  </span><br><span class="line">                    plus\_init\_center\[:i+1, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">    axes\[i\].set\_title(&quot;step&#123;&#125;&quot;.format(i))  </span><br><span class="line">  </span><br><span class="line">axes\[-1\].scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;&quot;b&quot;)  </span><br><span class="line">axes\[-1\].scatter(plus\_init\_center\[:, 0\], plus\_init\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line">axes\[-1\].set\_title(&quot;final center&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_96_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_96_1.png" alt="Text(0.5, 1.0, &#39;final center&#39;)">Text(0.5, 1.0, ‘final center’)</a></p><p>通过上图可以看到点的变化，即除了最初随机选择点之外，之后的每一个点都是尽可能选择远一些的点。这样就很好的保证初始中心点的分散。</p><p>通过多次执行代码可以看到，使用 K-Means++ 同样可能出现两个中心点较近的情况，因此，在极端情况也可能出现局部最优的问题。但相比于 K-Means 算法的随机选取，K-Means++ 的初始化中心点会在很大程度上降低局部最优问题出现的概率。</p><p>在通过 K-Means++ 算法初始化中心点后，下面我们通过 K-Means 算法对数据进行聚类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plus\_centers, plus\_clusters &#x3D; kmeans\_cluster(blobs\_plus, plus\_init\_center, 5)  </span><br><span class="line">plus\_final\_center &#x3D; plus\_centers\[-1\]  </span><br><span class="line">plus\_final\_cluster &#x3D; plus\_clusters\[-1\]  </span><br><span class="line">  </span><br><span class="line">plt.scatter(blobs\_plus\[:, 0\], blobs\_plus\[:, 1\], s&#x3D;20, c&#x3D;plus\_final\_cluster)  </span><br><span class="line">plt.scatter(plus\_final\_center\[:, 0\], plus\_final\_center\[:, 1\], s&#x3D;100, marker&#x3D;&#39;\*&#39;, c&#x3D;&quot;r&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_100_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_100_1.png" alt="&lt;matplotlib.collections.PathCollection at 0x2dbdb3e3630&gt;"></a></p><p>在 K-Means++ 算法中，我们依旧无法完全避免随机选择中心点带来的不稳定性，所以偶尔也会得到不太好的结果。当然，K-Means++ 算法得到不太好的聚类的概率远小于 K-Means 算法。所以，如果你并没有得到一个较好的聚类效果，可以再次初始化中心点尝试。</p><h2 id="Mini-Batch-K-Means-聚类算法"><a href="#Mini-Batch-K-Means-聚类算法" class="headerlink" title="Mini-Batch K-Means 聚类算法"></a><a href="#Mini-Batch-K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95" title="Mini-Batch K-Means 聚类算法"></a>Mini-Batch K-Means 聚类算法</h2><p>在「大数据」如此火的时代，K-Means 算法是否还能一如既往优秀的处理大数据呢？现在我们重新回顾下 K-Means 的算法原理：首先，计算每一个样本同所有中心点的距离，通过比较找到最近的中心点，将距离最近中心点的距离进行存储并归类。然后通过相同类别样本的特征值，更新中心点的位置。至此完成一次迭代，经过多次迭代后最终进行聚类。</p><p>通过上面的表述，你是否感觉到不断计算距离的过程，涉及到的计算量有多大呢？那么，设想一下数据量达到十万，百万，千万级别，且如果每一条数据有上百个特征，这将会消耗大量的计算资源。</p><p>为了解决大规模数据的聚类问题，我们就可以使用 K-Means 的另外一个变种 Mini Batch K-Means 来完成。</p><p>其算法原理也十分简单：在每一次迭代过程中，从数据集中随机抽取一部分数据形成小批量数据集，用该部分数据集进行距离计算和中心点的更新。由于每一次都是随机抽取，所以每一次抽取的数据能很好的表现原本数据集的特性。</p><p>下面，我们生成一组测试数据，并测试 K-Means 算法和 Mini Batch K-Means 在同一组数据上聚类时间和 SSE 上的差异。由于 scikit-learn 中 <code>MiniBatchKMeans()</code> 和 <code>KMeans()</code> 方法的参数几乎一致，这里就不再赘述了。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import time  </span><br><span class="line">from sklearn.cluster import MiniBatchKMeans, KMeans  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">test\_data, \_ &#x3D; make\_blobs(2000, n\_features&#x3D;2, cluster\_std&#x3D;2, centers&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">km &#x3D; KMeans(n\_clusters&#x3D;5)  </span><br><span class="line">mini\_km &#x3D; MiniBatchKMeans(n\_clusters&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">fig, axes &#x3D; plt.subplots(nrows&#x3D;1, ncols&#x3D;2, figsize&#x3D;(10, 5))  </span><br><span class="line">  </span><br><span class="line">for i, model in enumerate(\[km, mini\_km\]):  </span><br><span class="line">    t0 &#x3D; time.time()  </span><br><span class="line">    model.fit(test\_data)  </span><br><span class="line">    t1 &#x3D; time.time()  </span><br><span class="line">    t &#x3D; t1 - t0  </span><br><span class="line">    sse &#x3D; model.inertia\_  </span><br><span class="line">    axes\[i\].scatter(test\_data\[:, 0\], test\_data\[:, 1\], c&#x3D;model.labels\_)  </span><br><span class="line">    axes\[i\].set\_xlabel(&quot;time: &#123;:.4f&#125; s&quot;.format(t))  </span><br><span class="line">    axes\[i\].set\_ylabel(&quot;SSE: &#123;:.4f&#125;&quot;.format(sse))  </span><br><span class="line">  </span><br><span class="line">axes\[0\].set\_title(&quot;K-Means&quot;)  </span><br><span class="line">axes\[1\].set\_title(&quot;Mini Batch K-Means&quot;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_106_1.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-means/output_106_1.png" alt="Text(0.5, 1.0, &#39;Mini Batch K-Means&#39;)">Text(0.5, 1.0, ‘Mini Batch K-Means’)</a></p><p>以上是对 2000 条数据分别用 K-Means 和 Mini Batch K-Means进行聚类，从图像中可以看出，Mini Batch K-Means 在训练时间上明显比 K-Means 快（大于 2 倍不等），且聚类得到的 SSE 值比较接近。</p><hr><p><strong>拓展阅读：</strong></p><ul><li><a href="https://zh.wikipedia.org/zh-hans/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95">K-平均算法- 维基百科</a></li><li><a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">The 5 Clustering Algorithms Data Scientists Need to Know</a></li><li><a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">Visualizing K-Means Clustering</a></li></ul><p># <a href="/tags/cluster/">cluster</a>, <a href="/tags/k-means/">k-means</a>, <a href="/tags/machine-learning/">machine learning</a>, <a href="/tags/python/">python</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。&lt;/p&gt;
&lt;p&gt;在划分的过程中，首先由用户确定划分子集的个数 k$k$，然后随机选定 k$k$ 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 k$k$ 个子集，即将数据划分为 k$k$ 类。&lt;/p&gt;
&lt;p&gt;而评估划分的好坏标准就是：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="划分聚类" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB/"/>
    
    <category term="K-Means" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%92%E5%88%86%E8%81%9A%E7%B1%BB/K-Means/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="K-Means" scheme="http://www.laugh12321.cn/blog/tags/K-Means/"/>
    
    <category term="Cluster" scheme="http://www.laugh12321.cn/blog/tags/Cluster/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|装袋和提升方法详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/07/bagging_and_boosting/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/07/bagging_and_boosting/</id>
    <published>2019-02-07T00:00:00.000Z</published>
    <updated>2020-10-23T09:05:41.297Z</updated>
    
    <content type="html"><![CDATA[<p>前面的文章都是独立的讲解每一个分类器的分类过程，每一个分类器都有其独有的特点并非常适合某些数据。但在实际中，可能没有那样合适的数据，在应用前面的分类器时，可能会出现分类准确率低的问题。为解决这样的问题，集成学习便被提出，利用多个弱分类器结合的方式使得分类准确率提高。本次详细讲解了集成学习中十分经典的几个算法：装袋（Bagging）中的 Bagging tree 和随机森林（Ramdom Forest）以及 提升（Boosting）中的 Adaboost 和梯度提升树（GBDT）。</p><a id="more"></a><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="集成学习概念"><a href="#集成学习概念" class="headerlink" title="集成学习概念"></a>集成学习概念</h3><p>在学习装袋和提升算法之前，先引入一个概念：集成学习。集成学习，顾名思义就是通过构建多个分类器并结合使用来完成学习任务，同时也被称为多分类器系统。其最大的特点就是结合各个弱分类器的长处，从而达到“三个臭皮匠顶个诸葛亮”的效果。</p><p>每一个弱分类器我们将其称作「个体学习器」，集成学习的基本结构就是生成一组个体学习器，再用某种策略将他们结合起来。</p><p>从个体学习器类别来看，集成学习通常分为两种类型：</p><ul><li>「同质」集成，在一个集成学习中，「个体学习器」是同一类型，如 「决策树集成」 所有个体学习器都为决策树，「神经网络集成」所有的个体学习器都为神经网络。</li><li>「异质」集成，在一个集成学习中，「个体学习器」为不同类型，如一个集成学习中可以包含决策树模型也可以包含神经网络模型。</li></ul><p>同样从集成方式来看，集成学习也可以分为两类：</p><ul><li>并行式，当个体学习器之间不存在强依赖关系时，可同时生成并行化方法，其中代表算法为装袋（Bagging）算法。</li><li>串行式，当个体学习器之间存在强依赖关系时，必须串行生成序列化方法，其中代表算法为提升（Boosting）算法。</li></ul><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/1.png"></center><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p>集成学习中，当数据被多个个体学习器学习后，如何最终决定学习结果呢？假定集成包含 T$T$ 个个体学习器 {h1,h2,…,hT}${h1,h2,…,hT}$ 。常用的有三种方法：平均法，投票法，学习法。</p><h4 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a><a href="#%E5%B9%B3%E5%9D%87%E6%B3%95" title="平均法"></a>平均法</h4><p>在数值型输出中，最常用的结合策略为平均法（Averaging），在平均法中有两种方式：</p><p><strong>简单平均法：</strong></p><p>H(x)=1TT∑i=1hi(x)(1)</p><p>$$(1)H(x)=1T∑i=1Thi(x)$$</p><p>取每一个「个体学习器」学习后的平均值。</p><p><strong>加权平均法：</strong></p><p>H(x)=T∑i=1wihi(x)(2)</p><p>$$(2)H(x)=∑i=1Twihi(x)$$</p><p>其中 wi$wi$ 是每一个「个体学习器」 hi$hi$ 的权重，通常为 wi≥0,∑Ti=1wi=1$wi≥0,∑i=1Twi=1$。</p><h4 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a><a href="#%E6%8A%95%E7%A5%A8%E6%B3%95" title="投票法"></a>投票法</h4><p>对于分类输出而言，平均法显然效果不太好，最常用的结合策略为投票法(Voting)，在投票法中主要有三种方式：</p><p><strong>绝对多数投票法：</strong></p><p>H(X)={cj,if∑Ti=1hji&gt;0.5∑Nk=1∑Ti=1hki(x);None,if∑Ti=1hji&lt;=0.5∑Nk=1∑Ti=1hki(x);(3)</p><p>$$(3)H(X)={cj,if∑i=1Thij&gt;0.5∑k=1N∑i=1Thik(x);None,if∑i=1Thij&lt;=0.5∑k=1N∑i=1Thik(x);$$</p><p>简单而言，当某一个输出的分类超过了半数则输出该分类，若未超过半数则不输出分类。</p><p><strong>相对多数投票法：</strong></p><p>H(X)=cargmaxj∑Ti=1hji(x)(4)</p><p>$$(4)H(X)=cargmaxj∑i=1Thij(x)$$</p><p>即在「个体学习器」分类完成后，通过投票选出分类最多的标签作为此次分类的结果。</p><p><strong>加权投票法：</strong></p><p>H(X)=cargmaxj∑Ti=1wihji(x)(5)</p><p>$$(5)H(X)=cargmaxj∑i=1Twihij(x)$$</p><p>同加权平均法类似，wi$wi$ 是每一个「个体学习器」 hi$hi$ 的权重，通常为 wi≥0,∑Ti=1wi=1$wi≥0,∑i=1Twi=1$。</p><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a><a href="#%E5%AD%A6%E4%B9%A0%E6%B3%95" title="学习法"></a>学习法</h4><p>以上两种方法（平均法和投票法）相对比较简单，但是可能学习误差较大，为了解决这种情况，还有一种方法为学习法，其代表方法是 <code>stacking</code> ，当使用<code>stacking</code> 的结合策略时， 不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，即把训练集弱学习器的学习结果作为输入，把训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p><p>在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。</p><h2 id="装袋算法-Bagging"><a href="#装袋算法-Bagging" class="headerlink" title="装袋算法 Bagging"></a><a href="#%E8%A3%85%E8%A2%8B%E7%AE%97%E6%B3%95-Bagging" title="装袋算法 Bagging"></a>装袋算法 Bagging</h2><p>在大致了解集成学习相关概念之后，接下来就是对集成学习中常用算法思想之一的装袋算法进行详细的讲解。</p><h3 id="装袋算法原理"><a href="#装袋算法原理" class="headerlink" title="装袋算法原理"></a><a href="#%E8%A3%85%E8%A2%8B%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" title="装袋算法原理"></a>装袋算法原理</h3><p>装袋算法是并行式集成学习的代表，其原理也比较简单。算法步骤如下：</p><ol><li>数据处理：将数据根据实际情况进行清洗整理。</li><li>随机采样：重复 T 次，每一次从样本中随机选出 T 个子样本。</li><li>个体训练：将每一个子样本放入个体学习器训练。</li><li>分类决策：用投票法集成进行分类决策。</li></ol><h3 id="Bagging-tree"><a href="#Bagging-tree" class="headerlink" title="Bagging tree"></a><a href="#Bagging-tree" title="Bagging tree"></a>Bagging tree</h3><p>在前一节的决策树讲解中提到，决策树是一个十分「完美」的训练器，但特别容易出现过拟合的情况，最终导致预测准确率低的问题。事实上在装袋算法中，决策树常常被用作弱分类器。下面我们通过具体实验来看看决策树和以决策树作为装袋算法的预测效果。</p><h4 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a><a href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD" title="数据加载"></a>数据加载</h4><p>本实验我们使用在上一章讲决策树时所用的学生成绩预测数据集。其中数据处理已在上一章详细说明，本次实验我们使用处理过后的数据集。数据集名称为 <code>course-14-student.csv</code>.</p><p>数据集下载 👉 <a href="http://labfile.oss.aliyuncs.com/courses/1081/course-14-student.csv">传送门</a></p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd  </span><br><span class="line">  </span><br><span class="line">stu\_data &#x3D; pd.read\_csv(&quot;course-14-student.csv&quot;)  </span><br><span class="line">stu\_data.head(10)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/output_00.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/output_00.png"></a></p><h4 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a><a href="#%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86" title="数据划分"></a>数据划分</h4><p>加载好预处理的数据集之后，为了应用装袋算法，我们需要将数据集分为 <strong>训练集</strong>和<strong>测试集</strong>，依照经验：<strong>训练集</strong>占比为 70%，<strong>测试集</strong>占 30%。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model\_selection import train\_test\_split  </span><br><span class="line">  </span><br><span class="line">x\_train, x\_test, y\_train, y\_test &#x3D; train\_test\_split(stu\_data.iloc\[:,:-1\], stu\_data\[&quot;G3&quot;\],  </span><br><span class="line">                                                    test\_size&#x3D;0.3, random\_state&#x3D;35)  </span><br><span class="line">x\_test.head(10)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/output_01.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/output_01.png"></a></p><h4 id="决策树预测"><a href="#决策树预测" class="headerlink" title="决策树预测"></a><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E9%A2%84%E6%B5%8B" title="决策树预测"></a>决策树预测</h4><p>作为比较，首先我们将该数据集用决策树的方式进行预测,使用 <code>scikit-learn</code> 实现决策树预测的用法在前一章节已详细介绍，本实验直接使用。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier  </span><br><span class="line">  </span><br><span class="line">dt\_model &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;entropy&#39;, random\_state&#x3D;34)  </span><br><span class="line">dt\_model.fit(x\_train, y\_train)  # 使用训练集训练模型  </span><br><span class="line">dt\_y\_predict &#x3D; dt\_model.predict(x\_test)  </span><br><span class="line">dt\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([2, 0, 3, 2, 1, 2, 2, 2, 3, 3, 0, 2, 1, 3, 3, 2, 3, 0, 1, 2, 1, 2,</span><br><span class="line">       1, 2, 3, 3, 3, 0, 3, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 2,</span><br><span class="line">       1, 3, 2, 3, 3, 3, 3, 1, 1, 3, 2, 0, 1, 3, 2, 3, 3, 0, 0, 2, 2, 3,</span><br><span class="line">       3, 3, 2, 1, 0, 3, 2, 2, 3, 2, 1, 3, 0, 2, 3, 1, 3, 3, 0, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,</span><br><span class="line">       2, 1, 1, 1, 3, 0, 3, 3, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><p>计算使用决策树预测的准确率。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_accuracy(test\_labels, pred\_labels):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    test\_labels -- 测试集的真实值  </span><br><span class="line">    pred\_labels -- 测试集的预测值  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    accur -- 准确率  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    correct &#x3D; np.sum(test\_labels &#x3D;&#x3D; pred\_labels)  # 计算预测正确的数据个数  </span><br><span class="line">    n &#x3D; len(test\_labels)  # 总测试集数据个数  </span><br><span class="line">    accur &#x3D; correct&#x2F;n  </span><br><span class="line">    return accur  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">get\_accuracy(y\_test, dt\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7899159663865546 </span><br></pre></td></tr></table></figure><p>由于本次实验所采用的数据集特征值比前一章节多，所以决策树泛化能力更差。</p><h4 id="Bagging-Tree-数据模型构建"><a href="#Bagging-Tree-数据模型构建" class="headerlink" title="Bagging Tree 数据模型构建"></a><a href="#Bagging-Tree-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA" title="Bagging Tree 数据模型构建"></a>Bagging Tree 数据模型构建</h4><p>单棵决策树的预测结果并不能使我们满意，下面我们使用 <strong>装袋（Bagging）</strong> 的思想来提高预测准确率。我们通过 <code>scikit-learn</code> 来对 Bagging Tree 算法进行实现。</p><p>在 <code>scikit-learn</code> 中 Bagging tree 常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BaggingClassifier(base\_estimator&#x3D;None, n\_estimators&#x3D;10, max\_samples&#x3D;1.0, max\_features&#x3D;1.0)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>base_estimator</code>：表示基础分类器（弱分类器）种类，默认为决策树 。</li><li><code>n_estimators</code>：表示建立树的个数，默认值为 10 。</li><li><code>max_samples</code>：表示从抽取数据中选取训练样本的数量，int（整型）表示数量，float（浮点型）表示比例，默认为所有样本。</li><li><code>max_features</code>：表示抽取特征的数量，int（整型）表示数量，float（浮点型）表示比例，默认为所有特征。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>：训练。</li><li><code>predict(X)</code>：对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import BaggingClassifier  </span><br><span class="line">  </span><br><span class="line">tree &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;entropy&#39;, random\_state&#x3D;28)  </span><br><span class="line">bag &#x3D; BaggingClassifier(tree, n\_estimators&#x3D;100,  </span><br><span class="line">                        max\_samples&#x3D;1.0, random\_state&#x3D;3)  # 使用决策树  </span><br><span class="line">  </span><br><span class="line">bag.fit(x\_train, y\_train)  </span><br><span class="line">bt\_y\_predict &#x3D; bag.predict(x\_test)  </span><br><span class="line">bt\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 0, 3, 2, 1, 2, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2,</span><br><span class="line">       1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 0, 2, 1, 3, 3,</span><br><span class="line">       1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 0, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3,</span><br><span class="line">       2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 3, 1, 3, 3, 0, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,</span><br><span class="line">       3, 1, 1, 1, 3, 0, 3, 3, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h4 id="准确率计算"><a href="#准确率计算" class="headerlink" title="准确率计算"></a><a href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97" title="准确率计算"></a>准确率计算</h4><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_accuracy(y\_test,bt\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8991596638655462 </span><br></pre></td></tr></table></figure><p>根据准确率可以看到在决策树通过装袋（Bagging）算法后预测准确率有明显提升。</p><h3 id="随机森林-Random-Forest"><a href="#随机森林-Random-Forest" class="headerlink" title="随机森林 Random Forest"></a><a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-Random-Forest" title="随机森林 Random Forest"></a>随机森林 Random Forest</h3><p>其实，Bagging tree 算法，是应用子数据集中的所有特征构建一棵完整的树，最终通过投票的方式进行预测。而随机森林就是在 Bagging tree 算法的基础上进行进一步的改进。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/2.png"></a></p><p>随机森林的思想就是将一个大的数据集使用<strong>自助采样法</strong>进行处理，即从原样本数据集中随机抽取多个子样本集，并基于每一个子样本集生成相应的决策树。这样，就可以构建出由许多小决策树组形成的决策树「森林」。最后，实验通过<strong>投票法</strong>选择决策树最多的预测结果作为最终的输出。</p><p>所以，随机森林的名称来源就是「随机抽样 + 决策树森林」。</p><h4 id="随机森林算法原理"><a href="#随机森林算法原理" class="headerlink" title="随机森林算法原理"></a><a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" title="随机森林算法原理"></a>随机森林算法原理</h4><p>随机森林作为装袋（Bagging）的代表算法，算法原理和装袋十分相似，但在此基础上做了一些改进：</p><ol><li><p>对于普通的决策树，会在 N 个样本的所有特征中选择一个最优划分特征，但是随机森林首先会从所有特征中随机选择部分特征，再从该部分特征中选择一个最优划分特征。这样进一步增强了模型的泛化能力。</p></li><li><p>在决定部分特征个数时，通过交叉验证的方式来获取一个合适的值。</p></li></ol><p>随机森林算法流程：</p><ol><li>从样本集中有放回随机采样选出 <code>n</code> 个样本；</li><li>从所有特征中随机选择 <code>k</code> 个特征，对选出的样本利用这些特征建立决策树；</li><li>重复以上两步 <code>m</code> 次，即生成 <code>m</code> 棵决策树，形成随机森林；</li><li>对于新数据，经过每棵树决策，最后投票确认分到哪一类。</li></ol><h4 id="模型构建和数据预测"><a href="#模型构建和数据预测" class="headerlink" title="模型构建和数据预测"></a><a href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E5%92%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E6%B5%8B" title="模型构建和数据预测"></a>模型构建和数据预测</h4><p>在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 <code>scikit-learn</code> 来对其进行实现。</p><p>在 <code>scikit-learn</code> 随机森林常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RandomForestClassifier(n\_estimators,criterion,max\_features,random\_state&#x3D;None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>n_estimators</code>：表示建立树的个数，默认值为 10 。</li><li><code>criterion</code>：表示特征划分方法选择，默认为 <code>gini</code>，可选择为 <code>entropy</code> (信息增益)。</li><li><code>max_features</code>：表示随机选择特征个数，默认为特征数的根号。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>：训练随机森林。</li><li><code>predict(X)</code>：对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier  </span><br><span class="line">  </span><br><span class="line">\# 这里构建 100 棵决策树，采用信息熵来寻找最优划分特征。  </span><br><span class="line">rf &#x3D; RandomForestClassifier(  </span><br><span class="line">    n\_estimators&#x3D;100, max\_features&#x3D;None, criterion&#x3D;&#39;entropy&#39;)  </span><br><span class="line">rf.fit(x\_train, y\_train)  # 进行模型的训练  </span><br><span class="line">rf\_y\_predict &#x3D; rf.predict(x\_test)  </span><br><span class="line">rf\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 0, 3, 2, 1, 2, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2,</span><br><span class="line">       1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 3,</span><br><span class="line">       1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 2, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3,</span><br><span class="line">       2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 0, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,</span><br><span class="line">       3, 1, 1, 1, 3, 0, 3, 3, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h4 id="准确率计算-1"><a href="#准确率计算-1" class="headerlink" title="准确率计算"></a><a href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97-1" title="准确率计算"></a>准确率计算</h4><p>当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_accuracy(y\_test, rf\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8907563025210085 </span><br></pre></td></tr></table></figure><p>可以通过结果看到，本次实验的数据集用随机森林预测的准确率和用 Bagging tree 预测的准确率差别不大，但随着数据集的增大和特征数的增多，随机森林的优势就会慢慢显现出来。</p><h2 id="提升算法-Boosting"><a href="#提升算法-Boosting" class="headerlink" title="提升算法 Boosting"></a><a href="#%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95-Boosting" title="提升算法 Boosting"></a>提升算法 Boosting</h2><p>当「个体学习器」之间存在较强的依赖时，采用装袋的算法便有些不合适，此时最好的方法就是使用串行集成方式：提升（Boosting）。</p><h3 id="提升算法原理"><a href="#提升算法原理" class="headerlink" title="提升算法原理"></a><a href="#%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" title="提升算法原理"></a>提升算法原理</h3><p>提升算法是可以将弱学习器提升为强学习器的算法，其具体思想是从初始训练集训练出一个「个体学习器」，再根据个体学习器的表现对训练样本分布进行调整，使得在个体学习器中判断错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个「个体学习器」。如此重复进行，直至个体学习器数目达到事先指定的值 <code>T</code>，最终将这 <code>T</code> 个「个体学习器」输出的值进行加权结合得到最终的输出值。</p><h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a><a href="#Adaboost" title="Adaboost"></a>Adaboost</h3><p>提升（Boosting）算法中最具代表性的算法为 Adaboost。</p><p>AdaBoost（Adaptive Boosting）名为自适应增强，其主要自适应增强表现在：上一个「个体学习器」中被错误分类的样本的权值会增大，正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/3.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Bagging-and-Boosting/3.png"></a></p><h4 id="AdaBoost-原理"><a href="#AdaBoost-原理" class="headerlink" title="AdaBoost 原理"></a><a href="#AdaBoost-%E5%8E%9F%E7%90%86" title="AdaBoost 原理"></a>AdaBoost 原理</h4><p>AdaBoost 算法与 Boosting 算法不同的是，其不需要预先知道弱分类器的误差，并且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度。</p><p>Adaboost 算法流程：</p><ol><li>数据准备：通过数据清理和数据整理的方式得到符合规范的数据。</li><li>初始化权重：如果有 <code>N</code> 个训练样本数据，在最开始时每一个数据被赋予相同的权值：<code>1/N</code>。</li><li>弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。</li><li>更改权重：如果某个样本点被准确地分类，降低其权值；若被分类错误，那么提高其权值。然后，权值更新过的样本集被用于训练下一个分类器。</li><li>强分类器组合：重复 <code>3，4</code> 步骤，直至训练结束，加大分类误差率小的弱分类器的权重（这里的权重和样本权重不一样），使其在最终的分类函数中起着较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用，最终输出结果。</li></ol><h4 id="模型构建和数据预测-1"><a href="#模型构建和数据预测-1" class="headerlink" title="模型构建和数据预测"></a><a href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E5%92%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E6%B5%8B-1" title="模型构建和数据预测"></a>模型构建和数据预测</h4><p>在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 <code>scikit-learn</code> 来对其进行实现。</p><p>在 <code>scikit-learn</code> Adaboost 常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AdaBoostClassifier(base\_estimators,n\_estimators)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>base_estimators</code>：表示弱分类器种类，默认为 CART 分类树。</li><li><code>n_estimators</code>：表示弱学习器的最大个数，默认值为 <code>50</code>。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>：训练弱分类器。</li><li><code>predict(X)</code>：对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import AdaBoostClassifier  </span><br><span class="line">  </span><br><span class="line">ad &#x3D; AdaBoostClassifier(n\_estimators&#x3D;100)  </span><br><span class="line">ad.fit(x\_train, y\_train)  </span><br><span class="line">ad\_y\_predict &#x3D; ad.predict(x\_test)  </span><br><span class="line">ad\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 3, 3, 2, 0, 3, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 3,</span><br><span class="line">       1, 3, 3, 3, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 3, 1, 3, 3,</span><br><span class="line">       1, 3, 2, 3, 2, 3, 3, 3, 1, 2, 3, 3, 1, 3, 2, 2, 3, 2, 0, 2, 2, 3,</span><br><span class="line">       2, 3, 3, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 3, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 1, 2, 0, 2, 3, 0, 3, 1, 3, 1, 0, 3, 3, 3, 3, 2, 3,</span><br><span class="line">       3, 1, 3, 1, 3, 3, 3, 1, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h4 id="准确率计算-2"><a href="#准确率计算-2" class="headerlink" title="准确率计算"></a><a href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97-2" title="准确率计算"></a>准确率计算</h4><p>当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_accuracy(y\_test, ad\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7983193277310925 </span><br></pre></td></tr></table></figure><p>通过结果可以看到，应用 Adaboost 算法得到的准确率和决策树相差不大，说明在使用 Adaboost 算法时预测效果不好。</p><h3 id="梯度提升树-GBDT"><a href="#梯度提升树-GBDT" class="headerlink" title="梯度提升树 GBDT"></a><a href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-GBDT" title="梯度提升树 GBDT"></a>梯度提升树 GBDT</h3><p>梯度提升树（Gradient Boosting Decison Tree，GBDT）同样是 Boosting 算法家族中的一员， Adaboost 是利用前一轮迭代弱学习器的误差率来更新训练集的权重，而梯度提升树所采用的是前向分布算法，且弱学习器限定了只能使用CART树模型。</p><h4 id="梯度提升树算法原理"><a href="#梯度提升树算法原理" class="headerlink" title="梯度提升树算法原理"></a><a href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" title="梯度提升树算法原理"></a>梯度提升树算法原理</h4><p>在 GBDT 的迭代中，假设我们前一轮迭代得到的强学习器是 ft−1(x)$ft−1(x)$, 损失函数是 L(y,ft−1(x))$L(y,ft−1(x))$, 我们本轮迭代的目标是找到一个 CART 回归树模型的弱学习器 ht(x)$ht(x)$，让本轮的损失 L(y,ft(x)=L(y,ft−1(x)+ht(x))$L(y,ft(x)=L(y,ft−1(x)+ht(x))$ 最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p><p>算法流程：</p><ol><li>数据准备：通过数据清理和数据整理的方式得到符合规范的数据。</li><li>初始化权重：如果有 <code>N</code> 个训练样本数据，在最开始时每一个数据被赋予相同的权值：<code>1/N</code>。</li><li>弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。</li><li>CART 树拟合：计算每一个子样本的梯度值，通过梯度值和子样本拟合一棵 CART 树</li><li>更新强学习器：在拟合好的 CART树中通过损失函数计算出最佳的拟合值，更新先前组成的强学习器。</li><li>强分类器组合：重复 <code>3，4，5</code> 步骤，直至训练结束，得到一个强分类器，最终输出结果。</li></ol><h4 id="梯度提升树模型构建及预测"><a href="#梯度提升树模型构建及预测" class="headerlink" title="梯度提升树模型构建及预测"></a><a href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E5%8F%8A%E9%A2%84%E6%B5%8B" title="梯度提升树模型构建及预测"></a>梯度提升树模型构建及预测</h4><p>在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 <code>scikit-learn</code> 来对其进行实现。</p><p>在 <code>scikit-learn</code> GBDT 常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GradientBoostingClassifier(max\_depth &#x3D; 3，learning\_rate &#x3D; 0.1, n\_estimators &#x3D; 100，random\_state &#x3D; None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>max_depth</code>:表示生成 CART 树的最大深度，默认为 3</li><li><code>learning_rate</code>:表示学习效率，默认为 0.1。</li><li><code>n_estimators</code>：表示弱学习器的最大个数，默认值为 100。</li><li><code>random_state</code>:表示随机数种子。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>：训练弱分类器。</li><li><code>predict(X)</code>：对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import GradientBoostingClassifier  </span><br><span class="line">  </span><br><span class="line">clf &#x3D; GradientBoostingClassifier(  </span><br><span class="line">    n\_estimators&#x3D;100, learning\_rate&#x3D;1.0, random\_state&#x3D;33)  </span><br><span class="line">clf.fit(x\_train, y\_train)  </span><br><span class="line">gt\_y\_predict &#x3D; clf.predict(x\_test)  </span><br><span class="line">gt\_y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 0, 3, 2, 1, 3, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2,</span><br><span class="line">       1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 3,</span><br><span class="line">       1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3,</span><br><span class="line">       2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 0, 3, 3, 1,</span><br><span class="line">       3, 3, 1, 3, 3, 3, 2, 0, 0, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,</span><br><span class="line">       3, 1, 1, 1, 3, 2, 3, 3, 3], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h4 id="准确率计算-3"><a href="#准确率计算-3" class="headerlink" title="准确率计算"></a><a href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97-3" title="准确率计算"></a>准确率计算</h4><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_accuracy(y\_test, gt\_y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8739495798319328 </span><br></pre></td></tr></table></figure><p>可以看到，在使用装袋和提升算法时，在大部分情况下，会产生更好的预测结果，但有时也可能出现没有优化的情况。事实上，机器学习分类器的选择就是如此，没有最好的分类器只有最适合的分类器，不同的数据集，由于其数据特点的不同，在不同的分类器中表现也不同。</p><p><strong>拓展阅读：</strong></p><ul><li><a href="https://zh.wikipedia.org/zh-hans/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97">随机森林 - 维基百科</a></li><li><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">Bootstrap aggregating - 维基百科</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;前面的文章都是独立的讲解每一个分类器的分类过程，每一个分类器都有其独有的特点并非常适合某些数据。但在实际中，可能没有那样合适的数据，在应用前面的分类器时，可能会出现分类准确率低的问题。为解决这样的问题，集成学习便被提出，利用多个弱分类器结合的方式使得分类准确率提高。本次详细讲解了集成学习中十分经典的几个算法：装袋（Bagging）中的 Bagging tree 和随机森林（Ramdom Forest）以及 提升（Boosting）中的 Adaboost 和梯度提升树（GBDT）。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="装袋" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A3%85%E8%A2%8B/"/>
    
    <category term="提升" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8F%90%E5%8D%87/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Bagging" scheme="http://www.laugh12321.cn/blog/tags/Bagging/"/>
    
    <category term="BoosTing" scheme="http://www.laugh12321.cn/blog/tags/BoosTing/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|决策树详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/02/01/decision_tree/"/>
    <id>http://www.laugh12321.cn/blog/2019/02/01/decision_tree/</id>
    <published>2019-02-01T00:00:00.000Z</published>
    <updated>2020-10-23T08:33:20.820Z</updated>
    
    <content type="html"><![CDATA[<p>决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。  </p><a id="more"></a><p>举一个通俗的例子，假设在B站工作多年仍然单身的小B和他母亲在给他介绍对象时的一段对话：</p><blockquote><p>母亲：小B，你都 28 了还是单身，明天亲戚家要来个姑娘要不要去见见。<br>小B：多大年纪？<br>母亲：26。<br>小B：有多高？<br>母亲：165厘米。<br>小B：长的好看不。<br>母亲：还行，比较朴素。<br>小B：温柔不？<br>母亲：看起来挺温柔的，很有礼貌。<br>小B：好，去见见。</p></blockquote><p>作为程序员的小B的思考逻辑就是典型的决策树分类逻辑，将年龄，身高，长相，是否温柔作为特征，并最后对见或者不见进行决策。其决策逻辑如图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/1.png"></center><h2 id="决策树算法实现"><a href="#决策树算法实现" class="headerlink" title="决策树算法实现"></a>决策树算法实现</h2><p>其实决策树算法如同上面场景一样，其思想非常容易理解，具体的算法流程为：</p><ul><li><p><strong>第 1 步</strong>: 数据准备：通过数据清洗和数据处理，将数据整理为没有缺省值的向量。</p></li><li><p><strong>第 2 步</strong>: 寻找最佳特征：遍历每个特征的每一种划分方式，找到最好的划分特征。</p></li><li><p><strong>第 3 步</strong>: 生成分支：划分成两个或多个节点。</p></li><li><p><strong>第 4 步</strong>: 生成决策树：对分裂后的节点分别继续执行2-3步，直到每个节点只有一种类别。</p></li><li><p><strong>第 5 步</strong>: 决策分类：根据训练决策树模型，将预测数据进行分类。</p></li></ul><h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><p>下面我们依照决策树的算法流程，用 <code>python</code> 来实现决策树构建和分类。首先生成一组数据，数据包含两个类别 <code>man</code> 和 <code>woman</code>,特征分别为:</p><ul><li><code>hair</code>:头发长短(<code>long</code>:长,<code>short</code>:短)</li><li><code>voice</code>:声音粗细(<code>thick</code>:粗,<code>thin</code>:细)</li><li><code>height</code>:身高</li><li><code>ear_stud</code>:是否带有耳钉(<code>yes</code>:是,<code>no</code>:没有)</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;生成示例数据  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import numpy as np  </span><br><span class="line">import pandas as pd  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def create\_data():  </span><br><span class="line">    data\_value &#x3D; np.array(  </span><br><span class="line">        \[\[&#39;long&#39;, &#39;thick&#39;, 175, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 168, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 178, &#39;yes&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thick&#39;, 172, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;medium&#39;, 163, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thick&#39;, 180, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 173, &#39;yes&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 174, &#39;no&#39;, &#39;man&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 164, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;medium&#39;, 158, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 161, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;thin&#39;, 166, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 158, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 163, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thick&#39;, 161, &#39;yes&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;long&#39;, &#39;thin&#39;, 164, &#39;no&#39;, &#39;woman&#39;\],  </span><br><span class="line">         \[&#39;short&#39;, &#39;medium&#39;, 172, &#39;yes&#39;, &#39;woman&#39;\]\])  </span><br><span class="line">    columns &#x3D; np.array(\[&#39;hair&#39;, &#39;voice&#39;, &#39;height&#39;, &#39;ear\_stud&#39;, &#39;labels&#39;\])  </span><br><span class="line">    data &#x3D; pd.DataFrame(data\_value.reshape(17, 5), columns&#x3D;columns)  </span><br><span class="line">    return data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>在创建好数据之后，加载并打印出这些数据</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; create\_data()  </span><br><span class="line">data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a><a href="#%E5%88%92%E5%88%86%E9%80%89%E6%8B%A9" title="划分选择"></a>划分选择</h3><p>在得到数据后，根据算法流程，接下来需要寻找最优的划分特征，随着划分的不断进行，我们尽可能的将划分的分支所包含的样本归于同一类别，即结点的“纯度”越来越高。而常用的特征划分方式为信息增益和增益率。</p><h4 id="信息增益（ID3）"><a href="#信息增益（ID3）" class="headerlink" title="信息增益（ID3）"></a><a href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%EF%BC%88ID3%EF%BC%89" title="信息增益（ID3）"></a>信息增益（ID3）</h4><p>在介绍信息增益之前，先引入“信息熵”的概念。“信息熵”是度量样本纯度最常用的一种指标，其公式为：</p><p>Ent(D)=−|y|∑k=1pklog2pk(1)</p><p>$$(1)Ent(D)=−∑k=1|y|pklog2pk$$</p><p>其中 D$D$ 表示样本集合，pk$pk$ 表示第 k$k$ 类样本所占的比例。其中 Ent(D)$Ent(D)$ 的值越小，则 D$D$ 的纯度越高。根据以上数据，在计算数据集的“信息熵”时，|y|$|y|$ 显然只有 <code>man</code>,<code>woman</code> 共 2 种，其中为 <code>man</code> 的概率为 817$817$, <code>woman</code> 的概率为 917$917$,则根据公式(1)得到数据集的纯度为：</p><p>Ent(data)=−2∑k=1pklog2pk=−(817log2817+917log2917)=0.9975</p><p>$$Ent(data)=−∑k=12pklog2pk=−(817log2817+917log2917)=0.9975$$</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算信息熵  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import math  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_Ent(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    Ent -- 信息熵  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    num\_sample &#x3D; len(data)  # 样本个数  </span><br><span class="line">    label\_counts &#x3D; &#123;&#125;  # 初始化标签统计字典  </span><br><span class="line">    for i in range(num\_sample):  </span><br><span class="line">        each\_data &#x3D; data.iloc\[i, :\]  </span><br><span class="line">        current\_label &#x3D; each\_data\[&quot;labels&quot;\]  # 得到当前元素的标签（label）  </span><br><span class="line">  </span><br><span class="line">        # 如果标签不在当前字典中，添加该类标签并初始化 value&#x3D;0,否则该类标签 value+1  </span><br><span class="line">        if current\_label not in label\_counts.keys():  </span><br><span class="line">            label\_counts\[current\_label\] &#x3D; 0  </span><br><span class="line">        label\_counts\[current\_label\] +&#x3D; 1  </span><br><span class="line">      </span><br><span class="line">    Ent &#x3D; 0.0  # 初始化信息熵  </span><br><span class="line">    for key in label\_counts:  </span><br><span class="line">        prob &#x3D; float(label\_counts\[key\])&#x2F;num\_sample  </span><br><span class="line">        Ent -&#x3D; prob \* math.log(prob, 2)  # 应用信息熵公式计算信息熵  </span><br><span class="line">    return Ent  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>通过计算信息熵函数，计算根节点的信息熵：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">base\_ent &#x3D; get\_Ent(data)  </span><br><span class="line">base\_ent  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9975025463691153 </span><br></pre></td></tr></table></figure><p><strong>信息增益</strong> 就是建立在信息熵的基础上，在离散特征 x$x$ 有 M$M$ 个取值，如果用 x$x$ 对样本 D$D$ 来进行划分，就会产生 M$M$ 个分支，其中第 m$m$ 个分支包含了集合 D$D$ 的所有在特征 x$x$ 上取值为 m$m$ 的样本，记为 Dm$Dm$（例如：根据以上生成数据，如果我们用 <code>hair</code> 进行划分，则会产生<code>long</code>，<code>short</code>两个分支，每一个分支中分别包含了整个集合中属于 <code>long</code> 或者 <code>short</code> 的数据）。</p><p>考虑到不同分支节点包含样本数不同，给分支赋予权重 |Dm||D|$|Dm||D|$ ,使得样本越多的分支节点影响越大，则 <strong>信息增益</strong> 的公式就可以得到：</p><p>Gain(D,x)=Ent(D)−M∑m=1|Dm||D|Ent(Dm)(2)</p><p>$$(2)Gain(D,x)=Ent(D)−∑m=1M|Dm||D|Ent(Dm)$$</p><p>一般情况下，信息增益越大，则说明用 x$x$ 来划分样本集合 D$D$ 的纯度越高。以 <code>hair</code> 为例，其中它有 <code>short</code> 和 <code>long</code> 两个可能取值，则分别用 D1$D1$ (hair = long) 和 `D^{2} (hair = short)来表示。</p><p>其中为 D1$D1$ 的数据编号为 {0，4，6，8，9，10，12，14，15}${0，4，6，8，9，10，12，14，15}$ 共 9 个，在这之中为 <code>man</code> 的有 {0，4，6} 共3 个占比为39$39$，为 <code>woman</code> 的有{8, 9，10，12，14，15}共 6 个占比为69$69$; 同样 D2$D2$ 编号为{1，2，3，5，7，11，13, 16}共 8 个，其中为 <code>man</code> 的有{1，2，3，5，7}共 5 个占比58$58$,为 <code>woman</code> 的有{11，13, 16}共 3 个占比 38$38$,若按照 <code>hair</code> 进行划分，则两个分支点的信息熵为：</p><p>Ent(D1)=−(39log239+69log269)=0.449</p><p>$$Ent(D1)=−(39log239+69log269)=0.449$$</p><p>Ent(D2)=−(58log258+38log238)=0.486</p><p>$$Ent(D2)=−(58log258+38log238)=0.486$$</p><p>根据信息增益的公式可以计算出 <code>hair</code> 的信息增益为：</p><p>Gain(D,hair)=Ent(D)−2∑m=1|Dm||D|Ent(Dm)=0.9975−(917∗0.449+817∗0.486)=0.062</p><p>$$Gain(D,hair)=Ent(D)−∑m=12|Dm||D|Ent(Dm)=0.9975−(917∗0.449+817∗0.486)=0.062$$</p><p>下面我们用 <code>python</code> 来实现<strong>信息增益（ID3）</strong>算法：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算信息增益  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_gain(data, base\_ent, feature):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    base\_ent -- 根节点的信息熵  </span><br><span class="line">    feature -- 计算信息增益的特征  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    Ent -- 信息熵  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">    feature\_list &#x3D; data\[feature\]  # 得到一个特征的全部取值  </span><br><span class="line">    unique\_value &#x3D; set(feature\_list)  # 特征取值的类别  </span><br><span class="line">    feature\_ent &#x3D; 0.0  </span><br><span class="line">  </span><br><span class="line">    for each\_feature in unique\_value:  </span><br><span class="line">        temp\_data &#x3D; data\[data\[feature\] &#x3D;&#x3D; each\_feature\]  </span><br><span class="line">        weight &#x3D; len(temp\_data)&#x2F;len(feature\_list)  # 计算该特征的权重值  </span><br><span class="line">        temp\_ent &#x3D; weight\*get\_Ent(temp\_data)  </span><br><span class="line">        feature\_ent &#x3D; feature\_ent+temp\_ent  </span><br><span class="line">  </span><br><span class="line">    gain &#x3D; base\_ent - feature\_ent  # 信息增益  </span><br><span class="line">    return gain  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>完成 <strong>信息增益</strong> 函数后，尝试计算特征 <code>hair</code> 的信息增益值。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get\_gain(data,base\_ent,&#39;hair&#39;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.062200515199107964 </span><br></pre></td></tr></table></figure><h4 id="信息增益率（C4-5）"><a href="#信息增益率（C4-5）" class="headerlink" title="信息增益率（C4.5）"></a><a href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87%EF%BC%88C4-5%EF%BC%89" title="信息增益率（C4.5）"></a>信息增益率（C4.5）</h4><p>信息增益也存在许多不足之处，经过大量的实验发现，当信息增益作为标准时，易偏向于取值较多的特征，为了避免这种偏好给预测结果带来的不好影响，可以使用<strong>增益率</strong>来选择最优划分。<strong>增益率</strong>的公式定义为:</p><p>GainRatio(D,a)=Gain(D,a)IV(a)(3)</p><p>$$(3)GainRatio(D,a)=Gain(D,a)IV(a)$$</p><p>其中：</p><p>IV(a)=−M∑m=1|Dm||D|log2|Dm||D|(4)</p><p>$$(4)IV(a)=−∑m=1M|Dm||D|log2|Dm||D|$$</p><p>IV(a)$IV(a)$ 称为特征 a$a$ 的固有值，当 a$a$ 的取值数目越多，则 IV(a)$IV(a)$ 的值通常会比较大。例如：</p><p>IV(hair)=−917log2917−817log2817=0.998</p><p>$$IV(hair)=−917log2917−817log2817=0.998$$</p><p>IV(voice)=−717log2717−517log2517−517log2517=1.566</p><p>$$IV(voice)=−717log2717−517log2517−517log2517=1.566$$</p><h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a><a href="#%E8%BF%9E%E7%BB%AD%E5%80%BC%E5%A4%84%E7%90%86" title="连续值处理"></a>连续值处理</h3><p>在前面介绍的特征选择中，都是对离散型数据进行处理，但在实际的生活中数据常常会出现连续值的情况，如生成数据中的身高，当数据较少时，可以将每一个值作为一个类别，但当数据量大时，这样是不可取的，在 <strong>C4.5</strong> 算法中采用二分法对连续值进行处理。</p><p>对于连续的属性 X$X$ 假设共出现了 n 个不同的取值，将这些取值从小到大排序{x1,x2,x3,…,xn}${x1,x2,x3,…,xn}$ ，其中找一点作为划分点 t ，则将数据划分为两类，大于 t 的为一类，小于 t 的为另一类。而 t 的取值通常为相邻两点的平均数 t=xi+xi+12$t=xi+xi+12$。</p><p>则在 n 个连续值之中，可以作为划分点的 t 有 n-1 个。通过遍历可以像离散型一样来考察这些划分点。</p><p>Gain(D,X)=Ent(D)−|D&lt;t||D|Ent(D&lt;t)−|D&gt;t||D|Ent(D&gt;t)(5)</p><p>$$(5)Gain(D,X)=Ent(D)−|D&lt;t||D|Ent(D&lt;t)−|D&gt;t||D|Ent(D&gt;t)$$</p><p>其中得到样本 D$D$ 基于划分点 t 二分后的信息增益，于是我们可以选择使得 Ga∈(D,X)$Ga∈(D,X)$ 值最大的划分点。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line">30  </span><br><span class="line">31  </span><br><span class="line">32  </span><br><span class="line">33  </span><br><span class="line">34  </span><br><span class="line">35  </span><br><span class="line">36  </span><br><span class="line">37  </span><br><span class="line">38  </span><br><span class="line">39  </span><br><span class="line">40  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;计算连续值的划分点  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def get\_splitpoint(data, base\_ent, feature):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    base\_ent -- 根节点的信息熵  </span><br><span class="line">    feature -- 需要划分的连续特征  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    final\_t -- 连续值最优划分点  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    # 将连续值进行排序并转化为浮点类型  </span><br><span class="line">    continues\_value &#x3D; data\[feature\].sort\_values().astype(np.float64)  </span><br><span class="line">    continues\_value &#x3D; \[i for i in continues\_value\]  # 不保留原来的索引  </span><br><span class="line">    t\_set &#x3D; \[\]  </span><br><span class="line">    t\_ent &#x3D; &#123;&#125;  </span><br><span class="line">  </span><br><span class="line">    # 得到划分点 t 的集合  </span><br><span class="line">    for i in range(len(continues\_value)-1):  </span><br><span class="line">        temp\_t &#x3D; (continues\_value\[i\]+continues\_value\[i+1\])&#x2F;2  </span><br><span class="line">        t\_set.append(temp\_t)  </span><br><span class="line">  </span><br><span class="line">    # 计算最优划分点  </span><br><span class="line">    for each\_t in t\_set:  </span><br><span class="line">        # 将大于划分点的分为一类  </span><br><span class="line">        temp1\_data &#x3D; data\[data\[feature\].astype(np.float64) &gt; each\_t\]  </span><br><span class="line">        # 将小于划分点的分为一类  </span><br><span class="line">        temp2\_data &#x3D; data\[data\[feature\].astype(np.float64) &lt; each\_t\]  </span><br><span class="line">        weight1 &#x3D; len(temp1\_data)&#x2F;len(data)  </span><br><span class="line">        weight2 &#x3D; len(temp2\_data)&#x2F;len(data)  </span><br><span class="line">        # 计算每个划分点的信息增益  </span><br><span class="line">        temp\_ent &#x3D; base\_ent-weight1 \* \\  </span><br><span class="line">            get\_Ent(temp1\_data)-weight2\*get\_Ent(temp2\_data)  </span><br><span class="line">        t\_ent\[each\_t\] &#x3D; temp\_ent  </span><br><span class="line">    print(&quot;t\_ent:&quot;, t\_ent)  </span><br><span class="line">    final\_t &#x3D; max(t\_ent, key&#x3D;t\_ent.get)  </span><br><span class="line">    return final\_t  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>实现连续值最优划分点的函数后，寻找 <code>height</code> 连续特征值的划分点。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">final\_t &#x3D; get\_splitpoint(data, base\_ent, &#39;height&#39;)  </span><br><span class="line">final\_t  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t_ent: &#123;158.0: 0.1179805181500242, 159.5: 0.1179805181500242, 161.0: 0.2624392604045631, 162.0: 0.2624392604045631, 163.0: 0.3856047022157598, 163.5: 0.15618502398692893, 164.0: 0.3635040117533678, 165.0: 0.33712865788827096, 167.0: 0.4752766311586692, 170.0: 0.32920899348970845, 172.0: 0.5728389611412551, 172.5: 0.4248356349861979, 173.5: 0.3165383509071513, 174.5: 0.22314940393447813, 176.5: 0.14078143361499595, 179.0: 0.06696192680347068&#125;</span><br><span class="line"></span><br><span class="line">172.0 </span><br></pre></td></tr></table></figure><h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a><a href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0" title="算法实现"></a>算法实现</h3><p>在对决策树中最佳特征选择和连续值处理之后，接下来就是对决策树的构建。</p><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86" title="数据预处理"></a>数据预处理</h4><p>首先我们将连续值进行处理，在找到最佳划分点之后，将 &lt;t$&lt;t$ 的值设为 0，将 &gt;=t$&gt;=t$ 的值设为 1。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def choice\_1(x, t):  </span><br><span class="line">    if x &gt; t:  </span><br><span class="line">        return &quot;&gt;&#123;&#125;&quot;.format(t)  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;&lt;&#123;&#125;&quot;.format(t)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">deal\_data &#x3D; data.copy()  </span><br><span class="line">\# 使用lambda和map函数将 height 按照final\_t划分为两个类别  </span><br><span class="line">deal\_data\[&quot;height&quot;\] &#x3D; pd.Series(  </span><br><span class="line">    map(lambda x: choice\_1(int(x), final\_t), deal\_data\[&quot;height&quot;\]))  </span><br><span class="line">deal\_data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h4 id="选择最优划分特征"><a href="#选择最优划分特征" class="headerlink" title="选择最优划分特征"></a><a href="#%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%88%92%E5%88%86%E7%89%B9%E5%BE%81" title="选择最优划分特征"></a>选择最优划分特征</h4><p>将数据进行预处理之后，接下来就是选择最优的划分特征。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;选择最优划分特征  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def choose\_feature(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    best\_feature -- 最优的划分特征  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    num\_features &#x3D; len(data.columns) - 1  # 特征数量  </span><br><span class="line">    base\_ent &#x3D; get\_Ent(data)  </span><br><span class="line">    best\_gain &#x3D; 0.0  # 初始化信息增益  </span><br><span class="line">    best\_feature &#x3D; data.columns\[0\]  </span><br><span class="line">    for i in range(num\_features):  # 遍历所有特征  </span><br><span class="line">        temp\_gain &#x3D; get\_gain(data, base\_ent, data.columns\[i\])    # 计算信息增益  </span><br><span class="line">        if (temp\_gain &gt; best\_gain):  # 选择最大的信息增益  </span><br><span class="line">            best\_gain &#x3D; temp\_gain  </span><br><span class="line">            best\_feature &#x3D; data.columns\[i\]  </span><br><span class="line">    return best\_feature  # 返回最优特征  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>完成函数之后，我们首先看看数据集中信息增益值最大的特征是什么？</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">choose\_feature(deal\_data)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;height&#39; </span><br></pre></td></tr></table></figure><h4 id="构建决策树"><a href="#构建决策树" class="headerlink" title="构建决策树"></a><a href="#%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91" title="构建决策树"></a>构建决策树</h4><p>在将所有的子模块构建好之后，最后就是对核心决策树的构建，本次实验采用<strong>信息增益（ID3）</strong>的方式构建决策树。在构建的过程中，根据算法流程，我们反复遍历数据集，计算每一个特征的信息增益，通过比较将最好的特征作为父节点，根据特征的值确定分支子节点，然后重复以上操作，直到某一个分支全部属于同一类别，或者遍历完所有的数据特征，当遍历到最后一个特征时，若分支数据依然“不纯”，就将其中数量较多的类别作为子节点。</p><p>因此最好采用递归的方式来构建决策树。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line">25  </span><br><span class="line">26  </span><br><span class="line">27  </span><br><span class="line">28  </span><br><span class="line">29  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;构建决策树  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def create\_tree(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    tree -- 以字典的形式返回决策树  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    feature\_list &#x3D; data.columns\[:-1\].tolist()  </span><br><span class="line">    label\_list &#x3D; data.iloc\[:, -1\]  </span><br><span class="line">    if len(data\[&quot;labels&quot;\].value\_counts()) &#x3D;&#x3D; 1:  </span><br><span class="line">        leaf\_node &#x3D; data\[&quot;labels&quot;\].mode().values  </span><br><span class="line">        return leaf\_node            # 第一个递归结束条件：所有的类标签完全相同  </span><br><span class="line">    if len(feature\_list) &#x3D;&#x3D; 1:  </span><br><span class="line">        leaf\_node &#x3D; data\[&quot;labels&quot;\].mode().values  </span><br><span class="line">        return leaf\_node   # 第二个递归结束条件：用完了所有特征  </span><br><span class="line">    best\_feature &#x3D; choose\_feature(data)   # 最优划分特征  </span><br><span class="line">    tree &#x3D; &#123;best\_feature: &#123;&#125;&#125;  </span><br><span class="line">    feat\_values &#x3D; data\[best\_feature\]  </span><br><span class="line">    unique\_value &#x3D; set(feat\_values)  </span><br><span class="line">    for value in unique\_value:  </span><br><span class="line">        temp\_data &#x3D; data\[data\[best\_feature\] &#x3D;&#x3D; value\]  </span><br><span class="line">        temp\_data &#x3D; temp\_data.drop(\[best\_feature\], axis&#x3D;1)  </span><br><span class="line">        tree\[best\_feature\]\[value\] &#x3D; create\_tree(temp\_data)  </span><br><span class="line">    return tree  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>完成创建决策树函数后，接下来对我们第一棵树进行创建。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tree &#x3D; create\_tree(deal\_data)  </span><br><span class="line">tree  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;height&#39;: &#123;&#39;&lt;172.0&#39;: &#123;&#39;ear_stud&#39;: &#123;&#39;no&#39;: &#123;&#39;voice&#39;: &#123;&#39;thick&#39;: array([&#39;man&#39;], dtype&#x3D;object),</span><br><span class="line">      &#39;medium&#39;: array([&#39;man&#39;], dtype&#x3D;object),</span><br><span class="line">      &#39;thin&#39;: array([&#39;woman&#39;], dtype&#x3D;object)&#125;&#125;,</span><br><span class="line">    &#39;yes&#39;: array([&#39;woman&#39;], dtype&#x3D;object)&#125;&#125;,</span><br><span class="line">  &#39;&gt;172.0&#39;: array([&#39;man&#39;], dtype&#x3D;object)&#125;&#125; </span><br></pre></td></tr></table></figure><p>通过字典的方式表示构建好的树，可以通过图像的方式更加直观的了解。</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/2.png"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/2.png"></a></p><p>通过图形可以看出，在构建决策树时不一定每一个特征都会成为树的节点（如同 <code>hair</code>）。</p><h4 id="决策分类"><a href="#决策分类" class="headerlink" title="决策分类"></a><a href="#%E5%86%B3%E7%AD%96%E5%88%86%E7%B1%BB" title="决策分类"></a>决策分类</h4><p>在构建好决策树之后，最终就可以使用未知样本进行预测分类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line">20  </span><br><span class="line">21  </span><br><span class="line">22  </span><br><span class="line">23  </span><br><span class="line">24  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;决策分类  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def classify(tree, test):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">    test -- 需要测试的数据  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    class\_label -- 分类结果  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    first\_feature &#x3D; list(tree.keys())\[0\]  # 获取根节点  </span><br><span class="line">    feature\_dict &#x3D; tree\[first\_feature\]  # 根节点下的树  </span><br><span class="line">    labels &#x3D; test.columns.tolist()  </span><br><span class="line">    value &#x3D; test\[first\_feature\]\[0\]  </span><br><span class="line">    for key in feature\_dict.keys():  </span><br><span class="line">        if value &#x3D;&#x3D; key:  </span><br><span class="line">            if type(feature\_dict\[key\]).\_\_name\_\_ &#x3D;&#x3D; &#39;dict&#39;:  # 判断该节点是否为叶节点  </span><br><span class="line">                class\_label &#x3D; classify(feature\_dict\[key\], test)  # 采用递归直到遍历到叶节点  </span><br><span class="line">            else:  </span><br><span class="line">                class\_label &#x3D; feature\_dict\[key\]  </span><br><span class="line">    return class\_label  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>在分类函数完成之后，接下来我们尝试对未知数据进行分类。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test &#x3D; pd.DataFrame(&#123;&quot;hair&quot;: \[&quot;long&quot;\], &quot;voice&quot;: \[&quot;thin&quot;\], &quot;height&quot;: \[163\], &quot;ear\_stud&quot;: \[&quot;yes&quot;\]&#125;)  </span><br><span class="line">test  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>对连续值进行预处理。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test\[&quot;height&quot;\] &#x3D; pd.Series(map(lambda x: choice\_1(int(x), final\_t), test\[&quot;height&quot;\]))  </span><br><span class="line">test  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>分类预测。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classify(tree,test)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([&#39;woman&#39;], dtype&#x3D;object) </span><br></pre></td></tr></table></figure><p>一个身高 163 厘米，长发，带着耳钉且声音纤细的人，在我们构建的决策树判断后预测为一名女性。</p><p>上面的实验中，我们没有考虑 <code>=划分点</code> 的情况，你可以自行尝试将 <code>&gt;=划分点</code> 或 <code>&lt;=划分点</code> 归为一类，看看结果又有哪些不同？</p><h3 id="预剪枝和后剪枝"><a href="#预剪枝和后剪枝" class="headerlink" title="预剪枝和后剪枝"></a><a href="#%E9%A2%84%E5%89%AA%E6%9E%9D%E5%92%8C%E5%90%8E%E5%89%AA%E6%9E%9D" title="预剪枝和后剪枝"></a>预剪枝和后剪枝</h3><p>在决策树的构建过程中，特别在数据特征非常多时，为了尽可能正确的划分每一个训练样本，结点的划分就会不停的重复，则一棵决策树的分支就非常多。对于训练集而言，拟合出来的模型是非常完美的。但是，这种完美就使得整体模型的复杂度变高，同时对其他数据集的预测能力下降，也就是我们常说的过拟合使得模型的泛化能力变弱。为了避免过拟合问题的出现，在决策树中最常见的两种方法就是预剪枝和后剪枝。</p><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a><a href="#%E9%A2%84%E5%89%AA%E6%9E%9D" title="预剪枝"></a>预剪枝</h4><p>预剪枝，顾名思义预先减去枝叶，在构建决策树模型的时候，每一次对数据划分之前进行估计，如果当前节点的划分不能带来决策树泛化的提升，则停止划分并将当前节点标记为叶节点。例如前面构造的决策树，按照决策树的构建原则，通过 <code>height</code> 特征进行划分后 <code>&lt;172</code> 分支中又按照 <code>ear_stud</code> 特征值进行继续划分。如果应用预剪枝，则当通过 <code>height</code> 进行特征划分之后，对 <code>&lt;172</code> 分支是否进行 <code>ear_stud</code> 特征进行划分时计算划分前后的准确度，如果划分后的更高则按照 <code>ear_stud</code> 继续划分，如果更低则停止划分。</p><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a><a href="#%E5%90%8E%E5%89%AA%E6%9E%9D" title="后剪枝"></a>后剪枝</h4><p>跟预剪枝在构建决策树的过程中判断是否继续特征划分所不同的是，后剪枝在决策树构建好之后对树进行修剪。如果说预剪枝是自顶向下的修剪，那么后剪枝就是自底向上进行修剪。后剪枝将最后的分支节点替换为叶节点，判断是否带来决策树泛化的提升，是则进行修剪，并将该分支节点替换为叶节点，否则不进行修剪。例如在前面构建好决策树之后，<code>&gt;172</code>分支的 <code>voice</code> 特征，将其替换为叶节点如（<code>man</code>），计算替换前后划分准确度，如果替换后准确度更高则进行修剪（用叶节点替换分支节点），否则不修剪。</p><h2 id="预测分类"><a href="#预测分类" class="headerlink" title="预测分类"></a><a href="#%E9%A2%84%E6%B5%8B%E5%88%86%E7%B1%BB" title="预测分类"></a>预测分类</h2><p>在前面我们使用 <code>python</code> 将决策树的特征选择，连续值处理和预测分类做了详细的讲解。接下来我们应用决策树模型对真实的数据进行分类预测。</p><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a><a href="#%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE" title="导入数据"></a>导入数据</h3><p>本次应用到的数据为学生成绩数据集 <code>course-13-student.csv</code>，一共有 395 条数据，26 个特征。</p><blockquote><p>数据集下载 👉 <a href="http://labfile.oss.aliyuncs.com/courses/1081/course-13-student.csv">传送门</a></p></blockquote><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;导入数据集并预览  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">import pandas as pd  </span><br><span class="line">  </span><br><span class="line">stu\_grade &#x3D; pd.read\_csv(&#39;course-13-student.csv&#39;)  </span><br><span class="line">stu\_grade.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>由于特征过多，我们选择部分特征作为决策树模型的分类特征,分别为：</p><ul><li><code>school</code>：学生所读学校(<code>GP</code>，<code>MS</code>)</li><li><code>sex</code>: 性别(<code>F</code>：女，<code>M</code>：男)</li><li><code>address</code>: 家庭住址(<code>U</code>：城市，<code>R</code>：郊区)</li><li><code>Pstatus</code>: 父母状态(<code>A</code>：同居，<code>T</code>：分居)</li><li><code>Pedu</code>: 父母学历由低到高</li><li><code>reason</code>: 选择这所学校的原因(<code>home</code>：家庭,<code>course</code>：课程设计，<code>reputation</code>：学校地位，<code>other</code>：其他)</li><li><code>guardian</code>: 监护人(<code>mother</code>：母亲，<code>father</code>：父亲，<code>other</code>：其他)</li><li><code>studytime</code>: 周末学习时长</li><li><code>schoolsup</code>: 额外教育支持(<code>yes</code>：有，<code>no</code>：没有)</li><li><code>famsup</code>: 家庭教育支持(<code>yes</code>：有，<code>no</code>：没有)</li><li><code>paid</code>: 是否上补习班(<code>yes</code>：是，<code>no</code>：否)</li><li><code>higher</code>: 是否想受更好的教育(<code>yes</code>：是，<code>no</code>：否)</li><li><code>internet</code>: 是否家里联网(<code>yes</code>：是，<code>no</code>：否)</li><li><code>G1</code>: 一阶段测试成绩</li><li><code>G2</code>: 二阶段测试成绩</li><li><code>G3</code>: 最终成绩</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new\_data &#x3D; stu\_grade.iloc\[:, \[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 15, 24, 25, 26\]\]  </span><br><span class="line">new\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1" title="数据预处理"></a>数据预处理</h3><p>首先我们将成绩 <code>G1</code>，<code>G2</code>，<code>G3</code> 根据分数进行等级划分，将 <code>0-4</code> 划分为 <code>bad</code>，<code>5-9</code> 划分为 <code>medium</code> ，</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def choice\_2(x):  </span><br><span class="line">    x &#x3D; int(x)  </span><br><span class="line">    if x &lt; 5:  </span><br><span class="line">        return &quot;bad&quot;  </span><br><span class="line">    elif x &gt;&#x3D; 5 and x &lt; 10:  </span><br><span class="line">        return &quot;medium&quot;  </span><br><span class="line">    elif x &gt;&#x3D; 10 and x &lt; 15:  </span><br><span class="line">        return &quot;good&quot;  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;excellent&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">stu\_data &#x3D; new\_data.copy()  </span><br><span class="line">stu\_data\[&quot;G1&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G1&quot;\]))  </span><br><span class="line">stu\_data\[&quot;G2&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G2&quot;\]))  </span><br><span class="line">stu\_data\[&quot;G3&quot;\] &#x3D; pd.Series(map(lambda x: choice\_2(x), stu\_data\[&quot;G3&quot;\]))  </span><br><span class="line">stu\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>同样我们对 <code>Pedu</code> （父母教育程度）也进行划分</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def choice\_3(x):  </span><br><span class="line">    x &#x3D; int(x)  </span><br><span class="line">    if x &gt; 3:  </span><br><span class="line">        return &quot;high&quot;  </span><br><span class="line">    elif x &gt; 1.5:  </span><br><span class="line">        return &quot;medium&quot;  </span><br><span class="line">    else:  </span><br><span class="line">        return &quot;low&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">stu\_data\[&quot;Pedu&quot;\] &#x3D; pd.Series(map(lambda x: choice\_3(x), stu\_data\[&quot;Pedu&quot;\]))  </span><br><span class="line">stu\_data.head()  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>在等级划分之后，为遵循 <code>scikit-learn</code> 函数的输入规范，需要将数据特征进行替换。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;特征值替换  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def replace\_feature(data):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    data -- 数据集  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    data -- 将特征值替换后的数据集  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    for each in data.columns:  # 遍历每一个特征名称  </span><br><span class="line">        feature\_list &#x3D; data\[each\]  </span><br><span class="line">        unique\_value &#x3D; set(feature\_list)  </span><br><span class="line">        i &#x3D; 0  </span><br><span class="line">        for fea\_value in unique\_value:  </span><br><span class="line">            data\[each\] &#x3D; data\[each\].replace(fea\_value, i)  </span><br><span class="line">            i +&#x3D; 1  </span><br><span class="line">    return data  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>将特征值进行替换后展示。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stu\_data &#x3D; replace\_feature(stu\_data)  </span><br><span class="line">stu\_data.head(10)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a><a href="#%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86" title="数据划分"></a>数据划分</h3><p>加载好预处理的数据集之后，为了实现决策树算法，同样我们需要将数据集分为 <strong>训练集</strong>和<strong>测试集</strong>，依照经验：<strong>训练集</strong>占比为 70%，<strong>测试集</strong>占 30%。</p><p>同样在此我们使用 <code>scikit-learn</code> 模块的 <code>train_test_split</code> 函数完成数据集切分。  </p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model\_selection import train\_test\_split  </span><br><span class="line">  </span><br><span class="line">x\_train,x\_test, y\_train, y\_test &#x3D;train\_test\_split(train\_data,train\_target,test\_size&#x3D;0.4, random\_state&#x3D;0)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>x_train</code>,<code>x_test</code>, <code>y_train</code>, <code>y_test</code> 分别表示，切分后的 特征的训练集，特征的测试集，标签的训练集，标签的测试集；其中特征和标签的值是一一对应的。</li><li><code>train_data</code>,<code>train_target</code>分别表示为待划分的特征集和待划分的标签集。</li><li><code>test_size</code>：测试样本所占比例。</li><li><code>random_state</code>：随机数种子,在需要重复实验时，保证在随机数种子一样时能得到一组一样的随机数。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model\_selection import train\_test\_split  </span><br><span class="line">  </span><br><span class="line">x\_train, x\_test, y\_train, y\_test &#x3D; train\_test\_split(stu\_data.iloc\[:, :-1\], stu\_data\[&quot;G3&quot;\],   </span><br><span class="line">                                                    test\_size&#x3D;0.3, random\_state&#x3D;5)  </span><br><span class="line">  </span><br><span class="line">x\_test  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><h3 id="决策树构建"><a href="#决策树构建" class="headerlink" title="决策树构建"></a><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA" title="决策树构建"></a>决策树构建</h3><p>在划分好数据集之后，接下来就是进行预测。在前面的实验中我们采用 <code>python</code> 对决策树算法进行实现，下面我们通过 <code>scikit-learn</code> 来对其进行实现。 <code>scikit-learn</code> 决策树类及常用参数如下：</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(criterion&#x3D;’gini’，random\_state&#x3D;None)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>其中：</p><ul><li><code>criterion</code> 表示特征划分方法选择，默认为 <code>gini</code> (在后面会讲到)，可选择为 <code>entropy</code> (信息增益)。</li><li><code>ramdom_state</code> 表示随机数种子，当特征特别多时 <code>scikit-learn</code> 为了提高效率，随机选取部分特征来进行特征选择，即找到所有特征中较优的特征。</li></ul><p>常用方法:</p><ul><li><code>fit(x,y)</code>训练决策树。</li><li><code>predict(X)</code> 对数据集进行预测返回预测结果。</li></ul><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier  </span><br><span class="line">  </span><br><span class="line">dt\_model &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;entropy&#39;, random\_state&#x3D;34)  </span><br><span class="line">dt\_model.fit(x\_train,y\_train) # 使用训练集训练模型  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(class_weight&#x3D;None, criterion&#x3D;&#39;entropy&#39;, max_depth&#x3D;None,</span><br><span class="line">            max_features&#x3D;None, max_leaf_nodes&#x3D;None,</span><br><span class="line">            min_impurity_decrease&#x3D;0.0, min_impurity_split&#x3D;None,</span><br><span class="line">            min_samples_leaf&#x3D;1, min_samples_split&#x3D;2,</span><br><span class="line">            min_weight_fraction_leaf&#x3D;0.0, presort&#x3D;False, random_state&#x3D;34,</span><br><span class="line">            splitter&#x3D;&#39;best&#39;) </span><br></pre></td></tr></table></figure><h3 id="决策树可视化"><a href="#决策树可视化" class="headerlink" title="决策树可视化"></a><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96" title="决策树可视化"></a>决策树可视化</h3><p>在构建好决策树之后，我们需要对创建好的决策树进行可视化展示，引入 <code>export_graphviz</code> 进行画图。由于环境中没有函数需要进行安装。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\# Linux  </span><br><span class="line">!apt-get install --yes graphviz # 安装所需模块  </span><br><span class="line">!pip install graphviz  </span><br><span class="line">  </span><br><span class="line">\# windows Anaconda  </span><br><span class="line">conda install graphviz  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p>下面开始生成决策树图像，其中生成决策树较大需要拖动滑动条进行查看。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import export\_graphviz  </span><br><span class="line">import graphviz  </span><br><span class="line">  </span><br><span class="line">img &#x3D; export\_graphviz(  </span><br><span class="line">    dt\_model, out\_file&#x3D;None,  </span><br><span class="line">    feature\_names&#x3D;stu\_data.columns\[:-1\].values.tolist(),  # 传入特征名称  </span><br><span class="line">    class\_names&#x3D;np.array(\[&quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;, &quot;excellent&quot;\]),  # 传入类别值  </span><br><span class="line">    filled&#x3D;True, node\_ids&#x3D;True,  </span><br><span class="line">    rounded&#x3D;True)  </span><br><span class="line">  </span><br><span class="line">graphviz.Source(img)  # 展示决策树  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><p><a href="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/output_157_0.svg"><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/decision-making%20tree/output_157_0.svg" alt="svg">svg</a></p><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a><a href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B" title="模型预测"></a>模型预测</h3><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y\_predict &#x3D; dt\_model.predict(x\_test) # 使用模型对测试集进行预测  </span><br><span class="line">y\_predict  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([3, 1, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 3, 0, 3, 2, 0, 3, 2, 2, 0,</span><br><span class="line">       3, 2, 2, 0, 3, 3, 2, 2, 0, 0, 0, 2, 1, 0, 1, 2, 3, 0, 3, 3, 2, 2,</span><br><span class="line">       0, 2, 2, 2, 2, 2, 3, 2, 2, 0, 3, 0, 2, 2, 2, 1, 3, 0, 2, 2, 2, 2,</span><br><span class="line">       3, 3, 2, 0, 0, 0, 1, 2, 2, 0, 0, 3, 0, 3, 2, 3, 2, 2, 3, 1, 0, 0,</span><br><span class="line">       0, 2, 1, 2, 2, 2, 2, 3, 0, 0, 3, 0, 0, 2, 3, 2, 1, 2, 2, 0, 0, 2,</span><br><span class="line">       0, 2, 0, 3, 2, 2, 2, 3, 2], dtype&#x3D;int64) </span><br></pre></td></tr></table></figure><h3 id="分类准确率计算"><a href="#分类准确率计算" class="headerlink" title="分类准确率计算"></a><a href="#%E5%88%86%E7%B1%BB%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97" title="分类准确率计算"></a>分类准确率计算</h3><p>当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。</p><p>accur=∑Ni=1I(¯yi=yi)N(6)</p><p>$$(6)accur=∑i=1NI(yi¯=yi)N$$</p><p>公式(6)中 N$N$ 表示数据总条数，¯yi$yi¯$ 表示第 i$i$ 条数据的种类预测值，yi$yi$ 表示第 i$i$ 条数据的种类真实值，I$I$ 同样是指示函数，表示 ¯yi$yi¯$ 和 yi$yi$ 相同的个数。</p><p>| </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1  </span><br><span class="line">2  </span><br><span class="line">3  </span><br><span class="line">4  </span><br><span class="line">5  </span><br><span class="line">6  </span><br><span class="line">7  </span><br><span class="line">8  </span><br><span class="line">9  </span><br><span class="line">10  </span><br><span class="line">11  </span><br><span class="line">12  </span><br><span class="line">13  </span><br><span class="line">14  </span><br><span class="line">15  </span><br><span class="line">16  </span><br><span class="line">17  </span><br><span class="line">18  </span><br><span class="line">19  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> | </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;准确率计算  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">  </span><br><span class="line">def get\_accuracy(test\_labels, pred\_labels):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    参数:  </span><br><span class="line">    test\_labels -- 测试集的真实值  </span><br><span class="line">    pred\_labels -- 测试集的预测值  </span><br><span class="line">  </span><br><span class="line">    返回:  </span><br><span class="line">    accur -- 准确率  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    correct &#x3D; np.sum(test\_labels &#x3D;&#x3D; pred\_labels)  # 计算预测正确的数据个数  </span><br><span class="line">    n &#x3D; len(test\_labels)  # 总测试集数据个数  </span><br><span class="line">    accur &#x3D; correct&#x2F;n  </span><br><span class="line">    return accur  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">get\_accuracy(y\_test, y\_predict)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p> |</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.6974789915966386 </span><br></pre></td></tr></table></figure><h2 id="CART-决策树"><a href="#CART-决策树" class="headerlink" title="CART 决策树"></a><a href="#CART-%E5%86%B3%E7%AD%96%E6%A0%91" title="CART 决策树"></a>CART 决策树</h2><p>分类与回归树（classification and regression tree, CART）同样也是应用广泛的决策树学习算法，CART 算法是按照特征划分，由树的生成和树的剪枝构成，既可以进行分类又可以用于回归，按照作用将其分为决策树和回归树，由于本实验设计为决策树的概念，所以回归树的部分有兴趣的同学可以自己查找相关资料进一步学习。</p><p>CART决策树的构建和常见的 <strong>ID3</strong> 和 <strong>C4.5</strong> 算法的流程相似，但在特征划分选择上CART选择了 <strong>基尼指数</strong> 作为划分标准。数据集 D$D$ 的纯度可用基尼值来度量：</p><p>Gini(D)=|y|∑y=1∑k′≠kpkp′k(7)</p><p>$$(7)Gini(D)=∑y=1|y|∑k′≠kpkpk′$$</p><p><strong>基尼指数</strong>表示随机抽取两个样本，两个样本类别不一致的概率，<strong>基尼指数</strong>越小则数据集的纯度越高。同样对于每一个特征值的基尼指数计算，其和 <strong>ID3</strong> 、 <strong>C4.5</strong> 相似，定义为：</p><p>GiniValue(D,a)=M∑m=1|Dm||D|Gini(Dm)(8)</p><p>$$(8)GiniValue(D,a)=∑m=1M|Dm||D|Gini(Dm)$$</p><p>在进行特征划分的时候，选择特征中基尼值最小的作为最优特征划分点。</p><p>实际上，在应用过程中，更多的会使用 <strong>基尼指数</strong> 对特征划分点进行决策，最重要的原因是计算复杂度相较于 <strong>ID3</strong> 和 <strong>C4.5</strong> 小很多（没有对数运算）。</p><p><strong>拓展阅读：</strong></p><ul><li><a href="https://zh.wikipedia.org/zh-hans/%E5%86%B3%E7%AD%96%E6%A0%91">决策树- 维基百科</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。  &lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="决策树" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Decision Tree" scheme="http://www.laugh12321.cn/blog/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|感知机和人工神经网络详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/31/neural_network/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/31/neural_network/</id>
    <published>2019-01-31T00:00:00.000Z</published>
    <updated>2020-10-23T09:06:50.604Z</updated>
    
    <content type="html"><![CDATA[<p>人工神经网络是一种发展时间较早且十分常用的机器学习算法。因其模仿人类神经元工作的特点，在监督学习和非监督学习领域都给予了人工神经网络较高的期望。目前，由传统人工神经网络发展而来的卷积神经网络、循环神经网络已经成为了深度学习的基石。本篇文章中，我们将从人工神经网络的原型感知机出发，介绍机器学习中人工神经网络的特点及应用。  </p><a id="more"></a><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>在介绍人工神经网络之前，我们先介绍它的原型：感知机。关于感知机，我们先引用一段来自维基百科的背景介绍：</p><blockquote><p>感知器（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。</p></blockquote><p>如果你之前从未接触过人工神经网络，那么上面这句话或许还需要等到阅读完这篇文章才能完整理解。不过，你可以初步发现，感知机其实就是人工神经网络，只不过是其初级形态。</p><h3 id="感知机的推导过程"><a href="#感知机的推导过程" class="headerlink" title="感知机的推导过程"></a>感知机的推导过程</h3><p>那么，<strong>感知机到底是什么？它是怎样被发明出来的呢？</strong></p><p>要搞清楚上面的问题，我们就需要提到前面学习过的一个非常熟悉的知识点：线性回归。回忆关于逻辑回归的内容，你应该还能记得当初我们说过逻辑回归起源于线性回归。而感知机作为一种最简单的二分类模型，它其实就是使用了线性回归的方法完成平面数据点的分类。而逻辑回归后面引入了逻辑斯蒂估计来计算分类概率的方法甚至可以被当作是感知机的进步。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/1.png"></center><p>你还记得上面这张图片吗？当数据点处于线性可分时，我们可以使用一条直线将其分开，而分割线的函数为：</p><p>$$<br>f(x) = w_1x_1+w_2x_2+ \cdots +w_nx_n + b = WX+b \tag{1}<br>$$</p><p>对于公式（1）而言，我们可以认为分割直线方程其实就是针对数据集的每一个特征 $x_1,x_2,⋯,x_n$ 依次乘上权重 $w_1,w_2,⋯,w_n$ 所得。</p><p>当我们确定好公式（1）的参数后，每次输入一个数据点对应的特征 $x_1,x_2,⋯,x_n$ 就能得到对应的函数值 $f(x)$。那么，<strong>怎样判定这个数据点属于哪一个类别呢？</strong></p><p>在二分类问题中，我们最终的类别共有两个，通常被称之为正类别和负类别。而当我们使用线性回归中对应的公式（1）完成分类时，不同于逻辑回归中将 $f(x)$ 传入 <code>sigmoid</code> 函数，这里我们将 $f(x)$ 传入如下所示的 <code>sign</code> 函数。</p><p>$$<br>sign(x) =<br>\begin{cases}<br>+1, &amp; \text{if } x  \geq  0 \\<br>-1, &amp; \text{if } x  &lt;  0<br>\end{cases}<br>\tag{2}<br>$$</p><p><code>sign()</code> 函数又被称之为符号函数，它的函数值只有 <code>2</code> 个。即当自变量 $x≥0$ 时，因变量为 <code>1</code>。同理，当 $x&lt;0$ 时，因变量为 <code>-1</code>。函数图像如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/2.PNG"></center><p>于是，当我们将公式（1）中的 $f(x)$ 传入公式（2），就能得到 $sign(f(x))$ 的值。其中，当 $sign(f(x))=1$ 时，就为正分类点，而 $sign(f(x))=−1$ 时，则为负分类点。</p><p><strong>综上所示</strong>，我们就假设输入空间(特征向量)为 $X \subseteq R^n$，输出空间为 $Y=−1,+1$。输入 $x⊆X$ 表示实例的特征向量，对应于输入空间的点；输出 $y⊆Y$ 表示示例的类别。由输入空间到输出空间的函数如下：</p><p>$$<br>f(x) = sign(w*x +b) \tag{3}<br>$$</p><p><strong>公式（3）就被称之为感知机</strong>。注意，公式（3）中的 $f(x)$ 和公式（1）中的 $f(x)$ 不是同一个 $f(x)$。</p><h3 id="感知机计算流程图"><a href="#感知机计算流程图" class="headerlink" title="感知机计算流程图"></a>感知机计算流程图</h3><p>上面，我们针对感知机进行了数学推导。为了更加清晰地展示出感知机的计算过程，我们将其绘制成如下所示的流程图。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/3.png"></center><h3 id="感知机的损失函数"><a href="#感知机的损失函数" class="headerlink" title="感知机的损失函数"></a>感知机的损失函数</h3><p>前面的文章中，我们已经介绍过损失函数的定义。在感知机的学习过程中，我们同样需要确定每一个特征变量对应的参数，而损失函数的极小值往往就意味着参数最佳。那么，感知机学习的策略，也就是其通常采用哪种形式的损失函数呢？</p><p>如下图所示，当我们使用一条直线去分隔一个线性可分的数据集时，有可能会出现「误分类」的状况。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/4.png"></center><p>而在感知机的学习过程中，我们通常会使用误分类点到分割线（面）的距离去定义损失函数。</p><h4 id="点到直线的距离"><a href="#点到直线的距离" class="headerlink" title="点到直线的距离"></a>点到直线的距离</h4><p>中学阶段，我们学过点到直线的距离公式推导。对于 $n$ 维实数向量空间中任意一点 $x_0$ 到直线 $W⋅x+b=0$ 的距离为：</p><p>$$<br>d= \dfrac{1}{\parallel W\parallel}|W*x_{0}+b| \tag{4}<br>$$</p><p>其中 $||W||$ 表示 $L_2$ 范数，即向量各元素的平方和然后开方。</p><p>然后，对于误分类点 $(xi,yi)$ 来讲，公式（5）成立。</p><p>$$<br>y_i(W * x_{i}+b)&gt;0 \tag{5}<br>$$</p><p>那么，误分类点 $(xi,yi)$ 到分割线（面）的距离就为：</p><p>$$<br>d=-\dfrac{1}{\parallel W\parallel}y_i(W*x_{i}+b) \tag{6}<br>$$</p><p>于是，假设所有误分类点的集合为 $M$，全部误分类点到分割线（面）的距离就为：</p><p>$$<br>-\dfrac{1}{\parallel W\parallel}\sum_{x_i\epsilon M} y_i(W*x_{i}+b) \tag{7}<br>$$</p><p><strong>最后得到感知机的损失函数</strong>为：</p><p>$$<br>J(W,b) = - \sum_{x_i\epsilon M} y_i(W*x_{i}+b) \tag{8}<br>$$</p><p>从公式（8）可以看出，损失函数 $J(W,b)$ 是非负的。也就是说，当没有误分类点时，损失函数的值为 <code>0</code>。同时，误分类点越少，误分类点距离分割线（面）就越近，损失函数值就越小。同时，损失函数 $J(W,b)$ 是连续可导函数。</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>当我们在实现分类时，最终想要的结果肯定是没有误分类的点，也就是损失函数取极小值时的结果。在逻辑回归的中，为了找到损失函数的极小值，我们使用到了一种叫做梯度下降法（Gradient descent）。而在这篇中，我们尝试一种梯度下降法的改进方法，也称之为随机梯度下降法（Stochastic gradient descent，简称：SGD)。</p><p>实验 SGD 计算公式（8）的极小值时，首先任选一个分割面 $W_0$ 和 $b_0$，然后使用梯度下降法不断地极小化损失函数：</p><p>$$<br>min_{W,b} J(W,b) = - \sum_{x_i\epsilon M} y_i(W*x_{i}+b) \tag{9}<br>$$</p><p>随机梯度下降的特点在于，极小化过程中不是一次针对 $M$ 中的所有误分类点执行梯度下降，而是每次<strong>随机</strong>选取一个误分类点执行梯度下降。等到更新完 $W$ 和 $b$ 之后，下一次再另随机选择一个误分类点执行梯度下降直到收敛。</p><p>计算损失函数的偏导数：</p><p>$$<br>\frac{\partial J(W,b)}{\partial W} = - \sum_{x_i\epsilon M}y_ix_i \ \frac{\partial J(W,b)}{\partial b} = - \sum_{x_i\epsilon M}y_i \tag{10}<br>$$</p><p>如果 $yi(W∗xi+b)≤0$ 更新 $W$ 和 $b$：</p><p>$$<br>W \leftarrow   W + \lambda y_ix_i \ b \leftarrow  b + \lambda y_i \tag{11}<br>$$</p><p>同前面的梯度下降一致，$λ$ 为学习率，也就是每次梯度下降的步长。</p><p>下面，我们使用 Python 将上面的随机梯度下降算法进行实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">感知机随机梯度下降算法实现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceptron_sgd</span>(<span class="params">X, Y, alpha, epochs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 自变量数据矩阵</span></span><br><span class="line"><span class="string">    Y -- 因变量数据矩阵</span></span><br><span class="line"><span class="string">    alpha -- lamda 参数</span></span><br><span class="line"><span class="string">    epochs -- 迭代次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    w -- 权重系数</span></span><br><span class="line"><span class="string">    b -- 截距项</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w = np.zeros(<span class="built_in">len</span>(X[<span class="number">0</span>])) <span class="comment"># 初始化参数为 0</span></span><br><span class="line">    b = np.zeros(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs): <span class="comment"># 迭代</span></span><br><span class="line">        <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(X):</span><br><span class="line">            <span class="keyword">if</span> ((np.dot(X[i], w)+b)*Y[i]) &lt;= <span class="number">0</span>: <span class="comment"># 判断条件</span></span><br><span class="line">                w = w + alpha*X[i]*Y[i] <span class="comment"># 更新参数</span></span><br><span class="line">                b = b + alpha*Y[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h3 id="感知机分类实例"><a href="#感知机分类实例" class="headerlink" title="感知机分类实例"></a>感知机分类实例</h3><p>前面的内容中，我们讨论了感知机的计算流程，感知机的损失函数，以及如何使用随机梯度下降求解感知机的参数。理论说了这么多，下面就举一个实际的例子看一看。</p><h4 id="示例数据集"><a href="#示例数据集" class="headerlink" title="示例数据集"></a>示例数据集</h4><p>为了方便绘图到二维平面，这里只使用包含两个特征变量的数据，数据集名称为 <code>course-12-data.csv</code>。</p><blockquote><p>数据集下载 👉 <a href="http://labfile.oss.aliyuncs.com/courses/1081/course-12-data.csv">传送门</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;加载数据集</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;course-12-data.csv&quot;</span>, header=<span class="number">0</span>) <span class="comment"># 加载数据集</span></span><br><span class="line">df.head() <span class="comment"># 预览前 5 行数据</span></span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_00.png"></center><p>可以看到，该数据集共有两个特征变量 <code>X0</code> 和 <code>X1</code>, 以及一个目标值 <code>Y</code>。其中，目标值 <code>Y</code> 只包含 <code>-1</code> 和 <code>1</code>。我们尝试将该数据集绘制成图，看一看数据的分布情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制数据集</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.scatter(df[<span class="string">&#x27;X0&#x27;</span>],df[<span class="string">&#x27;X1&#x27;</span>], c=df[<span class="string">&#x27;Y&#x27;</span>])</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_71_1.png"></center><h4 id="感知机训练"><a href="#感知机训练" class="headerlink" title="感知机训练"></a>感知机训练</h4><p>接下来，我们就使用感知机求解最佳分割线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = df[[<span class="string">&#x27;X0&#x27;</span>,<span class="string">&#x27;X1&#x27;</span>]].values</span><br><span class="line">Y = df[<span class="string">&#x27;Y&#x27;</span>].values</span><br><span class="line"></span><br><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">epochs = <span class="number">150</span></span><br><span class="line"></span><br><span class="line">perceptron_sgd(X, Y, alpha, epochs)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([ 4.93, -6.98]), array([-3.3])) </span><br></pre></td></tr></table></figure><p>于是，我们求得的最佳分割线方程为：</p><p>$$<br>f(x) = 4.93 * x_{1} - 6.98 * x_{2} -3.3 \tag{12}<br>$$</p><p>此时，可以求解一下分类的正确率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">L = perceptron_sgd(X, Y, alpha, epochs)</span><br><span class="line">w1 = L[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">w2 = L[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">b = L[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">z = np.dot(X, np.array([w1, w2]).T) + b</span><br><span class="line">np.sign(z)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,</span><br><span class="line">       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,</span><br><span class="line">       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,</span><br><span class="line">       -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,</span><br><span class="line">        1.,  1.,  1.,  1.,  1.,  1.,  1.]) </span><br></pre></td></tr></table></figure><p>为了方便，我们就直接使用 <code>scikit-learn</code> 提供的准确率计算方法 <code>accuracy_score()</code>，该方法相信你已经非常熟悉了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">accuracy_score(Y, np.sign(z))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9866666666666667 </span><br></pre></td></tr></table></figure><p>所以，最终的分类准确率约为 <code>0.987</code>。</p><h4 id="绘制决策边界线"><a href="#绘制决策边界线" class="headerlink" title="绘制决策边界线"></a>绘制决策边界线</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制轮廓线图，不需要掌握</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.scatter(df[<span class="string">&#x27;X0&#x27;</span>],df[<span class="string">&#x27;X1&#x27;</span>], c=df[<span class="string">&#x27;Y&#x27;</span>])</span><br><span class="line"></span><br><span class="line">x1_min, x1_max = df[<span class="string">&#x27;X0&#x27;</span>].<span class="built_in">min</span>(), df[<span class="string">&#x27;X0&#x27;</span>].<span class="built_in">max</span>(),</span><br><span class="line">x2_min, x2_max = df[<span class="string">&#x27;X1&#x27;</span>].<span class="built_in">min</span>(), df[<span class="string">&#x27;X1&#x27;</span>].<span class="built_in">max</span>(),</span><br><span class="line"></span><br><span class="line">xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))</span><br><span class="line">grid = np.c_[xx1.ravel(), xx2.ravel()]</span><br><span class="line"></span><br><span class="line">probs = (np.dot(grid, np.array([L[<span class="number">0</span>][<span class="number">0</span>], L[<span class="number">0</span>][<span class="number">1</span>]]).T) + L[<span class="number">1</span>]).reshape(xx1.shape)</span><br><span class="line">plt.contour(xx1, xx2, probs, [<span class="number">0</span>], linewidths=<span class="number">1</span>, colors=<span class="string">&#x27;red&#x27;</span>);</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_87_0.png"></center><p>可以看到，上图中的红色直线就是我们最终的分割线，分类的效果还是不错的。</p><h4 id="绘制损失函数变换曲线"><a href="#绘制损失函数变换曲线" class="headerlink" title="绘制损失函数变换曲线"></a>绘制损失函数变换曲线</h4><p>除了绘制决策边界，也就是分割线。我们也可以将损失函数的变化过程绘制处理，看一看梯度下降的执行过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算每次迭代后的损失函数值</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceptron_loss</span>(<span class="params">X, Y, alpha, epochs</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 自变量数据矩阵</span></span><br><span class="line"><span class="string">    Y -- 因变量数据矩阵</span></span><br><span class="line"><span class="string">    alpha -- lamda 参数</span></span><br><span class="line"><span class="string">    epochs -- 迭代次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    loss_list -- 每次迭代损失函数值列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    w = np.zeros(<span class="built_in">len</span>(X[<span class="number">0</span>])) <span class="comment"># 初始化参数为 0</span></span><br><span class="line">    b = np.zeros(<span class="number">1</span>)</span><br><span class="line">    loss_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs): <span class="comment"># 迭代</span></span><br><span class="line">        loss_init = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(X):</span><br><span class="line">            <span class="keyword">if</span> ((np.dot(X[i], w)+b)*Y[i]) &lt;= <span class="number">0</span>: <span class="comment"># 判断条件</span></span><br><span class="line">                loss_init += (((np.dot(X[i], w)+b)*Y[i]))</span><br><span class="line">                w = w + alpha*X[i]*Y[i] <span class="comment"># 更新参数</span></span><br><span class="line">                b = b + alpha*Y[i]</span><br><span class="line">        loss_list.append(loss_init * <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss_list</span><br><span class="line">    </span><br><span class="line">loss_list = perceptron_loss(X, Y, alpha, epochs)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(loss_list))], loss_list)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Learning rate &#123;&#125;, Epochs &#123;&#125;&quot;</span>.<span class="built_in">format</span>(alpha, epochs))</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss function&quot;</span>)</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_94_1.png"></center><p>如上图所示，你会发现，让我们按照 <code>0.1</code> 的学习率迭代 <code>150</code> 次后，损失函数依旧无法到达 <code>0</code>。一般情况下，当我们的数据不是线性可分时，损失函数就会出现如上图所示的震荡线性。</p><p>不过，如果你仔细观察上方数据的散点图，你会发现这个数据集看起来是线性可分的。那么，当数据集线性可分，却造成损失函数变换曲线震荡的原因一般有两点：<strong>学习率太大</strong>或者<strong>迭代次数太少</strong>。</p><p>其中，迭代次数太少很好理解，也就是说我们迭代的次数还不足以求得极小值。至于学习率太大，可以看下方的示意图。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/5.png"></center><p>如上图所示，当我们的学习率太大时，往往容易出现在损失函数底部来回震荡的现象而无法到达极小值点。所以，面对上面这种情况，我们可以采取减小学习率 + 增加迭代次数的方法找到损失函数极小值点。</p><p>所以，下面就再试一次。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">alpha &#x3D; 0.05 # 减小学习率</span><br><span class="line">epochs &#x3D; 1000 # 增加迭代次数</span><br><span class="line"></span><br><span class="line">loss_list &#x3D; perceptron_loss(X, Y, alpha, epochs)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(10, 6))</span><br><span class="line">plt.plot([i for i in range(len(loss_list))], loss_list)</span><br><span class="line">plt.xlabel(&quot;Learning rate &#123;&#125;, Epochs &#123;&#125;&quot;.format(alpha, epochs))</span><br><span class="line">plt.ylabel(&quot;Loss function&quot;)</span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_101_1.png"></center><p>可以看到，当迭代次数约为 <code>700</code> 次，即上图后半段时，损失函数的值等于 <code>0</code>。根据我们在 <code>1.3</code> 小节中介绍的内容，当损失函数为 <code>0</code> 时，就代表没有误分类点存在。</p><p>此时，我们再一次计算分类准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">L = perceptron_sgd(X, Y, alpha, epochs)</span><br><span class="line">z = np.dot(X, L[<span class="number">0</span>].T) + L[<span class="number">1</span>]</span><br><span class="line">accuracy_score(Y, np.sign(z))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0 </span><br></pre></td></tr></table></figure><p>和损失函数变化曲线得到的结论一致，分类准确率已经 <code>100%</code>，表示全部数据点被正确分类。</p><h2 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h2><center><iframe src="//player.bilibili.com/player.html?aid=15532370&amp;cid=25368631&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="450px" width="100%"> </iframe></center><div style="color: #999;font-size: 12px;text-align: center;">神经网络的结构 | 视频来源：<a href="https://www.bilibili.com/video/av15532370" target="_blank" rel="noopener">3Blue1Brown</a></div><p>上面的内容中，我们已经了解到了什么是感知机，以及如何构建一个感知机分类模型。你会发现，感知机只能处理二分类问题，且必须是线性可分问题。如果是这样的话，该方法的局限性就比较大了。那么，面对线性不可分或者多分类问题时，我们有没有一个更好的方法呢？</p><h3 id="多层感知机与人工神经网络"><a href="#多层感知机与人工神经网络" class="headerlink" title="多层感知机与人工神经网络"></a>多层感知机与人工神经网络</h3><p>这里，就要提到本文的主角，也就是人工神经网络（英语：Artificial neural network，简称：ANN）。如果你第一次接触到人工神经网络，不要将其想的太神秘。其实，上面的感知机模型就是一个人工神经网络，只不过它是一个结构简单的单层神经网络。而如果我们要解决线性不可分或者多分类问题，往往会尝试将多个感知机组合在一起，变成一个更复杂的神经网络结构。</p><div style="color: #999;font-size: 12px;font-style: italic;">由于一些历史遗留问题，感知机、多层感知机、人工神经网络三种说法界限模糊，文中介绍到的人工神经网络从某种意义上代指多层感知机。</div><p>在上文 <code>1.2</code> 小节中，我们通过一张图展示了感知机的工作流程，我们将该流程图进一步精简如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/6.png"></center><p>这张图展示了一个感知机模型的执行流程。我们可以把输入称之为「输入层」，输出称之为「输出层」。对于像这样只包含一个输入层的网络结构就可以称之为单层神经网络结构。</p><p>单个感知机组成了单层神经网络，如果我们将一个感知机的输出作为另一个感知机的输入，就组成了多层感知机，也就是一个多层神经网络。其中，我们将输入和输出层之间的称为隐含层。如下图所示，这就是包含 <code>1</code> 个隐含层的神经网络结构。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/7.png"></center><p>一个神经网络结构在计算层数的时候，我们一般只计算输入和隐含层的数量，即上方是一个 <code>2</code> 层神经网络结构。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>目前，我们已经接触过逻辑回归、感知机、多层感知机与人工神经网络 <code>4</code> 个概念。你可能隐约感觉到，似乎这 <code>4</code> 种方法都与线性函数有关，而区别在于对线性函数的因变量的不同处理方式上面。</p><p>$$<br>f(x) = w_1x_1+w_2x_2+ \cdots +w_nx_n + b = WX+b \tag{13}<br>$$</p><ul><li>对于逻辑回归而言，我们是采用了 $sigmoid$ 函数将 $f(x)$ 转换为概率，最终实现二分类。</li><li>对于感知机而言，我们是采用了 $sign$ 函数将 $f(x)$ 转换为 <code>-1 和 +1</code> 最终实现二分类。</li><li>对于多层感知机而言，具有多层神经网络结构，在 $f(x)$ 的处理方式上，一般会有更多的操作。</li></ul><p>于是，$sigmoid$ 函数和 $sign$ 函数还有另外一个称谓，叫做「激活函数（Activation function）」。听到激活函数，大家首先不要觉得它有多么的高级。之所以有这样一个称谓，是因为函数本身有一些特点，但归根结底还是数学函数。下面，我们就列举一下常见的激活函数及其图像。</p><h4 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="$sigmoid$ 函数"></a>$sigmoid$ 函数</h4><p>$sigmoid$ 函数应该已经非常熟悉了吧，它的公式如下：</p><p>$$<br>sigmoid(x)=\frac{1}{1+e^{-x}} \tag{14}<br>$$</p><p>$sigmoid$ 函数的图像呈 S 型，函数值介于 $(0,1)$ 之间：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/8.png"></center><h4 id="Tanh-函数"><a href="#Tanh-函数" class="headerlink" title="$Tanh$ 函数"></a>$Tanh$ 函数</h4><p>$Tanh$ 函数与 $sigmoid$ 函数的图像很相似，都呈 S 型，只不过 $Tanh$ 函数值介于 $(-1,1)$ 之间，公式如下：</p><p>$$<br>tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}\tag{15}<br>$$</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/9.png"></center><h4 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="$ReLU$ 函数"></a>$ReLU$ 函数</h4><p>$ReLU$ 函数全称叫做 Rectified Linear Unit，也就是修正线性单元，公式如下：</p><p>$$<br>ReLU(x) = max(0,x)\tag{16}<br>$$</p><p>$ReLU$ 有很多优点，比如收敛速度会较快且不容易出现梯度消失。由于这次不会用到，我们之后再说。$ReLU$的图像如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/10.png"></center><h4 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h4><p>上面列举了 3 种常用的激活函数，其中 $sigmoid$ 函数是介绍的人工神经网络中十分常用的一种激活函数。谈到激活函数的作用，直白地讲就是针对数据进行非线性变换。只是不同的激活函数适用于不同的场景，而这些都是机器学习专家根据应用经验总结得到的。</p><p>在神经网络结构中，我们通过线性函数不断的连接输入和输出。你可以设想，在这种结构中，每一层输出都是上层输入的线性变换。于是，无论神经网络有多少层，最终输出都是输入的线性组合。这样的话，单层神经网络和多层神经网络有什么区别呢？（没有区别）</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/11.png"></center><p>如上图所示，线性变换的多重组合依旧还是线性变换。如果我们在网络结构中加入激活函数，就相当于引入了非线性因素，这样就可以解决线性模型无法完成的分类任务。</p><h3 id="反向传播算法（BP）直观认识"><a href="#反向传播算法（BP）直观认识" class="headerlink" title="反向传播算法（BP）直观认识"></a>反向传播算法（BP）直观认识</h3><center><iframe src="//player.bilibili.com/player.html?aid=16577449&amp;cid=27038097&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="450px" width="100%"> </iframe></center><div style="color: #999;font-size: 12px;text-align: center;">直观理解反向传播 | 视频来源：<a href="https://www.bilibili.com/video/av16577449/?p=1" target="_blank" rel="noopener">3Blue1Brown</a></div><p>前面感知机的章节中，我们定义了一个损失函数，并通过一种叫做随机梯度下降的方法去求解最优参数。如果你仔细观察随机梯度下降的过程，其实就是通过求解偏导数并组合成梯度用于更新权重 $W$ 和 $b$。感知机只有一层网络结构，求解梯度的过程还比较简单。但是，当我们组合成多层神经网络之后，更新权重的过程就变得复杂起来，而反向传播算法正是为了快速求解梯度而生。</p><p>反向传播的算法说起来很简单，但要顺利理解还比较复杂。这里，我们引用了波兰 AGH 科技大学的一篇 <a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html">科普文章</a> 中的配图来帮助理解反向传播的过程。</p><p>下图呈现了一个经典的 <code>3</code> 层神经网络结构，其包含有 <code>2</code> 个输入 $x_1$ 和 $x_2$ 以及 <code>1</code> 个输出 $y$。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/12.png"></center><p>网络中的每个紫色单元代表一个独立的神经元，它分别由两个单元组成。一个单元是权重和输入信号，而另一个则是上面提到的激活函数。其中，$e$ 代表激活信号，所以 $y=f(e)$ 就是被激活函数处理之后的非线性输出，也就是整个神经元的输出。</p><div style="color: #999;font-size: 12px;font-style: italic;">注：此处与下文使用g()作为激活函数稍有不同</div><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/13.png"></center><p>下面开始训练神经网络，训练数据由输入信号 $x_1$ 和 $x_2$ 以及期望输出 $z$ 组成，首先计算第 1 个隐含层中第 1 个神经元 $y1=f1(e)$ 对应的值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/14.png"></center><p>接下来，计算第 1 个隐含层中第 2 个神经元 $y2=f2(e)$ 对应的值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/15.png"></center><p>然后是计算第 1 个隐含层中第 3 个神经元 $y3=f3(e)$ 对应的值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/16.png"></center><p>与计算第 1 个隐含层的过程相似，我们可以计算第 2 个隐含层的数值。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/17.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/18.png"></center><p>最后，得到输出层的结果：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/19.png"></center><p>上面这个过程被称为前向传播过程，那什么是反向传播呢？接着来看：</p><p>当我们得到输出结果 $y$ 时，可以与期望输出 $z$ 对比得到误差 $δ$。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/20.png"></center><p>然后，我们将计算得到的误差 $δ$ 沿着神经元回路反向传递到前 1 个隐含层，而每个神经元对应的误差为传递过来的误差乘以权重。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/21.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/22.png"></center><p>同理，我们将第 2 个隐含层的误差继续向第 1 个隐含层反向传递。</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/23.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/24.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/25.png"></center><p>此时，我们就可以利用反向传递过来的误差对从输入层到第 1 个隐含层之间的权值 $w$ 进行更新，如下图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/26.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/27.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/28.png"></center><p>同样，对第 1 个隐含层与第 2 个隐含层之间的权值 $w$ 进行更新，如下图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/29.png"></center><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/30.png"></center><p>最后，更新第 2 个隐含层与输出层之间的权值 $w$ ，如下图所示：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/31.png"></center><p>图中的 $η$ 表示学习速率。这就完成了一个迭代过程。更新完权重之后，又开始下一轮的前向传播得到输出，再反向传播误差更新权重，依次迭代下去。</p><p>所以，反向传播其实代表的是反向传播误差。</p><h3 id="使用-Python-实现人工神经网络"><a href="#使用-Python-实现人工神经网络" class="headerlink" title="使用 Python 实现人工神经网络"></a>使用 Python 实现人工神经网络</h3><p>上面的内容，我们介绍了人工神经网络的构成和最重要的反向传播算法。接下来，尝试通过 Python 来实现一个神经网络运行的完整流程。</p><h4 id="定义神经网络结构"><a href="#定义神经网络结构" class="headerlink" title="定义神经网络结构"></a>定义神经网络结构</h4><p>为了让推导过程足够清晰，这里我们只构建包含 1 个隐含层的人工神经网络结构。其中，输入层为 2 个神经元，隐含层为 3 个神经元，并通过输出层实现 2 分类问题的求解。该神经网络的结构如下：</p><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/32.png"></center><p>在此中，我们使用的激活函数为 $sigmoid$ 函数：</p><p>$$<br>\mathit{sigmoid}(x) = \frac{1}{1+e^{-x}}       \tag{17a}<br>$$</p><p>由于下面要使用 $sigmoid$ 函数的导数，所以同样将其导数公式写出来：</p><p>$$<br>\Delta \mathit{sigmoid}(x)  = \mathit{sigmoid}(x)(1 - \mathit{sigmoid}(x))    \tag{17b}<br>$$</p><p>然后，我们通过 Python 实现公式（17）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid 函数求导</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(x) * (<span class="number">1</span> - sigmoid(x))</span><br></pre></td></tr></table></figure><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>前向（正向）传播中，每一个神经元的计算流程为：<strong>线性变换 → 激活函数→输出值</strong>。</p><p>同时，我们约定：</p><ul><li>$Z$ 表示隐含层输出，$Y$ 则为输出层最终输出。</li><li>$w_ij$ 表示从第 $i$ 层的第 $j$ 个权重。</li></ul><p>于是，上图中的前向传播的代数计算过程如下。</p><p>神经网络的输入 $X$，第一层权重 $W_1$，第二层权重 $W_2$。为了演示方便，$X$ 为单样本，因为是矩阵运算，我们很容易就能扩充为多样本输入。</p><p>$$<br>X = \begin{bmatrix} x_{1} &amp; x_{2} \end{bmatrix} \tag{18}<br>$$</p><p>$$<br>W_1 = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13}\  w_{14} &amp; w_{15} &amp; w_{16}\  \end{bmatrix} \tag{19}<br>$$</p><p>$$<br>W_2 = \begin{bmatrix} w_{21} \ w_{22} \ w_{23} \end{bmatrix} \tag{20}<br>$$</p><p>接下来，计算隐含层神经元输出 $Z$（线性变换 → 激活函数）。同样，为了使计算过程足够清晰，我们这里将截距项表示为 0。</p><p>$$<br>Z = \mathit{sigmoid}(X \cdot W_{1}) \tag{21}<br>$$</p><p>最后，计算输出层 $Y$（线性变换 → 激活函数）：</p><p>$$<br>Y = \mathit{sigmoid}(Z \cdot W_{2}) \tag{22}<br>$$</p><p>下面实现前向传播计算过程，将上面的公式转化为代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例样本</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y = np.array([[<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">X, y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([[1, 1]]), array([[1]])) </span><br></pre></td></tr></table></figure><p>然后，随机初始化隐含层权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W1 = np.random.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">W2 = np.random.rand(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">W1, W2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array([[0.4985742 , 0.16703231, 0.51487393],</span><br><span class="line">        [0.63075313, 0.46386686, 0.44365266]]), array([[0.69320812],</span><br><span class="line">        [0.74352002],</span><br><span class="line">        [0.2403471 ]])) </span><br></pre></td></tr></table></figure><p>前向传播的过程实现基于公式（21）和公式（22）完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input_layer = X <span class="comment"># 输入层</span></span><br><span class="line">hidden_layer = sigmoid(np.dot(input_layer, W1)) <span class="comment"># 隐含层，公式 20</span></span><br><span class="line">output_layer = sigmoid(np.dot(hidden_layer, W2)) <span class="comment"># 输出层，公式 22</span></span><br><span class="line"></span><br><span class="line">output_layer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.76546658]]) </span><br></pre></td></tr></table></figure><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><center><div class="media-wrap"><iframe src="//player.bilibili.com/player.html?aid=16577449&amp;cid=27038098&amp;page=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="450px" width="100%"></iframe></div></center><div style="color: #999;font-size: 12px;text-align: center;">反向传播的微积分原理 | 视频来源：<a href="https://www.bilibili.com/video/av16577449/?p=2" target="_blank" rel="noopener">3Blue1Brown</a></div><p>接下来，我们使用梯度下降法的方式来优化神经网络的参数。那么首先需要定义损失函数，然后计算损失函数关于神经网络中各层的权重的偏导数（梯度）。</p><p>此时，设神经网络的输出值为 <code>Y</code>，真实值为 <code>y</code>。然后，定义平方损失函数如下：</p><p>$$<br>Loss(y, Y) = \sum (y - Y)^2 \tag{23}<br>$$</p><p>接下来，求解梯度 $\frac{\partial Loss(y, Y)}{\partial{W_2}}$，需要使用链式求导法则：</p><p>$$<br>\frac{\partial Loss(y, Y)}{\partial{W_2}} = \frac{\partial Loss(y, Y)}{\partial{Y}} \frac{\partial Y}{\partial{W_2}}\tag{24a}<br>$$</p><p>$$<br>\frac{\partial Loss(y, Y)}{\partial{W_2}} = 2(Y-y) * \Delta \mathit{sigmoid}(Z \cdot W_2) \cdot Z\tag{24b}<br>$$</p><p>同理，梯度 $\frac{\partial Loss(y, Y)}{\partial{W_1}}$ 得：</p><p>$$<br>\frac{\partial Loss(y, Y)}{\partial{W_1}} = \frac{\partial Loss(y, Y)}{\partial{Y}} \frac{\partial Y }{\partial{Z}} \frac{\partial Z}{\partial{W_1}} \tag{25a}<br>$$</p><p>$$<br>\frac{\partial Loss(y, Y)}{\partial{W_1}} = 2(Y-y) * \Delta \mathit{sigmoid}(Z \cdot W_2) \cdot W_2 * \Delta \mathit{sigmoid}(X \cdot W_1) \cdot X \tag{25b}<br>$$</p><p>其中，$\frac{\partial Y}{\partial{W_2}}$，$\frac{\partial Y}{\partial{W_1}}$ 分别通过公式（22）和（21）求得。接下来，我们基于公式对反向传播过程进行代码实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 公式 24</span></span><br><span class="line">d_W2 = np.dot(hidden_layer.T, (<span class="number">2</span> * (output_layer - y) * </span><br><span class="line">              sigmoid_derivative(np.dot(hidden_layer, W2))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 公式 25</span></span><br><span class="line">d_W1 = np.dot(input_layer.T,  (</span><br><span class="line">       np.dot(<span class="number">2</span> * (output_layer - y) * sigmoid_derivative(</span><br><span class="line">       np.dot(hidden_layer, W2)), W2.T) * sigmoid_derivative(np.dot(input_layer, W1))))</span><br><span class="line"></span><br><span class="line">d_W2, d_W1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array([[-0.06363904],</span><br><span class="line">        [-0.05496356],</span><br><span class="line">        [-0.06086952]]), array([[-0.01077667, -0.01419321, -0.00405499],</span><br><span class="line">        [-0.01077667, -0.01419321, -0.00405499]])) </span><br></pre></td></tr></table></figure><p>现在，就可以设置学习率，并对 $W_1$, $W_2$ 进行一次更新了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降更新权重, 学习率为 0.05</span></span><br><span class="line"></span><br><span class="line">W1 -= <span class="number">0.05</span> * d_W1 <span class="comment"># 如果上面是 y - output_layer，则改成 +=</span></span><br><span class="line">W2 -= <span class="number">0.05</span> * d_W2</span><br><span class="line"></span><br><span class="line">d_W2, d_W1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array([[-0.06363904],</span><br><span class="line">        [-0.05496356],</span><br><span class="line">        [-0.06086952]]), array([[-0.01077667, -0.01419321, -0.00405499],</span><br><span class="line">        [-0.01077667, -0.01419321, -0.00405499]])) </span><br></pre></td></tr></table></figure><p>以上，我们就实现了单个样本在神经网络中的 1 次前向 → 反向传递，并使用梯度下降完成 1 次权重更新。那么，下面我们完整实现该网络，并对多样本数据集进行学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例神经网络完整实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X, y, lr</span>):</span></span><br><span class="line">        self.input_layer = X</span><br><span class="line">        self.W1 = np.random.rand(self.input_layer.shape[<span class="number">1</span>], <span class="number">3</span>)</span><br><span class="line">        self.W2 = np.random.rand(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.y = y</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.output_layer = np.zeros(self.y.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.hidden_layer = sigmoid(np.dot(self.input_layer, self.W1))</span><br><span class="line">        self.output_layer = sigmoid(np.dot(self.hidden_layer, self.W2))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self</span>):</span></span><br><span class="line">        d_W2 = np.dot(self.hidden_layer.T, (<span class="number">2</span> * (self.output_layer - self.y) *</span><br><span class="line">                      sigmoid_derivative(np.dot(self.hidden_layer, self.W2))))</span><br><span class="line">        </span><br><span class="line">        d_W1 = np.dot(self.input_layer.T, (</span><br><span class="line">               np.dot(<span class="number">2</span> * (self.output_layer - self.y) * sigmoid_derivative(</span><br><span class="line">               np.dot(self.hidden_layer, self.W2)), self.W2.T) * sigmoid_derivative(</span><br><span class="line">               np.dot(self.input_layer, self.W1))))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        self.W1 -= self.lr * d_W1</span><br><span class="line">        self.W2 -= self.lr * d_W2</span><br></pre></td></tr></table></figure><p>接下来，我们使用实验一开始的示例数据集测试，首先我们要对数据形状进行调整，以满足需要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="string">&#x27;X0&#x27;</span>,<span class="string">&#x27;X1&#x27;</span>]].values <span class="comment"># 输入值</span></span><br><span class="line">y = df[<span class="string">&#x27;Y&#x27;</span>].values.reshape(<span class="built_in">len</span>(X), <span class="number">-1</span>) <span class="comment"># 真实 y，处理成 [[],...,[]] 形状</span></span><br></pre></td></tr></table></figure><p>接下来，我们将其输入到网络中，并迭代 100 次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">nn = NeuralNetwork(X, y, lr=<span class="number">0.001</span>) <span class="comment"># 定义模型</span></span><br><span class="line">loss_list = [] <span class="comment"># 存放损失数值变化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    nn.forward() <span class="comment"># 前向传播</span></span><br><span class="line">    nn.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    loss = np.<span class="built_in">sum</span>((y - nn.output_layer) ** <span class="number">2</span>) <span class="comment"># 计算平方损失</span></span><br><span class="line">    loss_list.append(loss)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;final loss:&quot;</span>, loss)</span><br><span class="line">plt.plot(loss_list) <span class="comment"># 绘制 loss 曲线变化图</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final loss: 133.4221126605534 </span><br></pre></td></tr></table></figure><center><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Neural%20Network/output_239_2.png"></center><p>可以看到，损失函数逐渐减小并接近收敛，变化曲线比感知机计算会平滑很多。不过，由于我们去掉了截距项，且网络结构太过简单，导致收敛情况并不理想。本实验重点再于搞清楚 BP 的中间过程，准确度和学习难度不可两全。另外，需要注意的是由于权重是随机初始化，多次运行的结果会不同。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;人工神经网络是一种发展时间较早且十分常用的机器学习算法。因其模仿人类神经元工作的特点，在监督学习和非监督学习领域都给予了人工神经网络较高的期望。目前，由传统人工神经网络发展而来的卷积神经网络、循环神经网络已经成为了深度学习的基石。本篇文章中，我们将从人工神经网络的原型感知机出发，介绍机器学习中人工神经网络的特点及应用。  &lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="感知机" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    
    <category term="人工神经网络" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Artificial Neural Network" scheme="http://www.laugh12321.cn/blog/tags/Artificial-Neural-Network/"/>
    
    <category term="Neural Network" scheme="http://www.laugh12321.cn/blog/tags/Neural-Network/"/>
    
    <category term="NN" scheme="http://www.laugh12321.cn/blog/tags/NN/"/>
    
  </entry>
  
  <entry>
    <title>阿里云服务器ECS Ubuntu16.04 + Seafile 搭建私人网盘 （Seafile Pro）</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/24/seafile_pro_settings/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/24/seafile_pro_settings/</id>
    <published>2019-01-24T00:00:00.000Z</published>
    <updated>2020-10-23T08:30:57.674Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要讲述 使用 Ubuntu 16.04 云服务器 通过脚本实现对 Seafile Pro 的安装，完成私人网盘的搭建</p><a id="more"></a><p>首先给出 Seafile 专业版的下载地址（Linux）: 👉 <a href="https://download.seafile.com/d/6e5297246c/?p=/pro&mode=list">传送门</a></p><p>在本机下载好安装包后，通过 <a href="https://winscp.net/eng/download.php">WinSCP</a> 将安装包放在 <code>/opt/ </code> 目录下，并将专业版的安装包重命名为 <code>seafile-pro-server_6.3.11_x86-64.tar.gz</code> 的格式（方便安装）。这里使用的安装方式是使用官方给出的 <a href="https://github.com/haiwen/seafile-server-installer-cn">Seafile 安装脚本</a> 安装，优点是一步到位，坏处是安装失败需要还原到镜像。</p><h3 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h3><p>安装干净的 16.04 或 CentOS 7 系统，并<strong>做好镜像</strong> (如果安装失败需要还原到镜像)。</p><p>切换成 root 账号 (sudo -i)</p><h4 id="获取安装脚本"><a href="#获取安装脚本" class="headerlink" title="获取安装脚本"></a>获取安装脚本</h4><p>Ubuntu 16.04（适用于 6.0.0 及以上版本）:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;haiwen&#x2F;seafile-server-installer-cn&#x2F;master&#x2F;seafile-server-ubuntu-16-04-amd64-http</span><br></pre></td></tr></table></figure><h4 id="运行安装脚本并指定要安装的版本-6-3-11"><a href="#运行安装脚本并指定要安装的版本-6-3-11" class="headerlink" title="运行安装脚本并指定要安装的版本 (6.3.11)"></a>运行安装脚本并指定要安装的版本 (6.3.11)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash seafile-server-ubuntu-16-04-amd64-http 6.3.11</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_00.png"></p><p>输入 <code>2</code> 选择安装专业版</p><p>该脚本运行完后会在命令行中打印配置信息和管理员账号密码，请仔细阅读。(你也可以查看安装日志 <code>/opt/seafile/aio_seafile-server.log</code> )，MySQL 密码在 <code>/root/.my.cnf</code> 中。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_01.png"></p><h4 id="通过-Web-UI-对服务器进行配置"><a href="#通过-Web-UI-对服务器进行配置" class="headerlink" title="通过 Web UI 对服务器进行配置"></a>通过 Web UI 对服务器进行配置</h4><p>安装完成后，需要通过 Web UI 服务器进行基本的配置，以便能正常的从网页端进行文件的上传和下载：</p><ol><li><p>首先在浏览器中输入服务器的地址，并用管理员账号和初始密码登录</p></li><li><p>点击界面的右上角的头像按钮进入管理员界面</p><p> <img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_04.png"></p></li><li><p>进入设置页面填写正确的服务器对外的 SERVICE_URL 和 FILE_SERVER_ROOT，比如</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SERVICE_URL: http:&#x2F;&#x2F;126.488.125.111：8000</span><br><span class="line">FILE_SERVER_ROOT: &#39;http:&#x2F;&#x2F;126.488.125.111&#x2F;seafhttp&#39;</span><br></pre></td></tr></table></figure><p><strong><font color='red'>注意：</font></strong> <code>126.488.125.111</code> 是你服务器的公网 <code>ip</code></p></li></ol><p>对了，还要在还要在 <code>云服务器管理控制台</code> 设置新的安全组规则（<code>8082</code> 和 <code>80</code> 端口），可以参考下图自行配置</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_09.png"></p><p>现在可以退出管理员界面，并进行基本的测试。关于服务器的配置选项介绍和日常运维可以参考 <a href="http://manual-cn.seafile.com/config/index.html">http://manual-cn.seafile.com/config/index.html</a></p><h4 id="在本地打开-Web-UI"><a href="#在本地打开-Web-UI" class="headerlink" title="在本地打开 Web UI"></a>在本地打开 Web UI</h4><p>因为使用一键安装脚本安装，默认使用了 <code>nginx</code> 做反向代理，并且开启了防火墙，所以你需要直接通过 <code>80</code> 端口访问，而不是 <code>8000</code> 端口。</p><p><strong><font color='red'>注意：</font></strong>在本地输入的 <code>ip</code> 地址是你的云服务器的公网 <code>IP</code></p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_05.png"></p><h4 id="通过客户端登陆"><a href="#通过客户端登陆" class="headerlink" title="通过客户端登陆"></a>通过客户端登陆</h4><h5 id="Windows-客户端登陆"><a href="#Windows-客户端登陆" class="headerlink" title="Windows 客户端登陆"></a>Windows 客户端登陆</h5><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_06.png"></p><h5 id="Android-客户端登陆"><a href="#Android-客户端登陆" class="headerlink" title="Android 客户端登陆"></a>Android 客户端登陆</h5><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Seafile/Seafile_08.jpg"></p><hr><p>至此，私人云盘已经搭建完毕</p><p>更多详细步骤请阅读： 👉 <a href="https://github.com/haiwen/seafile-server-installer-cn">官方脚本说明</a></p><article class="message message-immersive is-primary"><div class="message-body"><i class="fas fa-globe-americas mr-2"></i>更多详细步骤请阅读：<a href="https://github.com/haiwen/seafile-server-installer-cn">官方脚本说明</a>.</div></article><article class="message message-immersive is-warning"><div class="message-body"><i class="fas fa-question-circle mr-2"></i>文章内容有误？请点击<a href="https://github.com/ppoffice/hexo-theme-icarus/edit/site/source/_posts/zh-CN/Configuring-Theme.md">此处</a>提交修改。</div></article><p><a style="background-color:black;color:white;text-decoration:none;padding:4px 6px;font-size:12px;line-height:1.2;display:inline-block;border-radius:3px" href="https://www.vecteezy.com/free-vector/vector-landscape" target="_blank" rel="noopener noreferrer" title="Vector Landscape Vectors by Vecteezy"><span style="display:inline-block;padding:2px 3px"><svg xmlns="http://www.w3.org/2000/svg" style="height:12px;width:auto;position:relative;vertical-align:middle;top:-1px;fill:white" viewBox="0 0 32 32"><path d="M20.8 18.1c0 2.7-2.2 4.8-4.8 4.8s-4.8-2.1-4.8-4.8c0-2.7 2.2-4.8 4.8-4.8 2.7.1 4.8 2.2 4.8 4.8zm11.2-7.4v14.9c0 2.3-1.9 4.3-4.3 4.3h-23.4c-2.4 0-4.3-1.9-4.3-4.3v-15c0-2.3 1.9-4.3 4.3-4.3h3.7l.8-2.3c.4-1.1 1.7-2 2.9-2h8.6c1.2 0 2.5.9 2.9 2l.8 2.4h3.7c2.4 0 4.3 1.9 4.3 4.3zm-8.6 7.5c0-4.1-3.3-7.5-7.5-7.5-4.1 0-7.5 3.4-7.5 7.5s3.3 7.5 7.5 7.5c4.2-.1 7.5-3.4 7.5-7.5z"></path></svg></span><span style="display:inline-block;padding:2px 3px">Vector Landscape Vectors by Vecteezy</span></a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要讲述 使用 Ubuntu 16.04 云服务器 通过脚本实现对 Seafile Pro 的安装，完成私人网盘的搭建&lt;/p&gt;</summary>
    
    
    
    <category term="Ubuntu" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/"/>
    
    <category term="环境配置" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="环境配置" scheme="http://www.laugh12321.cn/blog/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    <category term="Ubuntu" scheme="http://www.laugh12321.cn/blog/tags/Ubuntu/"/>
    
    <category term="ECS" scheme="http://www.laugh12321.cn/blog/tags/ECS/"/>
    
    <category term="Seafile" scheme="http://www.laugh12321.cn/blog/tags/Seafile/"/>
    
    <category term="服务器" scheme="http://www.laugh12321.cn/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>阿里云服务器ECS Ubuntu16.04 初次使用配置教程(图形界面安装)</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/23/ubuntu16.04_setting/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/23/ubuntu16.04_setting/</id>
    <published>2019-01-23T00:00:00.000Z</published>
    <updated>2020-10-23T08:30:08.435Z</updated>
    
    <content type="html"><![CDATA[<p>前一阵子购买了阿里云的云服务器ECS（学生优惠），折腾了一阵子后对有些东西不太满意，所以就重新初始化了磁盘，刚好要重新安装图形界面，于是就顺手写了这么一篇文章。</p><a id="more"></a><h3 id="首次登陆"><a href="#首次登陆" class="headerlink" title="首次登陆"></a>首次登陆</h3><p>第一次登陆服务器时，是这个样子的：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_00.png"></p><h3 id="输入账号"><a href="#输入账号" class="headerlink" title="输入账号"></a>输入账号</h3><p>在  <code>login:</code> 后输入 <code>root</code> , 会出现 <code>Password：</code>， 然后输入你的实例密码<br><strong><font color="red">注意：</font></strong>你输入的密码是不会显示出来的<br>输入成功后效果如下：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_01.png"></p><h3 id="输入指令"><a href="#输入指令" class="headerlink" title="输入指令"></a>输入指令</h3><p>然后依次输入下面的命令（期间需要手动确认三次）：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 更新软件库</span><br><span class="line"><span class="selector-tag">apt-get</span> <span class="selector-tag">update</span></span><br><span class="line"></span><br><span class="line"># 升级软件</span><br><span class="line"><span class="selector-tag">apt-get</span> <span class="selector-tag">upgrade</span></span><br><span class="line"></span><br><span class="line"># 安装桌面系统</span><br><span class="line"><span class="selector-tag">apt-get</span> <span class="selector-tag">install</span> <span class="selector-tag">ubuntu-desktop</span></span><br></pre></td></tr></table></figure><p>输入<code>apt-get update</code> 后，效果如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_02.png"></p><p>然后输入 <code>apt-get upgrade</code> ，期间需要输入 <code>y</code> 进行确认，如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_03.png"></p><p>然后进行第二次确认，选择默认选项，如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_04.png"></p><p>软件升级完成后如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_05.png"></p><p>接下来就可以安装图形界面了，我们输入 <code>apt-get install ubuntu-desktop</code> 指令，输入后还要进行最后一次手动确认如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_06.png"></p><p>输入 <code>y</code> 即可，等到图形界面安装完成输入 <code>reboot</code> 指令进行重启，如图：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_07.png"></p><p>重启后可以发现我们是以访客身份登陆的，而且不能选择登陆用户并且不需要密码就可以登陆，登陆后还会出现警告信息。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_08.png"></p><p>桌面警告：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_09.png"></p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>打开 <code>usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf</code> 文件并修改</p><p>修改前：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_10.png"></p><p>修改后：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_11.png"></p><p>代码如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[Seat:*]</span></span><br><span class="line">user-session=ubuntu</span><br><span class="line">greeter-show-manual-login=true</span><br><span class="line">allow-guest=false</span><br></pre></td></tr></table></figure><p>重启服务器后可以用 <code>root</code> 用户登录，但是登录还是有警告，这个需要修改 <code>/root/.profile</code> 文件</p><p>修改前：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_12.png"></p><p>修改后：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_13.png"></p><p>代码如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># ~/.profile: executed by Bourne-compatible login shells.</span><br><span class="line"><span class="selector-tag">if</span> <span class="selector-attr">[ <span class="string">&quot;$BASH&quot;</span> ]</span>; <span class="selector-tag">then</span></span><br><span class="line">  <span class="selector-tag">if</span> <span class="selector-attr">[ -f ~/.bashrc ]</span>; <span class="selector-tag">then</span></span><br><span class="line">    . ~/.bashrc</span><br><span class="line">  <span class="selector-tag">fi</span></span><br><span class="line"><span class="selector-tag">fi</span></span><br><span class="line">tty -s &amp;&amp; mesg n || true</span><br></pre></td></tr></table></figure><p>重启后只有root用户，登录后没有警告信息。 </p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Ubuntu_16.04/Ubuntu_14.png"></p><p>至此，<strong>服务器端安装桌面环境结束</strong>。</p><hr><p>参考博客：<a href="https://blog.csdn.net/qq_37608398/article/details/78155568">阿里云服务器ECS Ubuntu16.04-64-bit学习之一：配置桌面</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前一阵子购买了阿里云的云服务器ECS（学生优惠），折腾了一阵子后对有些东西不太满意，所以就重新初始化了磁盘，刚好要重新安装图形界面，于是就顺手写了这么一篇文章。&lt;/p&gt;</summary>
    
    
    
    <category term="Ubuntu" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/"/>
    
    <category term="环境配置" scheme="http://www.laugh12321.cn/blog/categories/Ubuntu/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="环境配置" scheme="http://www.laugh12321.cn/blog/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    <category term="Ubuntu" scheme="http://www.laugh12321.cn/blog/tags/Ubuntu/"/>
    
    <category term="ECS" scheme="http://www.laugh12321.cn/blog/tags/ECS/"/>
    
    <category term="服务器" scheme="http://www.laugh12321.cn/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>机器学习| 支持向量机详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/20/support_vector_machine/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/20/support_vector_machine/</id>
    <published>2019-01-20T00:00:00.000Z</published>
    <updated>2020-10-23T08:55:39.922Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。</p><a id="more"></a><h2 id="线性分类支持向量机"><a href="#线性分类支持向量机" class="headerlink" title="线性分类支持向量机"></a>线性分类支持向量机</h2><p>在逻辑回归中，我们尝试通过一条直线针对线性可分数据完成分类。同时，通过最小化对数损失函数来找到最优分割边界，也就是下图中的紫色直线。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/1.png"></p><p>逻辑回归是一种简单高效的线性分类方法。而在这里中，我们将接触到另一种针对线性可分数据进行分类的思路，并把这种方法称之为支持向量机（英语：Support vector machine，简称：SVM）。</p><p>如果你第一次接触支持向量机这个名字，可能会感觉读起来比较拗口。至少我当时初次接触支持向量机时，完全不知道为什么会有这样一个怪异的名字。假如你和当时的我一样，那么当你看完下面这段介绍内容后，就应该会对支持向量机这个名词有更深刻的认识了。</p><h3 id="支持向量机分类特点"><a href="#支持向量机分类特点" class="headerlink" title="支持向量机分类特点"></a>支持向量机分类特点</h3><p>假设给定一个训练数据集 $T=\lbrace(x_1,y_1),(x_2,y_2),\cdots ,(x_n,y_n)\rbrace$ 。同时，假定已经找到样本空间中的分割平面，其划分公式可以通过以下线性方程来描述：<br>$$<br>wx+b=0\tag{1}<br>$$<br>使用一条直线对线性可分数据集进行分类的过程中，我们已经知道这样的直线可能有很多条：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/4.png"></p><p>问题来了！<strong>哪一条直线是最优的划分方法呢？</strong></p><p>在逻辑回归中，我们引入了 S 形曲线和对数损失函数进行优化求解。如今，支持向量机给了一种从几何学上更加直观的方法进行求解，如下图所示：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/2.png"></p><p>上图展示了支持向量机分类的过程。图中 $wx-b=0$ 为分割直线，我们通过这条直线将数据点分开。与此同时，分割时会在直线的两边再设立两个互相平行的虚线，这两条虚线与分割直线的距离一致。这里的距离往往也被我们称之为「间隔」，而支持向量机的分割特点在于，要使得<strong>分割直线和虚线之间的间隔最大化</strong>。同时也就是两虚线之间的间隔最大化。</p><p>对于线性可分的正负样本点而言，位于 $wx-b=1$ 虚线外的点就是正样本点，而位于 $wx-b=-1$ 虚线外的点就是负样本点。另外，正好位于两条虚线上方的样本点就被我们称为支持向量，这也就是支持向量机的名字来源。</p><h3 id="支持向量机分类演示"><a href="#支持向量机分类演示" class="headerlink" title="支持向量机分类演示"></a>支持向量机分类演示</h3><p>下面，我们使用 Python 代码来演示支持向量机的分类过程。</p><p>首先，我们介绍一种新的示例数据生成方法。即通过 scikit-learn 提供的 <code>samples_generator()</code> 类完成。通过 <code>samples_generator()</code> 类下面提供的不同方法，可以产生不同分布状态的示例数据。首先要用到 <code>make_blobs</code> 方法，该方法可以生成团状数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> samples_generator</span><br><span class="line"></span><br><span class="line">x, y = samples_generator.make_blobs(n_samples=<span class="number">60</span>, centers=<span class="number">2</span>, random_state=<span class="number">30</span>, cluster_std=<span class="number">0.8</span>) <span class="comment"># 生成示例数据</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>)) <span class="comment"># 绘图</span></span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_00.png"></p><p>接下来，我们在示例数据中绘制任意 3 条分割线把示例数据分开。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 3 条不同的分割线</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">for</span> m, b <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">-8</span>), (<span class="number">0.5</span>, <span class="number">-6.5</span>), (<span class="number">-0.2</span>, <span class="number">-4.25</span>)]:</span><br><span class="line">    y_temp = m * x_temp + b</span><br><span class="line">    plt.plot(x_temp, y_temp, <span class="string">&#x27;-k&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_01.png"></p><p>然后，可以使用 <code>fill_between</code> 方法手动绘制出分类硬间隔。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 3 条不同的分割线</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">for</span> m, b, d <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">-8</span>, <span class="number">0.2</span>), (<span class="number">0.5</span>, <span class="number">-6.5</span>, <span class="number">0.55</span>), (<span class="number">-0.2</span>, <span class="number">-4.25</span>, <span class="number">0.75</span>)]:</span><br><span class="line">    y_temp = m * x_temp + b</span><br><span class="line">    plt.plot(x_temp, y_temp, <span class="string">&#x27;-k&#x27;</span>)</span><br><span class="line">    plt.fill_between(x_temp, y_temp - d, y_temp + d, color=<span class="string">&#x27;#f3e17d&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_02.png"></p><div style="color: #999;font-size: 12px;font-style: italic;"><strong>上图为了呈现出分类间隔的效果，手动指定了参数。</strong></div><p>可以看出，不同的分割线所对应的间隔大小是不一致的，而支持向量机的目标是找到最大的分类硬间隔所对应的分割线。</p><h3 id="硬间隔表示及求解"><a href="#硬间隔表示及求解" class="headerlink" title="硬间隔表示及求解"></a>硬间隔表示及求解</h3><p>我们已经知道支持向量机是根据最大间隔来划分，下面考虑如何求得一个几何间隔最大的分割线。</p><p>对于线性可分数据而言，几何间隔最大的分离超平面是唯一的，这里的间隔也被我们称之为「硬间隔」，而间隔最大化也就称为硬间隔最大化。上图实际上就是硬间隔的典型例子。</p><p>最大间隔分离超平面，我们希望最大化超平面 $(w,b)$ 关于训练数据集的几何间隔 $\gamma$，满足以下约束条件：每个训练样本点到超平面 $(w,b)$ 的几何间隔至少都是 $\gamma$ ，因此可以转化为以下的约束最优化问题： </p><p>$$<br>\max\limits_{w,b}\gamma =\frac{2}{\left |w\right |} \tag{2a}<br>$$</p><p>$$<br>\begin{equation}<br>\textrm s.t. y_i(\frac{w}{\left |w\right |}x_i+\frac{b}{\left |w\right |})\geq \frac{\gamma}{2} \tag{2b}<br>\end{equation}<br>$$</p><p>实际上，$\gamma$ 的取值并不会影响最优化问题的解，同时，我们根据数学对偶性原则，可以得到面向硬间隔的线性可分数据的支持向量机的最优化问题：<br>$$<br>\min\limits_{w,b}\frac{1}{2}\left |w\right |^2 \tag{3a}<br>$$</p><p>$$<br>\begin{equation}<br>\textrm s.t. y_i(wx_i+b)-1\geq 0\tag{3b}<br>\end{equation}<br>$$</p><p>我们通常使用拉格朗日乘子法来求解最优化问题，将原始问题转化为对偶问题，通过解对偶问题得到原始问题的解。对公式（3）使用拉格朗日乘子法可得到其「对偶问题」。具体来说，对每条约束添加拉格朗日乘子 $\alpha_i \geq 0$，则该问题的拉格朗日函数可写为：<br>$$<br>L(w,b,\alpha)=\frac{1}{2}\left | w\right |^2+\sum\limits_{i=1}^{m}\alpha_i(1-y_i(wx_i+b)) \tag{4}<br>$$<br>我们通过将公式（4）分别对 $w$ 和 $b$ 求偏导为 <code>0</code> 并代入原式中，可以将 $w$ 和 $b$ 消去，得到公式（3）的对偶问题：<br>$$<br>\max\limits_{\alpha} \sum\limits_{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i \alpha_j y_i y_j x_i x_j \tag{5a}<br>$$</p><p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_i y_i=0,\tag{5b}<br>$$</p><p>$$<br>\alpha_i \geq 0,i=1,2,\cdots,N  \tag{5c}<br>$$</p><p>解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{6}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{7}<br>$$</p><h3 id="软间隔表示及求解"><a href="#软间隔表示及求解" class="headerlink" title="软间隔表示及求解"></a>软间隔表示及求解</h3><p>上面，我们介绍了线性可分条件下的最大硬间隔的推导求解方法。在很多时候，我们还会遇到下面这种情况。你可以发现，在实心点和空心点中各混入了零星的不同类别的数据点。对于这种情况，数据集就变成了严格意义上的线性不可分。但是，造成这种线性不可分的原因往往是因为包含「噪声」数据，它同样可以被看作是不严格条件下的线性可分。</p><p>当我们使用支持向量机求解这类问题时，就会把最大间隔称之为最大「软间隔」，而软间隔就意味着可以容许零星噪声数据被误分类。</p><p><img width='300px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/3.png"></img></p><p>当出现上图所示的样本点不是严格线性可分的情况时，某些样本点 $(x_i,y_i)$ 就不能满足函数间隔 $\geqslant 1$ 的约束条件，即公式（3b）中的约束条件。为了解决这个问题，可以对每个样本点 $(x_i,y_i)$ 引入一个松弛变量 $\xi_i \geq 0$，使得函数间隔加上松弛变量 $\geqslant 1$，即约束条件转化为：<br>$$<br>y_i(wx_i+b) \geq 1-\xi_i \tag{8}<br>$$<br>同时，对每个松弛变量 $\xi_i$ 支付一个代价 $\xi_i$，目标函数由原来的 $\frac{1}{2}||w||^2$ 变成：<br>$$<br>\frac{1}{2}\left | w \right |^2+C\sum\limits_{j=1}^{N}\xi_i \tag{9}<br>$$<br>这里，$C&gt;0$ 称为惩罚参数，一般根据实际情况确定。$C$ 值越大对误分类的惩罚增大，最优化问题即为：<br>$$<br>\min\limits_{w,b,\xi} \frac{1}{2}\left | w \right |^2+C\sum\limits_{i=1}^{N}\xi_i \tag{10a}<br>$$</p><p>$$<br>s.t. y_i(wx_i+b) \geq 1-\xi_i,i=1,2,…,N \tag{10b}<br>$$</p><p>$$<br>\xi_i\geq 0,i=1,2,…,N \tag{10c}<br>$$</p><p>这就是软间隔支持向量机的表示过程。同理，我们可以使用拉格朗日乘子法将其转换为对偶问题求解：<br>$$<br>\max\limits_{\alpha}  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i*x_j)-\sum\limits_{i=1}^{N}\alpha_i \tag{11a}<br>$$</p><p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_iy_i=0 \tag{11b}<br>$$</p><p>$$<br>0 \leq \alpha_i \leq C ,i=1,2,…,N\tag{11c}<br>$$</p><p>解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{12}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{13}<br>$$</p><h3 id="线性支持向量机分类实现"><a href="#线性支持向量机分类实现" class="headerlink" title="线性支持向量机分类实现"></a>线性支持向量机分类实现</h3><p>上面，我们对硬间隔和软间隔支持向量机的求解过程进行了推演，推导过程比较复杂不需要完全掌握，但至少要知道硬间隔和软间隔区别。接下来，我们就使用 Python 对支持向量机找寻最大间隔的过程进行实战。由于支持向量机纯 Python 实现太过复杂，所以本次直接使用 scikit-learn 完成。</p><p>scikit-learn 中的支持向量机分类器对应的类及参数为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto&#x27;</span>, coef0=<span class="number">0.0</span>, shrinking=<span class="literal">True</span>, probability=<span class="literal">False</span>, tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, verbose=<span class="literal">False</span>, max_iter=<span class="number">-1</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>主要的参数如下：</p><ul><li><code>C</code>: 软间隔支持向量机对应的惩罚参数，详见公式（9）.</li><li><code>kernel</code>: 核函数，linear, poly, rbf, sigmoid, precomputed 可选，下文详细介绍。</li><li><code>degree</code>: poly 多项式核函数的指数。</li><li><code>tol</code>: 收敛停止的容许值。</li></ul><p>这里，我们还是使用上面生成的示例数据训练支持向量机模型。由于是线性可分数据，<code>kernel</code> 参数指定为 <code>linear</code> 即可。</p><p>首先，训练支持向量机线性分类模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">linear_svc = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">linear_svc.fit(x, y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,</span><br><span class="line">  decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto_deprecated&#x27;</span>,</span><br><span class="line">  kernel=<span class="string">&#x27;linear&#x27;</span>, max_iter=<span class="number">-1</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">  shrinking=<span class="literal">True</span>, tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>对于训练完成的模型，我们可以通过 <code>support_vectors_</code> 属性输出它对应的支持向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear_svc.support_vectors_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ <span class="number">2.57325754</span>, <span class="number">-3.92687452</span>],</span><br><span class="line">       [ <span class="number">2.49156506</span>, <span class="number">-5.96321164</span>],</span><br><span class="line">       [ <span class="number">4.62473719</span>, <span class="number">-6.02504452</span>]])</span><br></pre></td></tr></table></figure><p>可以看到，一共有 <code>3</code> 个支持向量。如果你输出 <code>x, y</code> 的坐标值，就能看到这 <code>3</code> 个支持向量所对应的数据。</p><p>接下来，我们可以使用 Matplotlib 绘制出训练完成的支持向量机对于的分割线和间隔。为了方便后文重复使用，这里将绘图操作写入到 <code>svc_plot()</code> 函数中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svc_plot</span>(<span class="params">model</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取到当前 Axes 子图数据，并为绘制分割线做准备</span></span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    x = np.linspace(ax.get_xlim()[<span class="number">0</span>], ax.get_xlim()[<span class="number">1</span>], <span class="number">50</span>)</span><br><span class="line">    y = np.linspace(ax.get_ylim()[<span class="number">0</span>], ax.get_ylim()[<span class="number">1</span>], <span class="number">50</span>)</span><br><span class="line">    Y, X = np.meshgrid(y, x)</span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用轮廓线方法绘制分割线</span></span><br><span class="line">    ax.contour(X, Y, P, colors=<span class="string">&#x27;green&#x27;</span>, levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], linestyles=[<span class="string">&#x27;--&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;--&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 标记出支持向量的位置</span></span><br><span class="line">    ax.scatter(model.support_vectors_[:, <span class="number">0</span>], model.support_vectors_[:, <span class="number">1</span>], c=<span class="string">&#x27;green&#x27;</span>, s=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制最大间隔支持向量图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">svc_plot(linear_svc)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_03.png"></p><p>如上图所示，绿色实线代表最终找到的分割线，绿色虚线之间的间隔也就是最大间隔。同时，绿色实心点即代表 <code>3</code> 个支持向量的位置。</p><p>上面的数据点可以被线性可分，所以得到的也就是硬间隔支持向量机的分类结果。那么，如果我们加入噪声使得数据集变成不完美线性可分，结果会怎么样呢？</p><p>接下来，我们就来还原软间隔支持向量机的分类过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向原数据集中加入噪声点</span></span><br><span class="line">x = np.concatenate((x, np.array([[<span class="number">3</span>, <span class="number">-4</span>], [<span class="number">4</span>, <span class="number">-3.8</span>], [<span class="number">2.5</span>, <span class="number">-6.3</span>], [<span class="number">3.3</span>, <span class="number">-5.8</span>]])))</span><br><span class="line">y = np.concatenate((y, np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_04.png"></p><p>可以看到，此时的红蓝数据团中各混入了两个噪声点。</p><p>训练软间隔支持向量机模型并绘制成分割线和最大间隔：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">linear_svc.fit(x, y) <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">svc_plot(linear_svc)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_05.png"></p><p>由于噪声点的混入，此时支持向量的数量由原来的 <code>3</code> 个变成了 <code>13</code> 个。</p><p>前面的实验中，我们提到了惩罚系数 $C$，下面可以通过更改 $C$ 的取值来观察支持向量的变化过程。与此同时，我们要引入一个可以在 Notebook 中实现交互操作的模块。你可以通过选择不同的 $C$ 查看最终绘图的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact</span><br><span class="line"><span class="keyword">import</span> ipywidgets <span class="keyword">as</span> widgets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_c</span>(<span class="params">c</span>):</span></span><br><span class="line">    linear_svc.C = c</span><br><span class="line">    linear_svc.fit(x, y)</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">    svc_plot(linear_svc)</span><br><span class="line">    </span><br><span class="line">interact(change_c, c=[<span class="number">1</span>, <span class="number">10000</span>, <span class="number">1000000</span>])</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_06.png"></p><h2 id="非线性分类支持向量机"><a href="#非线性分类支持向量机" class="headerlink" title="非线性分类支持向量机"></a>非线性分类支持向量机</h2><p>上面的内容中，我们假设样本是线性可分或不严格线性可分，然后通过支持向量机建立最大硬间隔或软间隔实现样本分类。然而，线性可分的样本往往只是理想情况，现实中的原始样本大多数情况下是线性不可分。此时，还能用支持向量机吗？</p><p>其实，对于线性不可分的数据集，我们也可以通过支持向量机去完成分类。但是，这里需要增加一个技巧把线性不可分数据转换为线性可分数据之后，再完成分类。</p><p>与此同时，<strong>我们把这种数据转换的技巧称作「核技巧」，实现数据转换的函数称之为「核函数」</strong>。</p><h3 id="核技巧与核函数"><a href="#核技巧与核函数" class="headerlink" title="核技巧与核函数"></a>核技巧与核函数</h3><p>根据上面的介绍，我们提到一个思路就是核技巧，即先把线性不可分数据转换为线性可分数据，然后再使用支持向量机去完成分类。那么，具体是怎样操作呢？</p><div style="text-align:center;color:blue;"><i>核技巧的关键在于空间映射，即将低维数据映射到高维空间中，使得数据集在高维空间能被线性可分。</i></div><div style="color: #999;font-size: 12px;font-style: italic;">* 核技巧是一种数学方法，仅针对于其在支持向量机中的应用场景进行讲解。</div><p><img width='500px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/5.png"></img></p><p>如上图所示，假设我们在二维空间中有蓝色和红色代表的两类数据点，很明显无法使用一条直线把这两类数据分开。此时，如果我们使用核技巧将其映射到三维空间中，就变成了可以被平面线性可分的状态。</p><p>对于「映射」过程，我们还可以这样理解：分布在二维桌面上的红蓝小球无法被线性分开，此时将手掌拍向桌面（好疼），小球在力的作用下跳跃到三维空间中，这也就是一个直观的映射过程。</p><p>同时，「映射」的过程也就是通过核函数转换的过程。这里需要补充说明一点，那就是将数据点从低维度空间转换到高维度空间的方法有很多，但往往涉及到庞大的计算量，而数学家们从中发现了几种特殊的函数，这类函数能大大降低计算的复杂度，于是被命名为「核函数」。也就是说，核技巧是一种特殊的「映射」技巧，而核函数是核技巧的实现方法。</p><p>下面，我们就认识几种常见的核函数：</p><h4 id="线性核函数"><a href="#线性核函数" class="headerlink" title="线性核函数"></a>线性核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=x_i*x_j \tag{14}<br>$$</p><h4 id="多项式核函数"><a href="#多项式核函数" class="headerlink" title="多项式核函数"></a>多项式核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=\left ( x_i*x_j \right )^d, d \geq 1 \tag{15}<br>$$</p><h4 id="高斯径向基核函数"><a href="#高斯径向基核函数" class="headerlink" title="高斯径向基核函数"></a>高斯径向基核函数</h4><p>$$<br>k\left ( x_i, x_j \right ) = \exp \left(-{\frac  {\left |{\mathbf  {x_i}}-{\mathbf  {x_j}}\right |<em>{2}^{2}}{2\sigma ^{2}}}\right)=exp\left ( -\gamma * \left | x_i-x_j \right |</em>{2} ^2 \right ), \gamma&gt;0 \tag{16}<br>$$</p><h4 id="Sigmoid-核函数"><a href="#Sigmoid-核函数" class="headerlink" title="Sigmoid 核函数"></a>Sigmoid 核函数</h4><p>$$<br>k\left ( x_i, x_j \right )=tanh\left ( \beta * x_ix_j+\theta \right ), \beta &gt; 0 , \theta &lt; 0 \tag{17}<br>$$</p><p>这 <code>4</code> 个核函数也就分别对应着上文介绍 <code>sklearn</code> 中 <code>SVC</code> 方法中 <code>kernel</code> 参数的 <code>linear, poly, rbf, sigmoid</code> 等 <code>4</code> 种不同取值。</p><p>此外，核函数还可以通过函数组合得到，例如：</p><p>若 $k_1$ 和 $k_2$ 是核函数，那么对于任意正数 $\lambda_1,\lambda_2$，其线性组合：<br>$$<br>\lambda_1 k_1+\lambda_2 k_2 \tag{18}<br>$$</p><h3 id="引入核函数的间隔表示及求解"><a href="#引入核函数的间隔表示及求解" class="headerlink" title="引入核函数的间隔表示及求解"></a>引入核函数的间隔表示及求解</h3><p>我们通过直接引入核函数 $k(x_i,x_j)$，而不需要显式的定义高维特征空间和映射函数，就可以利用解线性分类问题的方法来求解非线性分类问题的支持向量机。引入核函数以后，对偶问题就变为：<br>$$<br>\max\limits_{\alpha}  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_jy_iy_jk(x_i*x_j)-\sum\limits_{i=1}^{N}\alpha_i \tag{19a}<br>$$</p><p>$$<br>s.t. \sum\limits_{i=1}^{N}\alpha_iy_i=0 \tag{19b}<br>$$</p><p>$$<br>0 \leq \alpha_i \leq C ,i=1,2,…,N \tag{19c}<br>$$</p><p>同样，解出最优解 $\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)$ 后，基于此我们可以求得最优解 $w^*$, $b^*$，由此得到分离超平面：<br>$$<br>w^<em>x+b^</em>=0  \tag{20}<br>$$<br>使用符号函数求得正负类之间的分类决策函数为：<br>$$<br>f(x)=sign(w^<em>x+b^</em>) \tag{21}<br>$$</p><h3 id="非线性支持向量机分类实现"><a href="#非线性支持向量机分类实现" class="headerlink" title="非线性支持向量机分类实现"></a>非线性支持向量机分类实现</h3><p>同样，我们使用 scikit-learn 中提供的 SVC 类来构建非线性支持向量机模型，并绘制决策边界。</p><p>首先，实验需要生成一组示例数据。上面我们使用了 <code>make_blobs</code> 生成一组线性可分数据，这里使用 <code>make_circles</code> 生成一组线性不可分数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x2, y2 = samples_generator.make_circles(<span class="number">150</span>, factor=<span class="number">.5</span>, noise=<span class="number">.1</span>, random_state=<span class="number">30</span>) <span class="comment"># 生成示例数据</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>)) <span class="comment"># 绘图</span></span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_07.png"></p><p>上图明显是一组线性不可分数据，当我们训练支持向量机模型时就需要引入核技巧。例如，我们这里使用下式做一个简单的非线性映射：<br>$$<br>k\left ( x_i, x_j \right )=x_i^2 + x_j^2 \tag{22}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_function</span>(<span class="params">xi, xj</span>):</span></span><br><span class="line">    poly = xi**<span class="number">2</span> + xj**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> poly</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact, fixed</span><br><span class="line"></span><br><span class="line">r = kernel_function(x2[:,<span class="number">0</span>], x2[:,<span class="number">1</span>])</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">ax = plt.subplot(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.scatter3D(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], r, c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;r&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_08.png"></p><p>上面展示了二维空间点映射到效果维空间的效果。接下来，我们使用 sklearn 中 SVC 方法提供的 RBF 高斯径向基核函数完成实验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbf_svc = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>)</span><br><span class="line">rbf_svc.fit(x2, y2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,</span><br><span class="line">  decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto_deprecated&#x27;</span>,</span><br><span class="line">  kernel=<span class="string">&#x27;rbf&#x27;</span>, max_iter=<span class="number">-1</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">  shrinking=<span class="literal">True</span>, tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line"></span><br><span class="line">svc_plot(rbf_svc)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_09.png"></p><p>同样，我们可以挑战不同的惩罚系数 $C$，看一看决策边界和支持向量的变化情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_c</span>(<span class="params">c</span>):</span></span><br><span class="line">    rbf_svc.C = c</span><br><span class="line">    rbf_svc.fit(x2, y2)</span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br><span class="line">    svc_plot(rbf_svc)</span><br><span class="line">    </span><br><span class="line">interact(change_c, c=[<span class="number">1</span>, <span class="number">100</span>, <span class="number">10000</span>])</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Support%20vector%20machine/output_10.png"></p><h2 id="多分类支持向量机"><a href="#多分类支持向量机" class="headerlink" title="多分类支持向量机"></a>多分类支持向量机</h2><p>支持向量机最初是为二分类问题设计的，当我们面对多分类问题时，其实同样可以使用支持向量机解决。而解决的方法就是通过组合多个二分类器来实现多分类器的构造。根据构造的方式又分为 2 种方法：</p><ul><li><strong>一对多法</strong>：即训练时依次把某个类别的样本归为一类，剩余的样本归为另一类，这样 $k$ 个类别的样本就构造出了 $k$ 个支持向量机。</li><li><strong>一对一法</strong>：即在任意两类样本之间构造一个支持向量机，因此 $k$ 个类别的样本就需要设计 $k(k-1) \div 2$ 个支持向量机。</li></ul><p>而在 scikit-learn，实现多分类支持向量机通过设定参数 <code>decision_function_shape</code> 来确定，其中：</p><ul><li><code>decision_function_shape=&#39;ovo&#39;</code>：代表一对一法。</li><li><code>decision_function_shape=&#39;ovr&#39;</code>：代表一对多法。</li></ul><p>由于这里只需要修改参数，所以就不再赘述了。</p><hr><p><strong>拓展阅读：</strong></p><ul><li><p><a href="https://zh.wikipedia.org/zh-hans/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">支持向量机 - 维基百科</a></p></li><li><p>[知乎上关于支持向量机的问题讨论](<a href="https://www.zhihu.com/question/21094489">https://www.zhihu.com/question/21094489</a></p></li><li><p><a href="https://cuijiahua.com/blog/2017/11/ml_8_svm_1.html">机器学习实战教程（八）：支持向量机原理篇之手撕线性SVM</a></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="支持向量机" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="SVM" scheme="http://www.laugh12321.cn/blog/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>机器学习| 朴素贝叶斯详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/19/naive_bayes_basic/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/19/naive_bayes_basic/</id>
    <published>2019-01-19T00:00:00.000Z</published>
    <updated>2020-10-23T08:27:45.718Z</updated>
    
    <content type="html"><![CDATA[<p>在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。</p><a id="more"></a><h2 id="朴素贝叶斯基础"><a href="#朴素贝叶斯基础" class="headerlink" title="朴素贝叶斯基础"></a>朴素贝叶斯基础</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>朴素贝叶斯的数学理论基础源于概率论。所以，在学习朴素贝叶斯算法之前，首先对其中涉及到的概率论知识做简要讲解。</p><h4 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h4><p>条件概率就是指事件 $A$ 在另外一个事件 $B$ 已经发生条件下的概率。如图所示 ：</p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/1.png" width="370" height="370"><p>其中： </p><ul><li>$P(A)$ 表示 $A$ 事件发生的概率。</li><li>$P(B)$ 表示 $B$ 事件发生的概率。</li><li>$P(AB)$ 表示 $A, B$ 事件同时发生的概率。 </li></ul><p>而最终计算得到的 $P(A \mid B)$ 便是条件概率，表示在 $B$ 事件发生的情况下 $A$ 事件发生的概率。</p><h4 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h4><p>上面提到了条件概率的基本概念，那么当知道事件 $B$ 发生的情况下事件 $A$ 发生的概率 $P(A \mid B)$，如何求 $P(B \mid A)$ 呢？贝叶斯定理应运而生。根据条件概率公式可以得到:</p><p>$$<br>P(B \mid A)=\frac{P(AB)}{P(A)} \tag1<br>$$<br>而同样通过条件概率公式可以得到：</p><p>$$<br>P(AB)=P(A \mid B)*P(B) \tag2<br>$$</p><p>将 (2) 式带入 (1) 式便可得到完整的贝叶斯定理：</p><p>$$<br>P(B \mid A)=\frac{P(AB)}{P(A)}=\frac{P(A \mid B)*P(B)}{P(A)} \tag{3}<br>$$<br>以下，通过一张图来完整且形象的展示条件概率和贝叶斯定理的原理。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/2.png"></p><h4 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a>先验概率</h4><p>先验概率（Prior Probability）指的是根据以往经验和分析得到的概率。例如以上公式中的 $P(A), P(B)$,又例如：$X$ 表示投一枚质地均匀的硬币，正面朝上的概率，显然在我们根据以往的经验下，我们会认为 $X$ 的概率 $P(X) = 0.5$ 。其中 $P(X) = 0.5$ 就是先验概率。</p><h4 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h4><p>后验概率（Posterior Probability）是事件发生后求的反向条件概率；即基于先验概率通过贝叶斯公式求得的反向条件概率。例如公式中的 $P(B \mid A)$   就是通过先验概率 $P(A)$和$P(B)$ 得到的后验概率，其通俗的讲就是「执果寻因」中的「因」。</p><h3 id="什么是朴素贝叶斯"><a href="#什么是朴素贝叶斯" class="headerlink" title="什么是朴素贝叶斯"></a>什么是朴素贝叶斯</h3><p>朴素贝叶斯（Naive Bayes）就是将贝叶斯原理以及条件独立结合而成的算法，其思想非常的简单，根据贝叶斯公式：<br>$$<br>P(B \mid A)=\frac{P(A \mid B)*P(B)}{P(A)} \tag{4}<br>$$<br>变形表达式为：<br>$$<br>P(类别 \mid 特征)=\frac{P(特征 \mid 类别) * P(类别)}{P(特征)} \tag{5}<br>$$<br>公式（5）利用先验概率，即特征和类别的概率；再利用不同类别中各个特征的概率分布，最后计算得到后验概率，即各个特征分布下的预测不同的类别。</p><p>利用贝叶斯原理求解固然是一个很好的方法，但实际生活中数据的特征之间是有相互联系的，在计算 $P(特征\mid类别)$ 时，考虑特征之间的联系会比较麻烦，而朴素贝叶斯则人为的将各个特征割裂开，认定特征之间相互独立。</p><p>朴素贝叶斯中的「朴素」，即条件独立，表示其假设预测的各个属性都是相互独立的,每个属性独立地对分类结果产生影响，条件独立在数学上的表示为：$P(AB)=P(A)*P(B)$。这样，使得朴素贝叶斯算法变得简单，但有时会牺牲一定的分类准确率。对于预测数据，求解在该预测数据的属性出现时各个类别的出现概率，将概率值大的类别作为预测数据的类别。</p><h2 id="朴素贝叶斯算法实现"><a href="#朴素贝叶斯算法实现" class="headerlink" title="朴素贝叶斯算法实现"></a>朴素贝叶斯算法实现</h2><p>前面主要介绍了朴素贝叶斯算法中几个重要的概率论知识，接下来我们对其进行具体的实现，算法流程如下：</p><p><strong>第 1 步</strong>：设<br>$$<br>X = \left { a_{1},a_{2},a_{3},…,a_{n} \right }<br>$$<br>为预测数据，其中 $a_{i}$ 是预测数据的特征值。</p><p><strong>第 2 步</strong>：设<br>$$<br>Y = \left {y_{1},y_{2},y_{3},…,y_{m} \right }<br>$$<br>为类别集合。</p><p><strong>第 3 步</strong>：计算 $P(y_{1}\mid x)$, $P(y_{2}\mid x)$, $P(y_{3}\mid x)$, $…$, $P(y_{m}\mid x)$。</p><p><strong>第 4 步</strong>：寻找 $P(y_{1}\mid x)$, $P(y_{2}\mid x)$, $P(y_{3}\mid x)$, $…$, $P(y_{m}\mid x)$ 中最大的概率 $P(y_{k}\mid x)$ ，则 $x$ 属于类别 $y_{k}$。</p><h3 id="生成示例数据"><a href="#生成示例数据" class="headerlink" title="生成示例数据"></a>生成示例数据</h3><p>下面我们利用 python 完成一个朴素贝叶斯算法的分类。首先生成一组示例数据：由 <code>A</code> 和 <code>B</code>两个类别组成，每个类别包含 <code>x</code>,<code>y</code>两个特征值，其中 <code>x</code> 特征包含<code>r,g,b</code>（红，绿，蓝）三个类别，<code>y</code>特征包含<code>s,m,l</code>（小，中，大）三个类别，如同数据 $X = [g,l]$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;生成示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span>():</span></span><br><span class="line">    data = &#123;<span class="string">&quot;x&quot;</span>: [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>],</span><br><span class="line">            <span class="string">&quot;y&quot;</span>: [<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">            <span class="string">&quot;labels&quot;</span>: [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]&#125;</span><br><span class="line">    data = pd.DataFrame(data, columns=[<span class="string">&quot;labels&quot;</span>, <span class="string">&quot;x&quot;</span>, <span class="string">&quot;y&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>在创建好数据后，接下来进行加载数据，并进行预览。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;加载并预览数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">data = create_data()</span><br><span class="line">data</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/output_00.png"></p><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>根据朴素贝叶斯的原理，最终分类的决策因素是比较 $\left { P(类别 1 \mid 特征),P(类别 2 \mid 特征),…,P(类别 m \mid 特征) \right }$ 各个概率的大小，根据贝叶斯公式得知每一个概率计算的分母 $P(特征)$ 都是相同的，只需要比较分子 $P(类别)$ 和 $P(特征 \mid 类别)$ 乘积的大小。</p><p>那么如何得到 $P(类别)$,以及 $P(特征\mid 类别)$呢？在概率论中，可以应用<strong>极大似然估计法</strong>以及<strong>贝叶斯估计法</strong>来估计相应的概率。</p><h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>什么是极大似然？下面通过一个简单的例子让你有一个形象的了解：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/3.png"></p><blockquote><p><strong>前提条件：</strong>假如有两个外形完全相同箱子，甲箱中有 <code>99</code> 个白球，<code>1</code> 个黑球；乙箱中有 <code>99</code> 个黑球，<code>1</code> 个白球。</p></blockquote><blockquote><p><strong>问题：</strong>当我们进行一次实验，并取出一个球，取出的结果是白球。那么，请问白球是从哪一个箱子里取出的？</p></blockquote><p>我相信，你的第一印象很可能会是白球从甲箱中取出。因为甲箱中的白球数量多，所以这个推断符合人们经验。其中「最可能」就是「极大似然」。而极大似然估计的目的就是利用已知样本结果，反推最有可能造成这个结果的参数值。</p><p>极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：「模型已定，参数未知」。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p><p>在概率论中求解极大似然估计的方法比较复杂，基于实验，我们将讲解 $P(B)$ 和 $P(B/A)$ 是如何通过极大似然估计得到的。$P(种类)$ 用数学的方法表示 ：<br>$$<br>P(y_{i}=c_{k})=\frac{\sum_{N}^{i=1}I(y_{i}=c_{k})}{N},k=1,2,3,…,m \tag{6}<br>$$<br>公式(6)中的 $y_{i}$ 表示数据的类别，$c_{k}$ 表示每一条数据的类别。</p><p>你可以通俗的理解为，在现有的训练集中，每一个类别所占总数的比例，例如:<strong>生成的数据</strong>中 $P(Y=A)=\frac{8}{15}$，表示训练集中总共有 15 条数据，而类别为 <code>A</code> 的有 8 条数据。  </p><p>下面我们用 Python 代码来实现先验概率 $P(种类)$ 的求解：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;P(种类) 先验概率计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_P_labels</span>(<span class="params">labels</span>):</span></span><br><span class="line">    labels = <span class="built_in">list</span>(labels)  <span class="comment"># 转换为 list 类型</span></span><br><span class="line">    P_label = &#123;&#125;  <span class="comment"># 设置空字典用于存入 label 的概率</span></span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        P_label[label] = labels.count(label) / <span class="built_in">float</span>(<span class="built_in">len</span>(labels))  <span class="comment"># p = count(y) / count(Y)</span></span><br><span class="line">    <span class="keyword">return</span> P_label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">P_labels = get_P_labels(data[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line">P_labels</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">0.5333333333333333</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">0.4666666666666667</span>&#125;</span><br></pre></td></tr></table></figure><p>$P(特征 \mid 种类)$ 由于公式较为繁琐这里先不给出，直接用叙述的方式能更清晰地帮助理解：</p><p>实际需要求的先验估计是特征的每一个类别对应的每一个种类的概率，例如：<strong>生成数据</strong> 中 $P(x_{1}=”r” \mid Y=A)=\frac{4}{8}$， <code>A</code> 的数据有 8 条，而在种类为 <code>A</code> 的数据且特征 <code>x</code> 为 <code>r</code>的有 4 条。</p><p>同样我们用代码将先验概率 $P(特征 \mid 种类)$ 实现求解：</p><p>首先我们将特征按序号合并生成一个 <code>numpy</code> 类型的数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;导入特征数据并预览</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">train_data = np.array(data.iloc[:, <span class="number">1</span>:])</span><br><span class="line">train_data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;m&#x27;</span>],</span><br><span class="line">       [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;l&#x27;</span>]], dtype=<span class="built_in">object</span>)</span><br></pre></td></tr></table></figure><p>在寻找属于某一类的某一个特征时，我们采用对比索引的方式来完成。<br>开始得到每一个类别的索引：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;类别 A,B 索引</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">labels = data[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line">label_index = []</span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> P_labels.keys():</span><br><span class="line">    temp_index = []</span><br><span class="line">    <span class="comment"># enumerate 函数返回 Series 类型数的索引和值，其中 i 为索引，label 为值</span></span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels):</span><br><span class="line">        <span class="keyword">if</span> (label == y):</span><br><span class="line">            temp_index.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    label_index.append(temp_index)</span><br><span class="line">label_index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]]</span><br></pre></td></tr></table></figure><p>得到 <code>A</code> 和 <code>B</code> 的索引，其中是<code>A</code>类别为前 $8$ 条数据，<code>B</code>类别为后 $7$ 条数据。</p><p>在得到类别的索引之后，接下来就是找到我们需要的特征为 <code>r</code>的索引值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;特征 x 为 r 的索引</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">x_index = [i <span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data[:, <span class="number">0</span>]) <span class="keyword">if</span> feature == <span class="string">&#x27;r&#x27;</span>]  <span class="comment"># 效果等同于求类别索引中 for 循环</span></span><br><span class="line">x_index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">11</span>]</span><br></pre></td></tr></table></figure><p>得到的结果为 $x$ 特征值为 $r$ 的数据索引值。</p><p>最后通过对比类别为 <code>A</code> 的索引值，计算出既符合 <code>x = r</code> 又符合 <code>A</code> 类别的数据在 <code>A</code> 类别中所占比例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_label = <span class="built_in">set</span>(x_index) &amp; <span class="built_in">set</span>(label_index[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">&#x27;既符合 x = r 又是 A 类别的索引值：&#x27;</span>, x_label)</span><br><span class="line">x_label_count = <span class="built_in">len</span>(x_label)</span><br><span class="line">print(<span class="string">&#x27;先验概率 P(r|A):&#x27;</span>, x_label_count / <span class="built_in">float</span>(<span class="built_in">len</span>(label_index[<span class="number">0</span>])))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">既符合 x = r 又是 A 类别的索引值： &#123;<span class="number">0</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>&#125;</span><br><span class="line">先验概率 P(r|A): <span class="number">0.5</span></span><br></pre></td></tr></table></figure><p>为了方便后面函数调用，我们将求 $P(特征\mid 种类)$ 代码整合为一个函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;P(特征∣种类) 先验概率计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_P_fea_lab</span>(<span class="params">P_label, features, data</span>):</span></span><br><span class="line">    P_fea_lab = &#123;&#125;</span><br><span class="line">    train_data = data.iloc[:, <span class="number">1</span>:]</span><br><span class="line">    train_data = np.array(train_data)</span><br><span class="line">    labels = data[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> each_label <span class="keyword">in</span> P_label.keys():</span><br><span class="line">        label_index = [i <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">            labels) <span class="keyword">if</span> label == each_label]  <span class="comment"># labels 中出现 y 值的所有数值的下标索引</span></span><br><span class="line">        <span class="comment"># features[0] 在 trainData[:,0] 中出现的值的所有下标索引</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features)):</span><br><span class="line">            feature_index = [i <span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">                train_data[:, j]) <span class="keyword">if</span> feature == features[j]]</span><br><span class="line">            <span class="comment"># set(x_index)&amp;set(y_index) 列出两个表相同的元素</span></span><br><span class="line">            fea_lab_count = <span class="built_in">len</span>(<span class="built_in">set</span>(feature_index) &amp; <span class="built_in">set</span>(label_index))</span><br><span class="line">            key = <span class="built_in">str</span>(features[j]) + <span class="string">&#x27;|&#x27;</span> + <span class="built_in">str</span>(each_label)</span><br><span class="line">            P_fea_lab[key] = fea_lab_count / <span class="built_in">float</span>(<span class="built_in">len</span>(label_index))</span><br><span class="line">    <span class="keyword">return</span> P_fea_lab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">features = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>]</span><br><span class="line">get_P_fea_lab(P_labels, features, data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;r|A&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line"> <span class="string">&#x27;m|A&#x27;</span>: <span class="number">0.375</span>,</span><br><span class="line"> <span class="string">&#x27;r|B&#x27;</span>: <span class="number">0.14285714285714285</span>,</span><br><span class="line"> <span class="string">&#x27;m|B&#x27;</span>: <span class="number">0.42857142857142855</span>&#125;</span><br></pre></td></tr></table></figure><p>可以得到当特征 <code>x</code> 和 <code>y</code> 的值为 <code>r</code> 和 <code>m</code> 时，在不同类别下的先验概率。</p><h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><p>在做极大似然估计时，若类别中缺少一些特征，则就会出现概率值为 <code>0</code> 的情况。此时，就会影响后验概率的计算结果，使得分类产生偏差。而解决这一问题最好的方法就是采用贝叶斯估计。  </p><p>在计算先验概率 $P(种类)$ 中，贝叶斯估计的数学表达式为：<br>$$<br>P(y_{i}=c_{k})=\frac{\sum_{N}^{i=1}I(y_{i}=c_{k})+\lambda }{N+k\lambda} \tag{8}<br>$$<br>其中 $\lambda \geq 0$ 等价于在随机变量各个取值的频数上赋予一个正数，当 $\lambda=0$ 时就是极大似然估计。在平时常取 $\lambda=1$，这时称为拉普拉斯平滑。例如：<strong>生成数据</strong> 中，$P(Y=A)=\frac{8+1}{15+2*1}=\frac{9}{17}$,取 $\lambda=1$ 此时由于一共有 <code>A</code>，<code>B</code> 两个类别，则 <code>k</code> 取 2。</p><p>同样计算 $P(特征 \mid 种类)$ 时，也是给计算时的分子分母加上拉普拉斯平滑。例如：<strong>生成数据</strong> 中，$P(x_{1}=”r” \mid Y=A)=\frac{4+1}{8+3*1}=\frac{5}{11}$ 同样取 $\lambda=1$ 此时由于 <code>x</code> 中有 <code>r</code>, <code>g</code>, <code>b</code> 三个种类，所以这里 k 取值为 3。</p><h3 id="朴素贝叶斯算法实现-1"><a href="#朴素贝叶斯算法实现-1" class="headerlink" title="朴素贝叶斯算法实现"></a>朴素贝叶斯算法实现</h3><p>通过上面的内容，相信你已经对朴素贝叶斯算法原理有一定印象。接下来，我们对朴素贝叶斯分类过程进行完整实现。其中，参数估计方法则使用极大似然估计。<br><em>注：分类器实现的公式，请参考《机器学习》- 周志华 P151 页</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;朴素贝叶斯分类器</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">data, features</span>):</span></span><br><span class="line">    <span class="comment"># 求 labels 中每个 label 的先验概率</span></span><br><span class="line">    labels = data[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">    P_label = get_P_labels(labels)</span><br><span class="line">    P_fea_lab = get_P_fea_lab(P_label, features, data)</span><br><span class="line"></span><br><span class="line">    P = &#123;&#125;</span><br><span class="line">    P_show = &#123;&#125;  <span class="comment"># 后验概率</span></span><br><span class="line">    <span class="keyword">for</span> each_label <span class="keyword">in</span> P_label:</span><br><span class="line">        P[each_label] = P_label[each_label]</span><br><span class="line">        <span class="keyword">for</span> each_feature <span class="keyword">in</span> features:</span><br><span class="line">            key = <span class="built_in">str</span>(each_label)+<span class="string">&#x27;|&#x27;</span>+<span class="built_in">str</span>(features)</span><br><span class="line">            P_show[key] = P[each_label] * \</span><br><span class="line">                P_fea_lab[<span class="built_in">str</span>(each_feature) + <span class="string">&#x27;|&#x27;</span> + <span class="built_in">str</span>(each_label)]</span><br><span class="line">            P[each_label] = P[each_label] * \</span><br><span class="line">                P_fea_lab[<span class="built_in">str</span>(each_feature) + <span class="string">&#x27;|&#x27;</span> +</span><br><span class="line">                          <span class="built_in">str</span>(each_label)]  <span class="comment"># 由于分母相同，只需要比较分子</span></span><br><span class="line">    print(P_show)</span><br><span class="line">    features_label = <span class="built_in">max</span>(P, key=P.get)  <span class="comment"># 概率最大值对应的类别</span></span><br><span class="line">    <span class="keyword">return</span> features_label</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classify(data, [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;A|[&#x27;r&#x27;, &#x27;m&#x27;]&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;B|[&#x27;r&#x27;, &#x27;m&#x27;]&quot;</span>: <span class="number">0.02857142857142857</span>&#125;</span><br><span class="line"><span class="string">&#x27;A&#x27;</span></span><br></pre></td></tr></table></figure><p>对于特征为 <code>[r,m]</code> 的数据通过朴素贝叶斯分类得到不同类别的概率值，经过比较后分为 <code>A</code> 类。</p><h3 id="朴素贝叶斯的三种常见模型"><a href="#朴素贝叶斯的三种常见模型" class="headerlink" title="朴素贝叶斯的三种常见模型"></a>朴素贝叶斯的三种常见模型</h3><p>了解完朴素贝叶斯算法原理后，在实际数据中，我们可以依照特征的数据类型不同，在计算先验概率方面对朴素贝叶斯模型进行划分，并分为：<strong>多项式模型</strong>，<strong>伯努利模型</strong>和<strong>高斯模型</strong>。</p><h4 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h4><p>当特征值为离散时，常常使用多项式模型。事实上，在以上实验的参数估计中，我们所应用的就是多项式模型。为避免概率值为 0 的情况出现，多项式模型采用的是贝叶斯估计。</p><h4 id="伯努利模型"><a href="#伯努利模型" class="headerlink" title="伯努利模型"></a>伯努利模型</h4><p>与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是 <code>1</code> 和 <code>0</code>（以文本分类为例，某个单词在文档中出现过，则其特征值为 <code>1</code>，否则为 <code>0</code>）。</p><p>在伯努利模型中，条件概率 $P(x_{i} \mid y_{k})$ 的计算方式为：</p><ul><li>当特征值 $x_{i}=1$ 时，$P(x_{i} \mid y_{k})=P(x_{i}=1 \mid y_{k})$;  </li><li>当特征值 $x_{i}=0$ 时，$P(x_{i} \mid y_{k})=P(x_{i}=0 \mid y_{k})$。</li></ul><h4 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h4><p>当特征是连续变量的时候，在不做平滑的情况下，运用多项式模型就会导致很多 $P(x_{i} \mid y_{k})=0$，此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，采用高斯模型。高斯模型是假设连续变量的特征数据是服从高斯分布的，高斯分布函数表达式为：<br>$$<br>P(x_{i}|y_{k})=\frac{1}{\sqrt{2\pi}\sigma_{y_{k},i}}exp(-\frac{(x-\mu_{y_{k},i}) ^{2}}{2\sigma ^{2}<em>{y</em>{k}},i})<br>$$<br>其中：</p><ul><li>$\mu_{y_{k},i}$ 表示类别为 $y_{k}$ 的样本中，第 $i$ 维特征的均值。  </li><li>$\sigma ^{2}<em>{y</em>{k}},i$ 表示类别为 $y_{k}$ 的样本中，第 $i$ 维特征的方差。  </li></ul><p>高斯分布示意图如下：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/Naive%20Bayes/4.png"></p><hr><p>关于贝叶斯定理，这里有一个有趣的视频，希望能加深大家对该定理的理解。</p><center><video width='800px' controls src="http://labfile.oss.aliyuncs.com/courses/1081/beyes_video.mp4" /></center><div style="color: #999;font-size: 12px;text-align: center;">如何用贝叶斯方法帮助内容审核 | 视频来源：[回形针PaperClip](https://weibo.com/u/6414205745?is_all=1)</div>]]></content>
    
    
    <summary type="html">&lt;p&gt;在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="朴素贝叶斯" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Naive Bayes" scheme="http://www.laugh12321.cn/blog/tags/Naive-Bayes/"/>
    
  </entry>
  
  <entry>
    <title>机器学习| K-近邻算法详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/07/k_nearest_neighbors/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/07/k_nearest_neighbors/</id>
    <published>2019-01-07T00:00:00.000Z</published>
    <updated>2020-10-23T08:27:12.649Z</updated>
    
    <content type="html"><![CDATA[<h3 id="最近邻算法"><a href="#最近邻算法" class="headerlink" title="最近邻算法"></a>最近邻算法</h3><p>介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 $x$，在训练集中找到与 $x$ 最相似的训练样本 $y$，用 $y$ 的样本对应的类别作为未知类别数据 $x$ 的类别，从而达到分类的效果。</p><a id="more"></a><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/1.png"></p><p>如上图所示，通过计算数据 $X_{u}$ （未知样本）和已知类别 ${\omega_{1},\omega_{2},\omega_{3}}$ （已知样本）之间的距离，判断 $X_{u}$ 与不同训练集的相似度，最终判断 $X_{u}$ 的类别。显然，这里将<font color="green">绿色未知样本</font>类别判定与<font color="red">红色已知样本</font>类别相同较为合适。</p><h3 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K-近邻算法"></a>K-近邻算法</h3><p>K-近邻（K-Nearest Neighbors，简称：KNN）算法是最近邻（NN）算法的一个推广，也是机器学习分类算法中最简单的方法之一。KNN 算法的核心思想和最近邻算法思想相似，都是通过寻找和未知样本相似的类别进行分类。但 NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/2.png"></p><p>如上图所示，对于未知测试样本(图中<font color='red'> ？</font>所示)采用 KNN 算法进行分类，首先计算未知样本和训练样本之间的相似度，找出最近 K 个相邻样本（在图中 K 值为 3，圈定距离 ？最近的 3 个样本），再根据最近的 K 个样本最终判断未知样本的类别。</p><h2 id="K-近邻算法实现"><a href="#K-近邻算法实现" class="headerlink" title="K-近邻算法实现"></a>K-近邻算法实现</h2><p>KNN 算法在理论上已经非常成熟，其简单、易于理解的思想以及良好的分类准确度使得 KNN 算法应用非常广泛。算法的具体流程主要是以下的 4 个步骤：</p><ol><li><strong>数据准备</strong>：通过数据清洗，数据处理，将每条数据整理成向量。  </li><li><strong>计算距离</strong>：计算测试数据与训练数据之间的距离。  </li><li><strong>寻找邻居</strong>：找到与测试数据距离最近的 K 个训练数据样本。  </li><li><strong>决策分类</strong>：根据决策规则，从 K 个邻居得到测试数据的类别。</li></ol><p><img width='900px' style="border:2px dashed #000000;" src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/3.png"></img></p><h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><p>下面，我们尝试完成一个 KNN 分类流程。首先，生成一组示例数据，共包含 2 个类别（<code>A</code>和<code>B</code>），其中每一条数据包含两个特征（<code>x</code>和<code>y</code>）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;生成示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span>():</span></span><br><span class="line">    features = np.array(</span><br><span class="line">        [[<span class="number">2.88</span>, <span class="number">3.05</span>], [<span class="number">3.1</span>, <span class="number">2.45</span>], [<span class="number">3.05</span>, <span class="number">2.8</span>], [<span class="number">2.9</span>, <span class="number">2.7</span>], [<span class="number">2.75</span>, <span class="number">3.4</span>],</span><br><span class="line">         [<span class="number">3.23</span>, <span class="number">2.9</span>], [<span class="number">3.2</span>, <span class="number">3.75</span>], [<span class="number">3.5</span>, <span class="number">2.9</span>], [<span class="number">3.65</span>, <span class="number">3.6</span>], [<span class="number">3.35</span>, <span class="number">3.3</span>]])</span><br><span class="line">    labels = [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure><p>然后，我们尝试加载并打印这些数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;打印示例数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">features, labels = create_data()</span><br><span class="line">print(<span class="string">&#x27;features: \n&#x27;</span>, features)</span><br><span class="line">print(<span class="string">&#x27;labels: \n&#x27;</span>, labels)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">features: </span><br><span class="line"> [[<span class="number">2.88</span> <span class="number">3.05</span>]</span><br><span class="line"> [<span class="number">3.1</span>  <span class="number">2.45</span>]</span><br><span class="line"> [<span class="number">3.05</span> <span class="number">2.8</span> ]</span><br><span class="line"> [<span class="number">2.9</span>  <span class="number">2.7</span> ]</span><br><span class="line"> [<span class="number">2.75</span> <span class="number">3.4</span> ]</span><br><span class="line"> [<span class="number">3.23</span> <span class="number">2.9</span> ]</span><br><span class="line"> [<span class="number">3.2</span>  <span class="number">2.75</span>]</span><br><span class="line"> [<span class="number">3.5</span>  <span class="number">2.9</span> ]</span><br><span class="line"> [<span class="number">3.65</span> <span class="number">3.6</span> ]</span><br><span class="line"> [<span class="number">3.35</span> <span class="number">3.3</span> ]]</span><br><span class="line">labels: </span><br><span class="line"> [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]</span><br></pre></td></tr></table></figure><p>为了更直观地理解数据，接下来用 Matplotlib 下的 pyplot 包来对数据集进行可视化。为了代码的简洁，我们使用了 <code>map</code> 函数和 <code>lamda</code> 表达式对数据进行处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;示例数据绘图</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">plt.ylim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">x_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], features))  <span class="comment"># 返回每个数据的x特征值</span></span><br><span class="line">y_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], features))</span><br><span class="line">plt.scatter(x_feature[:<span class="number">5</span>], y_feature[:<span class="number">5</span>], c=<span class="string">&quot;b&quot;</span>)  <span class="comment"># 在画布上绘画出&quot;A&quot;类标签的数据点</span></span><br><span class="line">plt.scatter(x_feature[<span class="number">5</span>:], y_feature[<span class="number">5</span>:], c=<span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.scatter([<span class="number">3.18</span>], [<span class="number">3.15</span>], c=<span class="string">&quot;r&quot;</span>, marker=<span class="string">&quot;x&quot;</span>)  <span class="comment"># 待测试点的坐标为 [3.1，3.2]</span></span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/out_00.png"></p><p>由上图所示，标签为 <code>A</code>（蓝色圆点）的数据在画布的左下角位置，而标签为 <code>B</code>（绿色圆点）的数据在画布的右上角位置，通过图像可以清楚看出不同标签数据的分布情况。其中<font color="red">红色 x 点</font>即表示需预测类别的测试数据。</p><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>在计算两个样本间的相似度时，可以通过计算样本之间特征值的距离进行表示。若两个样本距离值越大（相距越远），则表示两个样本相似度低，相反，若两个样本值越小（相距越近），则表示两个样本相似度越高。</p><p>计算距离的方法有很多，本实验介绍两个最为常用的距离公式：<strong>曼哈顿距离</strong>和<strong>欧式距离</strong>。这两个距离的计算图示如下：</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/4.png"></p><h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><p>曼哈顿距离又称马氏距离，出租车距离，是计算距离最简单的方式之一。公式如下：</p><p>$$<br>d_{man}=\sum_{i=1}^{N}\left | X_{i}-Y_{i} \right |<br>$$<br>其中： </p><ul><li>$X$,$Y$：两个数据点</li><li>$N$：每个数据中有 $N$ 个特征值</li><li>$X_{i}$ ：数据 $X$ 的第 $i$ 个特征值  </li></ul><p>公式表示为将两个数据 $X$ 和 $Y$ 中每一个对应特征值之间差值的绝对值，再求和，便得到曼哈顿距离。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;曼哈顿距离计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_man</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    d = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(x - y))</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">3.1</span>, <span class="number">3.2</span>])</span><br><span class="line">print(<span class="string">&quot;x:&quot;</span>, x)</span><br><span class="line">y = np.array([<span class="number">2.5</span>, <span class="number">2.8</span>])</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>, y)</span><br><span class="line">d_man = d_man(x, y)</span><br><span class="line">print(d_man)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x: [<span class="number">3.1</span> <span class="number">3.2</span>]</span><br><span class="line">y: [<span class="number">2.5</span> <span class="number">2.8</span>]</span><br><span class="line"><span class="number">1.0000000000000004</span></span><br></pre></td></tr></table></figure><h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><p>欧式距离源自 $N$ 维欧氏空间中两点之间的距离公式。表达式如下:</p><p>$$<br>d_{euc}= \sqrt{\sum_{i=1}^{N}(X_{i}-Y_{i})^{2}}<br>$$<br>其中：</p><ul><li>$X$, $Y$ ：两个数据点</li><li>$N$：每个数据中有 $N$ 个特征值</li><li>$X_{i}$ ：数据 $X$ 的第 $i$ 个特征值  </li></ul><p>公式表示为将两个数据 X 和 Y 中的每一个对应特征值之间差值的平方，再求和，最后开平方，便是欧式距离。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;欧氏距离的计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_euc</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    d = np.sqrt(np.<span class="built_in">sum</span>(np.square(x - y)))</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.random.random(<span class="number">10</span>)  <span class="comment"># 随机生成10个数的数组作为x特征的值</span></span><br><span class="line">print(<span class="string">&quot;x:&quot;</span>, x)</span><br><span class="line">y = np.random.random(<span class="number">10</span>)</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>, y)</span><br><span class="line">distance_euc = d_euc(x, y)</span><br><span class="line">print(distance_euc)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x: [<span class="number">0.10725148</span> <span class="number">0.78394185</span> <span class="number">0.85568109</span> <span class="number">0.5774587</span>  <span class="number">0.96974919</span> <span class="number">0.79467734</span></span><br><span class="line"> <span class="number">0.26009361</span> <span class="number">0.93204</span>    <span class="number">0.08424034</span> <span class="number">0.16970618</span>]</span><br><span class="line">y: [<span class="number">0.88013554</span> <span class="number">0.5943479</span>  <span class="number">0.31357311</span> <span class="number">0.20830397</span> <span class="number">0.20686205</span> <span class="number">0.9475627</span></span><br><span class="line"> <span class="number">0.61453761</span> <span class="number">0.27882129</span> <span class="number">0.61228018</span> <span class="number">0.75968914</span>]</span><br><span class="line"><span class="number">1.6876178018976438</span></span><br></pre></td></tr></table></figure><h3 id="决策规则"><a href="#决策规则" class="headerlink" title="决策规则"></a>决策规则</h3><p>在得到测试样本和训练样本之间的相似度后，通过相似度的排名，可以得到每一个测试样本的 K 个相邻的训练样本，那如何通过 K 个邻居来判断测试样本的最终类别呢？可以根据数据特征对决策规则进行选取，不同的决策规则会产生不同的预测结果，最常用的决策规则是：  </p><ul><li><strong>多数表决法</strong>：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。  </li><li><strong>加权表决法</strong>：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。</li></ul><p>这里推荐使用多数表决法，这种方法更加简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;多数表决法</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majority_voting</span>(<span class="params">class_count</span>):</span></span><br><span class="line">    sorted_class_count = <span class="built_in">sorted</span>(</span><br><span class="line">        class_count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_class_count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">2</span>, <span class="string">&quot;C&quot;</span>: <span class="number">6</span>, <span class="string">&quot;D&quot;</span>: <span class="number">5</span>&#125;</span><br><span class="line">majority_voting(arr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;C&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;D&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure><p>在多数表决法的定义中，我们导入了 <code>operater</code> 计算模块，目的是对字典类型结构排序。可以从结果中看出函数返回的结果为票数最多的 <code>C</code>，得票为 <code>6</code> 次。</p><h3 id="KNN-算法实现"><a href="#KNN-算法实现" class="headerlink" title="KNN 算法实现"></a>KNN 算法实现</h3><p>在学习完以上的各个步骤之后，KNN 算法也逐渐被勾勒出来。以下就是对 KNN 算法的完整实现，本次的距离计算采用<strong>欧式距离</strong>，分类的决策规则为<strong>多数表决法</strong>，定义函数 <code>knn_classify()</code>，其中函数的参数包括：</p><ul><li><code>test_data</code>：用于分类的输入向量。</li><li><code>train_data</code>：输入的训练样本集。</li><li><code>labels</code>：样本数据的类标签向量。</li><li><code>k</code>：用于选择最近邻居的数目。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;KNN 方法完整实现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn_classify</span>(<span class="params">test_data, train_data, labels, k</span>):</span></span><br><span class="line">    distances = np.array([])  <span class="comment"># 创建一个空的数组用于存放距离</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each_data <span class="keyword">in</span> train_data:  <span class="comment"># 使用欧式距离计算数据相似度</span></span><br><span class="line">        d = d_euc(test_data, each_data)</span><br><span class="line">        distances = np.append(distances, d)</span><br><span class="line"></span><br><span class="line">    sorted_distance_index = distances.argsort()  <span class="comment"># 获取按距离大小排序后的索引</span></span><br><span class="line">    sorted_distance = np.sort(distances)</span><br><span class="line">    r = (sorted_distance[k]+sorted_distance[k<span class="number">-1</span>])/<span class="number">2</span>  <span class="comment"># 计算</span></span><br><span class="line"></span><br><span class="line">    class_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):  <span class="comment"># 多数表决</span></span><br><span class="line">        vote_label = labels[sorted_distance_index[i]]</span><br><span class="line">        class_count[vote_label] = class_count.get(vote_label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    final_label = majority_voting(class_count)</span><br><span class="line">    <span class="keyword">return</span> final_label, r</span><br></pre></td></tr></table></figure><h3 id="分类预测"><a href="#分类预测" class="headerlink" title="分类预测"></a>分类预测</h3><p>在实现 KNN 算法之后，接下来就可以对我们未知数据<code>[3.18,3.15]</code>开始分类,假定我们 K 值初始设定为 5，让我们看看分类的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_data = np.array([<span class="number">3.18</span>, <span class="number">3.15</span>])</span><br><span class="line">final_label, r = knn_classify(test_data, features, labels, <span class="number">5</span>)</span><br><span class="line">final_label</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;B&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure><h3 id="可视化展示"><a href="#可视化展示" class="headerlink" title="可视化展示"></a>可视化展示</h3><p>在对数据 <code>[3.18,3.15]</code> 实现分类之后，接下来我们同样用画图的方式形象化展示 KNN 算法决策方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">circle</span>(<span class="params">r, a, b</span>):</span>  <span class="comment"># 为了画出圆，这里采用极坐标的方式对圆进行表示 ：x=r*cosθ，y=r*sinθ。</span></span><br><span class="line">    theta = np.arange(<span class="number">0</span>, <span class="number">2</span>*np.pi, <span class="number">0.01</span>)</span><br><span class="line">    x = a+r * np.cos(theta)</span><br><span class="line">    y = b+r * np.sin(theta)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k_circle_x, k_circle_y = circle(r, <span class="number">3.18</span>, <span class="number">3.15</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">plt.ylim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">x_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], features))  <span class="comment"># 返回每个数据的x特征值</span></span><br><span class="line">y_feature = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], features))</span><br><span class="line">plt.scatter(x_feature[:<span class="number">5</span>], y_feature[:<span class="number">5</span>], c=<span class="string">&quot;b&quot;</span>)  <span class="comment"># 在画布上绘画出&quot;A&quot;类标签的数据点</span></span><br><span class="line">plt.scatter(x_feature[<span class="number">5</span>:], y_feature[<span class="number">5</span>:], c=<span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.scatter([<span class="number">3.18</span>], [<span class="number">3.15</span>], c=<span class="string">&quot;r&quot;</span>, marker=<span class="string">&quot;x&quot;</span>)  <span class="comment"># 待测试点的坐标为 [3.1，3.2]</span></span><br><span class="line">plt.plot(k_circle_x, k_circle_y)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/K-Nearest%20Neighbors/out_01.png"></p><p>如图所示，当我们 <code>K</code> 值为 <code>5</code> 时，与测试样本距离最近的 <code>5</code> 个训练数据（如蓝色圆圈所示）中属于 <code>B</code> 类的有 <code>3</code> 个，属于 <code>A</code> 类的有 <code>2</code> 个，根据多数表决法决策出测试样本的数据为 <code>B</code> 类。</p><p>通过尝试不同的 K 值我们会发现，不同的 K 值预测出不同的结果。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;最近邻算法&quot;&gt;&lt;a href=&quot;#最近邻算法&quot; class=&quot;headerlink&quot; title=&quot;最近邻算法&quot;&gt;&lt;/a&gt;最近邻算法&lt;/h3&gt;&lt;p&gt;介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 $x$，在训练集中找到与 $x$ 最相似的训练样本 $y$，用 $y$ 的样本对应的类别作为未知类别数据 $x$ 的类别，从而达到分类的效果。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="K-近邻算法" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="K-Nearest Neighbors" scheme="http://www.laugh12321.cn/blog/tags/K-Nearest-Neighbors/"/>
    
    <category term="KNN" scheme="http://www.laugh12321.cn/blog/tags/KNN/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|多项式回归算法详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/04/polynomial_regression/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/04/polynomial_regression/</id>
    <published>2019-01-04T00:00:00.000Z</published>
    <updated>2020-10-23T08:26:13.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多项式回归介绍"><a href="#多项式回归介绍" class="headerlink" title="多项式回归介绍"></a>多项式回归介绍</h2><p>在线性回归中，我们通过建立自变量 <code>x</code> 的一次方程来拟合数据。而非线性回归中，则需要建立因变量和自变量之间的非线性关系。从直观上讲，也就是拟合的直线变成了「曲线」。</p><a id="more"></a><p>如下图所示，是某地区人口数量的变化数据。如果我们使用线性方差去拟合数据，那么就会存在「肉眼可见」的误差。而对于这样的数据，使用一条曲线去拟合则更符合数据的发展趋势。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/1.png" alt="此处输入图片的描述"></p><p>对于非线性回归问题而言，最简单也是最常见的方法就是本次实验要讲解的「多项式回归」。多项式是中学时期就会接触到的概念，这里引用 <a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E9%A0%85%E5%BC%8F">维基百科</a> 的定义如下：</p><blockquote><p>多项式（Polynomial）是代数学中的基础概念，是由称为未知数的变量和称为系数的常量通过有限次加法、加减法、乘法以及自然数幂次的乘方运算得到的代数表达式。多项式是整式的一种。未知数只有一个的多项式称为一元多项式；例如 $x^2-3x+4$ 就是一个一元多项式。未知数不止一个的多项式称为多元多项式，例如 $x^3-2xyz^2+2yz+1$ 就是一个三元多项式。</p></blockquote><h2 id="多项式回归基础"><a href="#多项式回归基础" class="headerlink" title="多项式回归基础"></a>多项式回归基础</h2><p>首先，我们通过一组示例数据来认识多项式回归</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载示例数据</span></span><br><span class="line">x = [<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">25</span>, <span class="number">32</span>, <span class="number">43</span>, <span class="number">58</span>, <span class="number">63</span>, <span class="number">69</span>, <span class="number">79</span>]</span><br><span class="line">y = [<span class="number">20</span>, <span class="number">33</span>, <span class="number">50</span>, <span class="number">56</span>, <span class="number">42</span>, <span class="number">31</span>, <span class="number">33</span>, <span class="number">46</span>, <span class="number">65</span>, <span class="number">75</span>]</span><br></pre></td></tr></table></figure><p>示例数据一共有 10 组，分别对应着横坐标和纵坐标。接下来，通过 Matplotlib 绘制数据，查看其变化趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/2.png"></p><h3 id="实现-2-次多项式拟合"><a href="#实现-2-次多项式拟合" class="headerlink" title="实现 2 次多项式拟合"></a>实现 2 次多项式拟合</h3><p>接下来，通过多项式来拟合上面的散点数据。首先，一个标准的一元高阶多项式函数如下所示：</p><p>$$<br>y(x, w) = w_0 + w_1x + w_2x^2 +…+w_mx^m = \sum\limits_{j=0}^{m}w_jx^j \tag{1}<br>$$</p><p>其中，m 表示多项式的阶数，x^j 表示 x 的 j 次幂，w 则代表该多项式的系数。</p><p>当我们使用上面的多项式去拟合散点时，需要确定两个要素，分别是：多项式系数 $w$ 以及多项式阶数 $m$，这也是多项式的两个基本要素。</p><p>如果通过手动指定多项式阶数 $m$ 的大小，那么就只需要确定多项式系数 $w$ 的值是多少。例如，这里首先指定 $m=2$，多项式就变成了：<br>$$<br>y(x, w) = w_0 + w_1x + w_2x^2= \sum\limits_{j=0}^{2}w_jx^j \tag{2}<br>$$<br>当我们确定 $w$ 的值的大小时，就回到了前面线性回归中学习到的内容。</p><p>首先，我们构造两个函数，分别是用于拟合的多项式函数，以及误差函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;实现 2 次多项式函数及误差函数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">p, x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据公式，定义 2 次多项式函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w0, w1, w2 = p</span><br><span class="line">    f = w0 + w1*x + w2*x*x</span><br><span class="line">    <span class="keyword">return</span> f</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">err_func</span>(<span class="params">p, x, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差函数（观测值与拟合值之间的差距）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ret = func(p, x) - y</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p>接下来，使用 NumPy 提供的随机数方法初始化 3 个 $w$ 参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">p_init = np.random.randn(<span class="number">3</span>) <span class="comment"># 生成 3 个随机数</span></span><br><span class="line"></span><br><span class="line">p_init</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">0.60995017</span>,  <span class="number">1.32614407</span>, <span class="number">-1.22657863</span>])</span><br></pre></td></tr></table></figure><p>接下来，就是使用最小二乘法求解最优参数的过程。这里为了方便，我们直接使用 Scipy 提供的最小二乘法类，得到最佳拟合参数。当然，你完全可以按照线性回归实验中最小二乘法公式自行求解参数。不过，实际工作中为了快速实现，往往会使用像 Scipy 这样现成的函数，这里也是为了给大家多介绍一种方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;使用 Scipy 提供的最小二乘法函数得到最佳拟合参数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> leastsq</span><br><span class="line"></span><br><span class="line">parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Fitting Parameters: &#x27;</span>, parameters[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><blockquote><p>关于 <code>scipy.optimize.leastsq()</code> 的具体使用介绍，可以阅读 <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html">官方文档</a>。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Fitting Parameters:  [ <span class="number">3.76893126e+01</span> <span class="number">-2.60474221e-01</span>  <span class="number">8.00078171e-03</span>]</span><br></pre></td></tr></table></figure><p>我们这里得到的最佳拟合参数 $w_0$, $w_1$, $w_2$ 依次为 <code>3.76893117e+01</code>, <code>-2.60474147e-01</code> 和 <code>8.00078082e-03</code>。也就是说，我们拟合后的函数（保留两位有效数字）为：</p><p>$$<br>y(x) = 37 - 0.26<em>x + 0.0080</em>x^2 \tag{3}<br>$$</p><p>然后，我们尝试绘制出拟合后的图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制 2 次多项式拟合图像</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 绘制拟合图像时需要的临时点</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">80</span>, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制拟合函数曲线</span></span><br><span class="line">plt.plot(x_temp, func(parameters[<span class="number">0</span>], x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制原数据点</span></span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/3.png"></p><h3 id="实现-N-次多项式拟合"><a href="#实现-N-次多项式拟合" class="headerlink" title="实现 N 次多项式拟合"></a>实现 N 次多项式拟合</h3><p>你会发现，上面采用 <code>2</code> 次多项式拟合的结果也不能恰当地反映散点的变化趋势。此时，我们可以尝试 <code>3</code> 次及更高次多项式拟合。接下来的代码中，我们将针对上面 <code>2</code> 次多项式拟合的代码稍作修改，实现一个 <code>N</code> 次多项式拟合的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;实现 n 次多项式拟合</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_func</span>(<span class="params">p, x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据公式，定义 n 次多项式函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    f = np.poly1d(p)</span><br><span class="line">    <span class="keyword">return</span> f(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">err_func</span>(<span class="params">p, x, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差函数（观测值与拟合值之间的差距）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ret = fit_func(p, x) - y</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">n_poly</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;n 次多项式拟合</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p_init = np.random.randn(n) <span class="comment"># 生成 n 个随机数</span></span><br><span class="line">    parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))</span><br><span class="line">    <span class="keyword">return</span> parameters[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>可以使用 <code>n=3</code> 验证一下上面的代码是否可用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_poly(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">8.00077828e-03</span>, <span class="number">-2.60473932e-01</span>,  <span class="number">3.76893089e+01</span>])</span><br></pre></td></tr></table></figure><p>此时得到的参数结果和公式（3）的结果一致，只是顺序有出入。这是因为 NumPy 中的多项式函数 <code>np.poly1d(3)</code> 默认的样式是：</p><p>$$<br>y(x) = 0.0080<em>x^2 - 0.26</em>x + 37\tag{4}<br>$$<br>接下来，我们绘制出 <code>4，5，6，7, 8, 9</code> 次多项式的拟合结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制出 4，5，6，7, 8, 9 次多项式的拟合图像</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制拟合图像时需要的临时点</span></span><br><span class="line">x_temp = np.linspace(<span class="number">0</span>, <span class="number">80</span>, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制子图</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>,<span class="number">0</span>].plot(x_temp, fit_func(n_poly(<span class="number">4</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">0</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">0</span>].set_title(<span class="string">&quot;m = 4&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>,<span class="number">1</span>].plot(x_temp, fit_func(n_poly(<span class="number">5</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">1</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">1</span>].set_title(<span class="string">&quot;m = 5&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>,<span class="number">2</span>].plot(x_temp, fit_func(n_poly(<span class="number">6</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">2</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">0</span>,<span class="number">2</span>].set_title(<span class="string">&quot;m = 6&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>,<span class="number">0</span>].plot(x_temp, fit_func(n_poly(<span class="number">7</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">0</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">0</span>].set_title(<span class="string">&quot;m = 7&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>,<span class="number">1</span>].plot(x_temp, fit_func(n_poly(<span class="number">8</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">1</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">1</span>].set_title(<span class="string">&quot;m = 8&quot;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>,<span class="number">2</span>].plot(x_temp, fit_func(n_poly(<span class="number">9</span>), x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">2</span>].scatter(x, y)</span><br><span class="line">axes[<span class="number">1</span>,<span class="number">2</span>].set_title(<span class="string">&quot;m = 9&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/4.png"></p><p>从上面的 <code>6</code> 张图可以看出，当 <code>m=4</code>（4 次多项式） 时，图像拟合的效果已经明显优于 <code>m=3</code> 的结果。但是随着 m 次数的增加，当 m=8 时，曲线呈现出明显的震荡，这也就是线性回归实验中所讲到的过拟和（Overfitting）现象。</p><h3 id="使用-scikit-learn-进行多项式拟合"><a href="#使用-scikit-learn-进行多项式拟合" class="headerlink" title="使用 scikit-learn 进行多项式拟合"></a>使用 scikit-learn 进行多项式拟合</h3><p>除了像上面我们自己去定义多项式及实现多项式回归拟合过程，也可以使用 <code>scikit-learn</code> 提供的多项式回归方法来完成。这里，我们会用到<code>sklearn.preprocessing.PolynomialFeatures()</code> 这个类。<code>PolynomialFeatures()</code> 主要的作用是产生多项式特征矩阵。<strong>如果你第一次接触这个概念，可能需要仔细理解下面的内容。</strong></p><p>对于一个二次多项式而言，我们知道它的标准形式为：$ y(x, w) = w_0 + w_1x + w_2x^2 $。但是，多项式回归却相当于线性回归的特殊形式。例如，我们这里令 $x = x_1$, $x^2 = x_2$ ，那么原方程就转换为：$ y(x, w) = w_0 + w_1<em>x_1 + w_2</em>x_2 $，这也就变成了多元线性回归。这就完成了<strong>一元高次多项式到多元一次多项式之间的转换</strong>。</p><p>举例说明，对于自变量向量 $X$ 和因变量 $y$，如果 $X$：</p><p>$$<br>\mathbf{X} = \begin{bmatrix}<br>       2    \[0.3em]<br>       -1 \[0.3em]<br>       3<br>     \end{bmatrix} \tag{5a}<br>$$<br>我们可以通过 $ y = w_1 x + w_0$ 线性回归模型进行拟合。同样，如果对于一元二次多项式 $ y(x, w) = w_0 + w_1x + w_2x^2 $，如果能得到由 $x = x_1$, $x^2 = x_2$ 构成的特征矩阵，即：</p><p>$$<br>\mathbf{X} = \left [ X, X^2 \right ] = \begin{bmatrix}<br> 2&amp; 4\ -1<br> &amp; 1\ 3<br> &amp; 9<br>\end{bmatrix}<br>\tag{5b}<br>$$<br>那么也就可以通过线性回归进行拟合了。</p><p>你可以手动计算上面的结果，但是<strong>当多项式为一元高次或者多元高次时，特征矩阵的表达和计算过程就变得比较复杂了</strong>。例如，下面是二元二次多项式的特征矩阵表达式。</p><p>$$<br>\mathbf{X} = \left [ X_{1}, X_{2}, X_{1}^2, X_{1}X_{2}, X_{2}^2 \right ]<br>\tag{5c}<br>$$<br>还好，在 scikit-learn 中，我们可以通过 <code>PolynomialFeatures()</code> 类自动产生多项式特征矩阵，<code>PolynomialFeatures()</code> 类的默认参数及常用参数定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.preprocessing.PolynomialFeatures(degree=<span class="number">2</span>, interaction_only=<span class="literal">False</span>, include_bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><ul><li><code>degree</code>: 多项式次数，默认为 2 次多项式</li><li><code>interaction_only</code>: 默认为 False，如果为 True 则产生相互影响的特征集。</li><li><code>include_bias</code>: 默认为 True，包含多项式中的截距项。</li></ul><p>对应上面的特征向量，我们使用 <code>PolynomialFeatures()</code> 的主要作用是产生 2 次多项式对应的特征矩阵，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;使用 PolynomialFeatures 自动生成特征矩阵</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">X=[<span class="number">2</span>, <span class="number">-1</span>, <span class="number">3</span>]</span><br><span class="line">X_reshape = np.array(X).reshape(<span class="built_in">len</span>(X), <span class="number">1</span>) <span class="comment"># 转换为列向量</span></span><br><span class="line">PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>).fit_transform(X_reshape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ <span class="number">2.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [<span class="number">-1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">9.</span>]])</span><br></pre></td></tr></table></figure><p>对于上方单元格中的矩阵，第 1 列为 $X^1$，第 2 列为 $X^2$。我们就可以通过多元线性方程 $ y(x, w) = w_0 + w_1<em>x_1 + w_2</em>x_2 $ 对数据进行拟合。</p><blockquote><p>注意：本篇文章中，你会看到大量的 <code>reshape</code> 操作，它们的目的都是为了满足某些类传参的数组形状。这些操作在本实验中是必须的，因为数据原始形状（如上面的一维数组）可能无法直接传入某些特定类中。但在实际工作中并不是必须的，因为你手中的原始数据集形状可能支持直接传入。所以，不必为这些 <code>reshape</code> 操作感到疑惑，也不要死记硬背。</p></blockquote><p>回到 <code>2.1</code> 小节中的示例数据，其自变量应该是 $x$，而因变量是 $y$。如果我们使用 2 次多项式拟合，那么首先使用 <code>PolynomialFeatures()</code> 得到特征矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;使用 sklearn 得到 2 次多项式回归特征矩阵</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">x = np.array(x).reshape(<span class="built_in">len</span>(x), <span class="number">1</span>) <span class="comment"># 转换为列向量</span></span><br><span class="line">y = np.array(y).reshape(<span class="built_in">len</span>(y), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">poly_x = poly_features.fit_transform(x)</span><br><span class="line"></span><br><span class="line">poly_x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">4.000e+00</span>, <span class="number">1.600e+01</span>],</span><br><span class="line">       [<span class="number">8.000e+00</span>, <span class="number">6.400e+01</span>],</span><br><span class="line">       [<span class="number">1.200e+01</span>, <span class="number">1.440e+02</span>],</span><br><span class="line">       [<span class="number">2.500e+01</span>, <span class="number">6.250e+02</span>],</span><br><span class="line">       [<span class="number">3.200e+01</span>, <span class="number">1.024e+03</span>],</span><br><span class="line">       [<span class="number">4.300e+01</span>, <span class="number">1.849e+03</span>],</span><br><span class="line">       [<span class="number">5.800e+01</span>, <span class="number">3.364e+03</span>],</span><br><span class="line">       [<span class="number">6.300e+01</span>, <span class="number">3.969e+03</span>],</span><br><span class="line">       [<span class="number">6.900e+01</span>, <span class="number">4.761e+03</span>],</span><br><span class="line">       [<span class="number">7.900e+01</span>, <span class="number">6.241e+03</span>]])</span><br></pre></td></tr></table></figure><p>可以看到，输出结果正好对应一元二次多项式特征矩阵公式：$\left [ X, X^2 \right ]$</p><p>然后，我们使用 scikit-learn 训练线性回归模型。这里将会使用到 <code>LinearRegression()</code> 类，<code>LinearRegression()</code> 类的默认参数及常用参数定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.LinearRegression(fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, copy_X=<span class="literal">True</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li><code>fit_intercept</code>: 默认为 True，计算截距项。</li><li><code>normalize</code>: 默认为 False，不针对数据进行标准化处理。</li><li><code>copy_X</code>: 默认为 True，即使用数据的副本进行操作，防止影响原数据。</li><li><code>n_jobs</code>: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;转换为线性回归预测</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(poly_x, y) <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到模型拟合参数</span></span><br><span class="line">model.intercept_, model.coef_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([<span class="number">2.13162821e-14</span>]), array([[<span class="number">1.00000000e+00</span>, <span class="number">4.35999447e-18</span>]]))</span><br></pre></td></tr></table></figure><p>你会发现，这里得到的参数值和公式（3），（4）一致。为了更加直观，这里同样绘制出拟合后的图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;绘制拟合图像</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">x_temp = np.array(x_temp).reshape(<span class="built_in">len</span>(x_temp),<span class="number">1</span>)</span><br><span class="line">poly_x_temp = poly_features.fit_transform(x_temp)</span><br><span class="line"></span><br><span class="line">plt.plot(x_temp, model.predict(poly_x_temp), <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/polynomial%20regression/5.png"></p><p>你会发现，上图似曾相识。它和公式（3）下方的图其实是一致的。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;多项式回归介绍&quot;&gt;&lt;a href=&quot;#多项式回归介绍&quot; class=&quot;headerlink&quot; title=&quot;多项式回归介绍&quot;&gt;&lt;/a&gt;多项式回归介绍&lt;/h2&gt;&lt;p&gt;在线性回归中，我们通过建立自变量 &lt;code&gt;x&lt;/code&gt; 的一次方程来拟合数据。而非线性回归中，则需要建立因变量和自变量之间的非线性关系。从直观上讲，也就是拟合的直线变成了「曲线」。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="多项式回归" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Polynomial Regression" scheme="http://www.laugh12321.cn/blog/tags/Polynomial-Regression/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|线性回归三大评价指标实现『MAE, MSE, MAPE』</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/02/evaluation_index_with_linear_regression/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/02/evaluation_index_with_linear_regression/</id>
    <published>2019-01-02T00:00:00.000Z</published>
    <updated>2020-10-23T08:25:21.765Z</updated>
    
    <content type="html"><![CDATA[<p>对于回归预测结果，通常会有平均绝对误差、平均绝对百分比误差、均方误差等多个指标进行评价。这里，我们先介绍最常用的3个：</p><a id="more"></a><p><strong>平均绝对误差（MAE）</strong><br>就是绝对误差的平均值，它的计算公式如下：<br>$$<br>MAE(y,\hat{y}) = \frac{1}{n}(\sum_{i = 1}^{n}\left | y - \hat{y} \right |)<br>$$<br>其中，$y_{i}$ 表示真实值，$\hat y_{i}$ 表示预测值，$n$ 则表示值的个数。MAE 的值越小，说明预测模型拥有更好的精确度。我们可以尝试使用 Python 实现 MAE 计算函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mae_value</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    y_true -- 测试集目标真实值</span></span><br><span class="line"><span class="string">    y_pred -- 测试集目标预测值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    mae -- MAE 评价指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    n = <span class="built_in">len</span>(y_true)</span><br><span class="line">    mae = <span class="built_in">sum</span>(np.<span class="built_in">abs</span>(y_true - y_pred))/n</span><br><span class="line">    <span class="keyword">return</span> mae</span><br></pre></td></tr></table></figure><p><strong>均方误差（MSE）</strong><br>它表示误差的平方的期望值，它的计算公式如下：<br>$$<br>{MSE}(y, \hat{y} ) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y})^{2}<br>$$</p><p>其中，$y_{i}$ 表示真实值，$\hat y_{i}$ 表示预测值，$n$ 则表示值的个数。MSE 的值越小，说明预测模型拥有更好的精确度。同样，我们可以尝试使用 Python 实现 MSE 计算函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse_value</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    y_true -- 测试集目标真实值</span></span><br><span class="line"><span class="string">    y_pred -- 测试集目标预测值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    mse -- MSE 评价指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    n = <span class="built_in">len</span>(y_true)</span><br><span class="line">    mse = <span class="built_in">sum</span>(np.square(y_true - y_pred))/n</span><br><span class="line">    <span class="keyword">return</span> mse</span><br></pre></td></tr></table></figure><p>**平均绝对百分比误差 $MAPE$**。</p><p>$MAPE$ 是 $MAD$ 的变形，它是一个百分比值，因此比其他统计量更容易理解。例如，如果 $MAPE$ 为 $5$，则表示预测结果较真实结果平均偏离 $5%$。$MAPE$ 的计算公式如下：<br>$$<br>{MAPE}(y, \hat{y} ) = \frac{\sum_{i=1}^{n}{|\frac{y_{i}-\hat y_{i}}{y_{i}}|}}{n} \times 100<br>$$</p><p>其中，$y_{i}$ 表示真实值，$\hat y_{i}$ 表示预测值，$n$ 则表示值的个数。$MAPE$ 的值越小，说明预测模型拥有更好的精确度。使用 Python 实现 MSE 计算函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mape</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    y_true -- 测试集目标真实值</span></span><br><span class="line"><span class="string">    y_pred -- 测试集目标预测值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    mape -- MAPE 评价指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    n = <span class="built_in">len</span>(y_true)</span><br><span class="line">    mape = <span class="built_in">sum</span>(np.<span class="built_in">abs</span>((y_true - y_pred)/y_true))/n*<span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> mape</span><br></pre></td></tr></table></figure><hr><p><strong>参考</strong>：</p><ul><li><a href="https://blog.csdn.net/cqfdcw/article/details/78173839">方差（variance）、标准差（Standard Deviation）、均方差、均方根值（RMS）、均方误差（MSE）、均方根误差（RMSE）</a></li><li><a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean squared error-Wikipedia</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;对于回归预测结果，通常会有平均绝对误差、平均绝对百分比误差、均方误差等多个指标进行评价。这里，我们先介绍最常用的3个：&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Linear Regression" scheme="http://www.laugh12321.cn/blog/tags/Linear-Regression/"/>
    
    <category term="MAE" scheme="http://www.laugh12321.cn/blog/tags/MAE/"/>
    
    <category term="MSE" scheme="http://www.laugh12321.cn/blog/tags/MSE/"/>
    
    <category term="MAPE" scheme="http://www.laugh12321.cn/blog/tags/MAPE/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|线性回归算法详解</title>
    <link href="http://www.laugh12321.cn/blog/2019/01/01/linear_regression/"/>
    <id>http://www.laugh12321.cn/blog/2019/01/01/linear_regression/</id>
    <published>2019-01-01T00:00:00.000Z</published>
    <updated>2020-10-23T08:24:42.196Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。</p><a id="more"></a><h2 id="线性回归介绍"><a href="#线性回归介绍" class="headerlink" title="线性回归介绍"></a>线性回归介绍</h2><p>在了解线性回归之前，我们得先了解分类和回归问题的区别。</p><p>首先，回归问题和分类问题一样，训练数据都包含标签，这也是监督学习的特点。而不同之处在于，分类问题预测的是类别，回归问题预测的是连续值。</p><p>例如，回归问题往往解决：</p><ul><li>股票价格预测</li><li>房价预测</li><li>洪水水位线</li></ul><p>上面列举的问题，我们需要预测的目标都不是类别，而是实数连续值。</p><p><img width='800px' src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/00.png"></img></p><p>也就是说，回归问题旨在实现对连续值的预测，例如股票的价格、房价的趋势等。比如，下方展现了一个房屋面积和价格的对应关系图。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/1.png" alt="此处输入图片的描述"></p><p>如上图所示，不同的房屋面积对应着不同的价格。现在，假设我手中有一套房屋想要出售，而出售时就需要预先对房屋进行估值。于是，我想通过上图，也就是其他房屋的售价来判断手中的房产价值是多少。应该怎么做呢？</p><p>我采用的方法是这样的。如下图所示，首先画了一条<font color="red">红色</font>的直线，让其大致验证<font color="orange">橙色</font>点分布的延伸趋势。然后，我将已知房屋的面积大小对应到红色直线上，也就是<font color="blue">蓝色</font>点所在位置。最后，再找到蓝色点对应于房屋的价格作为房屋最终的预估价值。</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/2.png" alt="此处输入图片的描述"></p><p>在上图呈现的这个过程中，通过找到一条直线去拟合数据点的分布趋势的过程，就是<strong>线性回归</strong>的过程。而线性回归中的「线性」代指线性关系，也就是图中所绘制的红色直线。</p><p>此时，你可能心中会有一个疑问。上图中的红色直线是怎么绘制出来的呢？为什么不可以像下图中另外两条绿色虚线，而偏偏要选择红色直线呢？</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/3.png" alt="此处输入图片的描述"></p><p>上图中的绿色虚线的确也能反应数据点的分布趋势。所以，找到最适合的那一条红色直线，也是线性回归中需要解决的重要问题之一。</p><p>通过上面这个小例子，相信你对线性回归已经有一点点印象了，至少大致明白它能做什么。接下来的内容中，我们将了解线性回归背后的数学原理，以及使用 Python 代码对其实现。</p><h2 id="线性回归原理及实现"><a href="#线性回归原理及实现" class="headerlink" title="线性回归原理及实现"></a>线性回归原理及实现</h2><h3 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h3><p>上面针对线性回归的介绍内容中，我们列举了一个房屋面积与房价变化的例子。其中，房屋面积为自变量，而房价则为因变量。另外，我们将只有 1 个自变量的线性拟合过程叫做一元线性回归。</p><p>下面，我们就生成一组房屋面积和房价变化的示例数据。<code>x</code> 为房屋面积，单位是平方米; <code>y</code> 为房价，单位是万元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">56</span>, <span class="number">72</span>, <span class="number">69</span>, <span class="number">88</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">76</span>, <span class="number">79</span>, <span class="number">94</span>, <span class="number">74</span>])</span><br><span class="line">y = np.array([<span class="number">92</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">110</span>, <span class="number">130</span>, <span class="number">99</span>, <span class="number">96</span>, <span class="number">102</span>, <span class="number">105</span>, <span class="number">92</span>])</span><br></pre></td></tr></table></figure><p>示例数据由 <code>10</code> 组房屋面积及价格对应组成。接下来，通过 Matplotlib 绘制数据点，<code>x, y</code> 分别对应着横坐标和纵坐标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Area&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Price&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/out_00.png"></p><p>正如上面所说，线性回归即通过线性方程（<code>1</code> 次函数）去拟合数据点。那么，我们令函数的表达式为：</p><p>$$ y(x, w) = w_0 + w_1x \tag{1} $$</p><p>公式（1）是典型的一元一次函数表达式，我们通过组合不同的 $w_0$ 和 $w_1$ 的值得到不同的拟合直线。我们对公式（1）进行代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x, w0, w1</span>):</span></span><br><span class="line">    y = w0 + w1 * x</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>那么，<strong>哪一条直线最能反应出数据的变化趋势呢？</strong></p><p>如下图所示，当我们使用 $y(x, w) = w_0 + w_1x$ 对数据进行拟合时，我们能得到拟合的整体误差，即图中蓝色线段的长度总和。如果某一条直线对应的误差值最小，是不是就代表这条直线最能反映数据点的分布趋势呢？</p><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/4.png"></p><h3 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h3><p>正如上面所说，如果一个数据点为 ($x_{i}$, $y_{i}$)，那么它对应的误差就为:</p><p>$$y_{i}-(w_0 + w_1x_{i}) \tag2$$</p><p>上面的误差往往也称之为残差。但是在机器学习中，我们更喜欢称作「损失」，即真实值和预测值之间的偏离程度。那么，对应 n 个全部数据点而言，其对应的残差损失总和就为：<br>$$<br>\sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i})) \tag3<br>$$<br>在线性回归中，我们更偏向于使用均方误差作为衡量损失的指标，而均方误差即为残差的平方和。公式如下：</p><p>$$<br>\sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2\tag4<br>$$<br>对于公式（4）而言，机器学习中有一个专门的名词，那就是「平方损失函数」。而为了得到拟合参数 $w_0$ 和 $w_1$ 最优的数值，我们的目标就是让公式（4）对应的平方损失函数最小。</p><p>同样，我们可以对公式（4）进行代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span>(<span class="params">x, y, w0, w1</span>):</span></span><br><span class="line">    loss = <span class="built_in">sum</span>(np.square(y - (w0 + w1*x)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="最小二乘法及代数求解"><a href="#最小二乘法及代数求解" class="headerlink" title="最小二乘法及代数求解"></a>最小二乘法及代数求解</h3><p>最小二乘法是用于求解线性回归拟合参数 $w$ 的一种常用方法。最小二乘法中的「二乘」代表平方，最小二乘也就是最小平方。而这里的平方就是指代上面的平方损失函数。</p><p>简单来讲，最小二乘法也就是求解平方损失函数最小值的方法。那么，到底该怎样求解呢？这就需要使用到高等数学中的知识。推导如下：</p><p>首先，平方损失函数为：</p><p>$$<br>f = \sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2 \tag5<br>$$<br>我们的目标是求取平方损失函数 $min(f)$ 最小时，对应的 $w$。首先求 $f$ 的 <code>1</code> 阶偏导数：</p><p>$$<br>\frac{\partial f}{\partial w_{0}}=-2(\sum_{i=1}^{n}{y_i}-nw_{0}-w_{1}\sum_{i=1}^{n}{x_i})\<br>\frac{\partial f}{\partial w_{1}}=-2(\sum_{i=1}^{n}{x_iy_i}-w_{0}\sum_{i=1}^{n}{x_i}-w_{1}\sum_{i=1}^{n}{x_i}^2) \tag6<br>$$<br>然后，我们令 $\frac{\partial f}{\partial w_{0}}=0$ 以及  $\frac{\partial f}{\partial w_{1}}=0$，解得：<br>$$<br>w_{1}=\frac {n\sum_{}^{}{x_iy_i}-\sum_{}^{}{x_i}\sum_{}^{}{y_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2}\<br>w_{0}=\frac {\sum_{}^{}{x_i}^2\sum_{}^{}{y_i}-\sum_{}^{}{x_i}\sum_{}^{}{x_iy_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2}\tag7<br>$$<br>到目前为止，已经求出了平方损失函数最小时对应的 $w$ 参数值，这也就是最佳拟合直线。</p><h3 id="线性回归-Python-实现"><a href="#线性回归-Python-实现" class="headerlink" title="线性回归 Python 实现"></a>线性回归 Python 实现</h3><p>我们将公式（7）求解得到 $w$ 的过程进行代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w_calculator</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(x)</span><br><span class="line">    w1 = (n*<span class="built_in">sum</span>(x*y) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(y))/(n*<span class="built_in">sum</span>(x*x) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x))</span><br><span class="line">    w0 = (<span class="built_in">sum</span>(x*x)*<span class="built_in">sum</span>(y) - <span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x*y))/(n*<span class="built_in">sum</span>(x*x)-<span class="built_in">sum</span>(x)*<span class="built_in">sum</span>(x))</span><br><span class="line">    <span class="keyword">return</span> w0, w1</span><br></pre></td></tr></table></figure><p>于是，可以向函数 <code>w_calculator(x, y)</code> 中传入 <code>x</code> 和 <code>y</code> 得到 $w_0$ 和 $w_1$ 的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w_calculator(x, y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">41.33509168550616</span>, <span class="number">0.7545842753077117</span>)</span><br></pre></td></tr></table></figure><p>当然，我们也可以求得此时对应的平方损失的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w0 = w_calculator(x, y)[<span class="number">0</span>]</span><br><span class="line">w1 = w_calculator(x, y)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">square_loss(x, y, w0, w1)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">447.69153479025357</span></span><br></pre></td></tr></table></figure><p>接下来，我们尝试将拟合得到的直线绘制到原图中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_temp = np.linspace(<span class="number">50</span>,<span class="number">120</span>,<span class="number">100</span>) <span class="comment"># 绘制直线生成的临时点</span></span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x_temp, x_temp*w1 + w0, <span class="string">&#x27;r&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://laugh12321-1258080753.cos.ap-chengdu.myqcloud.com/laugh's%20blog/images/linear%20regression/out_01.png"></p><p>从上图可以看出，拟合的效果还是不错的。那么，如果你手中有一套 <code>150</code> 平米的房产想售卖，获得预估报价就只需要带入方程即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(<span class="number">150</span>, w0, w1)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">154.5227329816629</span></span><br></pre></td></tr></table></figure><p>这里得到的预估售价约为 <code>154</code> 万元。</p><h3 id="线性回归-scikit-learn-实现"><a href="#线性回归-scikit-learn-实现" class="headerlink" title="线性回归 scikit-learn 实现"></a>线性回归 scikit-learn 实现</h3><p>上面的内容中，我们学习了什么是最小二乘法，以及使用 Python 对最小二乘线性回归进行了完整实现。那么，我们如何利用机器学习开源模块 scikit-learn 实现最小二乘线性回归方法呢？</p><p>使用 scikit-learn 实现线性回归的过程会简单很多，这里要用到 <code>LinearRegression()</code> 类。看一下其中的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.LinearRegression(fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, copy_X=<span class="literal">True</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>fit_intercept</code>: 默认为 True，计算截距项。</li><li><code>normalize</code>: 默认为 False，不针对数据进行标准化处理。</li><li><code>copy_X</code>: 默认为 True，即使用数据的副本进行操作，防止影响原数据。</li><li><code>n_jobs</code>: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;scikit-learn 线性回归拟合</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(x.reshape(<span class="built_in">len</span>(x),<span class="number">1</span>), y) <span class="comment"># 训练, reshape 操作把数据处理成 fit 能接受的形状</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到模型拟合参数</span></span><br><span class="line">model.intercept_, model.coef_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">41.33509168550615</span>, array([<span class="number">0.75458428</span>]))</span><br></pre></td></tr></table></figure><p>我们通过 <code>model.intercept_</code> 得到拟合的截距项，即上面的 $w_{0}$，通过 <code>model.coef_</code> 得到 $x$ 的系数，即上面的 $w_{1}$。对比发现，结果是<strong>完全一致</strong>的。</p><p>同样，我们可以预测 <code>150</code> 平米房产的价格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict([[<span class="number">150</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">154.52273298</span>])</span><br></pre></td></tr></table></figure><p>可以看到，这里得出的结果和自行实现计算结果一致。</p><h2 id="最小二乘法的矩阵推导及实现"><a href="#最小二乘法的矩阵推导及实现" class="headerlink" title="最小二乘法的矩阵推导及实现"></a>最小二乘法的矩阵推导及实现</h2><p>学习完上面的内容，相信你已经了解了什么是最小二乘法，以及如何使用最小二乘法进行线性回归拟合。上面，实验采用了求偏导数的方法，并通过代数求解找到了最佳拟合参数 <code>w</code> 的值。</p><p>这里，我们尝试另外一种方法，即通过矩阵的变换来计算参数 <code>w</code> 。推导如下：</p><p>首先，一元线性函数的表达式为 $ y(x, w) = w_0 + w_1x$，表达成矩阵形式为：</p><p>$$\begin{bmatrix}1, x_{1} \ 1, x_{2} \ … \ 1, x_{9} \ 1, x_{10} \end{bmatrix} * \begin{bmatrix}w_{0} \ w_{1} \end{bmatrix} = \begin{bmatrix}y_{1} \ y_{2} \ … \ y_{9} \ y_{10} \end{bmatrix} \Rightarrow \begin{bmatrix}1, 56 \ 1, 72 \ … \ 1, 94 \ 1, 74 \end{bmatrix}* \begin{bmatrix}w_{0} \ w_{1} \end{bmatrix}= \begin{bmatrix}92 \ 102 \ … \ 105 \ 92 \end{bmatrix} \tag{8a}$$</p><p>即：<br>$$ y(x, w) = XW \tag{8b} $$</p><p>（8）式中，$W$ 为 $\begin{bmatrix}w_{0}<br>\ w_{1}<br>\end{bmatrix}$，而 $X$ 则是 $\begin{bmatrix}1, x_{1}<br>\ 1, x_{2}<br>\ …<br>\ 1, x_{9}<br>\ 1, x_{10}<br>\end{bmatrix}$ 矩阵。然后，平方损失函数为：<br>$$<br>f = \sum\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2 =(y-XW)^T(y-XW)\tag{9}<br>$$<br>此时，对矩阵求偏导数（超纲）得到：</p><p>$$<br>\frac{\partial f}{\partial W}=2<em>X^TXW-2</em>X^Ty=0 \tag{10}<br>$$<br>当矩阵 $X^TX$ 满秩（不满秩后面的实验中会讨论）时，$(X^TX)^{-1}X^TX=E$，且 $EW=W$。所以，$(X^TX)^{-1}X^TXW=(X^TX)^{-1}X^Ty$。最终得到：<br>$$<br>W=(X^TX)^{-1}X^Ty \tag{11}<br>$$<br>我们可以针对公式（11）进行代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w_matrix</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    w = (x.T * x).I * x.T * y</span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure><p>我们针对原 <code>x</code> 数据添加截距项系数 <code>1</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.matrix([[<span class="number">1</span>,<span class="number">56</span>],[<span class="number">1</span>,<span class="number">72</span>],[<span class="number">1</span>,<span class="number">69</span>],[<span class="number">1</span>,<span class="number">88</span>],[<span class="number">1</span>,<span class="number">102</span>],[<span class="number">1</span>,<span class="number">86</span>],[<span class="number">1</span>,<span class="number">76</span>],[<span class="number">1</span>,<span class="number">79</span>],[<span class="number">1</span>,<span class="number">94</span>],[<span class="number">1</span>,<span class="number">74</span>]])</span><br><span class="line">y = np.matrix([<span class="number">92</span>, <span class="number">102</span>, <span class="number">86</span>, <span class="number">110</span>, <span class="number">130</span>, <span class="number">99</span>, <span class="number">96</span>, <span class="number">102</span>, <span class="number">105</span>, <span class="number">92</span>])</span><br><span class="line"></span><br><span class="line">w_matrix(x, y.reshape(<span class="number">10</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">matrix([[<span class="number">41.33509169</span>],</span><br><span class="line">        [ <span class="number">0.75458428</span>]])</span><br></pre></td></tr></table></figure><p>可以看到，矩阵计算结果和前面的代数计算结果一致。你可能会有疑问，那就是为什么要采用矩阵变换的方式计算？一开始学习的代数计算方法不好吗？</p><p>其实，并不是说代数计算方式不好，在小数据集下二者运算效率接近。但是，当我们面对十万或百万规模的数据时，矩阵计算的效率就会高很多，这就是为什么要学习矩阵计算的原因。</p><p><strong>参考</strong>：</p><ul><li><a href="https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">最小二乘法-维基百科</a></li><li><a href="https://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8">线性回归-维基百科</a></li><li><a href="https://www.zhihu.com/question/37031188">知乎问答-最小二乘法的本质是什么？</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;线性回归&lt;/h1&gt;&lt;p&gt;线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="http://www.laugh12321.cn/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    
    <category term="Machine Learning" scheme="http://www.laugh12321.cn/blog/tags/Machine-Learning/"/>
    
    <category term="Linear Regression" scheme="http://www.laugh12321.cn/blog/tags/Linear-Regression/"/>
    
  </entry>
  
</feed>
